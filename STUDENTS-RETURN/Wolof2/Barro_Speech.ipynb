{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CTC.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"jzzuGAMna4NG","colab":{},"executionInfo":{"status":"ok","timestamp":1593989202963,"user_tz":-120,"elapsed":1637,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kp0Mh55Sy19-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":523},"executionInfo":{"status":"ok","timestamp":1593989231920,"user_tz":-120,"elapsed":25191,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"33b51e50-19e2-4435-8a73-5453d9d21808"},"source":["!pip install torchaudio\n","!pip install PyDrive\n","!pip install soundfile"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting torchaudio\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/0a/40e53c686c2af65b2a4e818d11d9b76fa79178440caf99f3ceb2a32c3b04/torchaudio-0.5.1-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 5.0MB/s \n","\u001b[?25hRequirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.6/dist-packages (from torchaudio) (1.5.1+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->torchaudio) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->torchaudio) (1.18.5)\n","Installing collected packages: torchaudio\n","Successfully installed torchaudio-0.5.1\n","Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n","Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n","Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.12)\n","Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.12.0)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n","Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n","Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.17.2)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.6)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (4.1.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (47.3.1)\n","Collecting soundfile\n","  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n","Installing collected packages: soundfile\n","Successfully installed soundfile-0.10.3.post1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qB1FVfDQy9K4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1593989241897,"user_tz":-120,"elapsed":6637,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"d2091b07-10d0-462c-fe1f-c4ea972e6eb2"},"source":["! git clone https://github.com/facebookresearch/CPC_audio.git"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Cloning into 'CPC_audio'...\n","remote: Enumerating objects: 84, done.\u001b[K\n","remote: Counting objects:   1% (1/84)\u001b[K\rremote: Counting objects:   2% (2/84)\u001b[K\rremote: Counting objects:   3% (3/84)\u001b[K\rremote: Counting objects:   4% (4/84)\u001b[K\rremote: Counting objects:   5% (5/84)\u001b[K\rremote: Counting objects:   7% (6/84)\u001b[K\rremote: Counting objects:   8% (7/84)\u001b[K\rremote: Counting objects:   9% (8/84)\u001b[K\rremote: Counting objects:  10% (9/84)\u001b[K\rremote: Counting objects:  11% (10/84)\u001b[K\rremote: Counting objects:  13% (11/84)\u001b[K\rremote: Counting objects:  14% (12/84)\u001b[K\rremote: Counting objects:  15% (13/84)\u001b[K\rremote: Counting objects:  16% (14/84)\u001b[K\rremote: Counting objects:  17% (15/84)\u001b[K\rremote: Counting objects:  19% (16/84)\u001b[K\rremote: Counting objects:  20% (17/84)\u001b[K\rremote: Counting objects:  21% (18/84)\u001b[K\rremote: Counting objects:  22% (19/84)\u001b[K\rremote: Counting objects:  23% (20/84)\u001b[K\rremote: Counting objects:  25% (21/84)\u001b[K\rremote: Counting objects:  26% (22/84)\u001b[K\rremote: Counting objects:  27% (23/84)\u001b[K\rremote: Counting objects:  28% (24/84)\u001b[K\rremote: Counting objects:  29% (25/84)\u001b[K\rremote: Counting objects:  30% (26/84)\u001b[K\rremote: Counting objects:  32% (27/84)\u001b[K\rremote: Counting objects:  33% (28/84)\u001b[K\rremote: Counting objects:  34% (29/84)\u001b[K\rremote: Counting objects:  35% (30/84)\u001b[K\rremote: Counting objects:  36% (31/84)\u001b[K\rremote: Counting objects:  38% (32/84)\u001b[K\rremote: Counting objects:  39% (33/84)\u001b[K\rremote: Counting objects:  40% (34/84)\u001b[K\rremote: Counting objects:  41% (35/84)\u001b[K\rremote: Counting objects:  42% (36/84)\u001b[K\rremote: Counting objects:  44% (37/84)\u001b[K\rremote: Counting objects:  45% (38/84)\u001b[K\rremote: Counting objects:  46% (39/84)\u001b[K\rremote: Counting objects:  47% (40/84)\u001b[K\rremote: Counting objects:  48% (41/84)\u001b[K\rremote: Counting objects:  50% (42/84)\u001b[K\rremote: Counting objects:  51% (43/84)\u001b[K\rremote: Counting objects:  52% (44/84)\u001b[K\rremote: Counting objects:  53% (45/84)\u001b[K\rremote: Counting objects:  54% (46/84)\u001b[K\rremote: Counting objects:  55% (47/84)\u001b[K\rremote: Counting objects:  57% (48/84)\u001b[K\rremote: Counting objects:  58% (49/84)\u001b[K\rremote: Counting objects:  59% (50/84)\u001b[K\rremote: Counting objects:  60% (51/84)\u001b[K\rremote: Counting objects:  61% (52/84)\u001b[K\rremote: Counting objects:  63% (53/84)\u001b[K\rremote: Counting objects:  64% (54/84)\u001b[K\rremote: Counting objects:  65% (55/84)\u001b[K\rremote: Counting objects:  66% (56/84)\u001b[K\rremote: Counting objects:  67% (57/84)\u001b[K\rremote: Counting objects:  69% (58/84)\u001b[K\rremote: Counting objects:  70% (59/84)\u001b[K\rremote: Counting objects:  71% (60/84)\u001b[K\rremote: Counting objects:  72% (61/84)\u001b[K\rremote: Counting objects:  73% (62/84)\u001b[K\rremote: Counting objects:  75% (63/84)\u001b[K\rremote: Counting objects:  76% (64/84)\u001b[K\rremote: Counting objects:  77% (65/84)\u001b[K\rremote: Counting objects:  78% (66/84)\u001b[K\rremote: Counting objects:  79% (67/84)\u001b[K\rremote: Counting objects:  80% (68/84)\u001b[K\rremote: Counting objects:  82% (69/84)\u001b[K\rremote: Counting objects:  83% (70/84)\u001b[K\rremote: Counting objects:  84% (71/84)\u001b[K\rremote: Counting objects:  85% (72/84)\u001b[K\rremote: Counting objects:  86% (73/84)\u001b[K\rremote: Counting objects:  88% (74/84)\u001b[K\rremote: Counting objects:  89% (75/84)\u001b[K\rremote: Counting objects:  90% (76/84)\u001b[K\rremote: Counting objects:  91% (77/84)\u001b[K\rremote: Counting objects:  92% (78/84)\u001b[K\rremote: Counting objects:  94% (79/84)\u001b[K\rremote: Counting objects:  95% (80/84)\u001b[K\rremote: Counting objects:  96% (81/84)\u001b[K\rremote: Counting objects:  97% (82/84)\u001b[K\rremote: Counting objects:  98% (83/84)\u001b[K\rremote: Counting objects: 100% (84/84)\u001b[K\rremote: Counting objects: 100% (84/84), done.\u001b[K\n","remote: Compressing objects:   1% (1/65)\u001b[K\rremote: Compressing objects:   3% (2/65)\u001b[K\rremote: Compressing objects:   4% (3/65)\u001b[K\rremote: Compressing objects:   6% (4/65)\u001b[K\rremote: Compressing objects:   7% (5/65)\u001b[K\rremote: Compressing objects:   9% (6/65)\u001b[K\rremote: Compressing objects:  10% (7/65)\u001b[K\rremote: Compressing objects:  12% (8/65)\u001b[K\rremote: Compressing objects:  13% (9/65)\u001b[K\rremote: Compressing objects:  15% (10/65)\u001b[K\rremote: Compressing objects:  16% (11/65)\u001b[K\rremote: Compressing objects:  18% (12/65)\u001b[K\rremote: Compressing objects:  20% (13/65)\u001b[K\rremote: Compressing objects:  21% (14/65)\u001b[K\rremote: Compressing objects:  23% (15/65)\u001b[K\rremote: Compressing objects:  24% (16/65)\u001b[K\rremote: Compressing objects:  26% (17/65)\u001b[K\rremote: Compressing objects:  27% (18/65)\u001b[K\rremote: Compressing objects:  29% (19/65)\u001b[K\rremote: Compressing objects:  30% (20/65)\u001b[K\rremote: Compressing objects:  32% (21/65)\u001b[K\rremote: Compressing objects:  33% (22/65)\u001b[K\rremote: Compressing objects:  35% (23/65)\u001b[K\rremote: Compressing objects:  36% (24/65)\u001b[K\rremote: Compressing objects:  38% (25/65)\u001b[K\rremote: Compressing objects:  40% (26/65)\u001b[K\rremote: Compressing objects:  41% (27/65)\u001b[K\rremote: Compressing objects:  43% (28/65)\u001b[K\rremote: Compressing objects:  44% (29/65)\u001b[K\rremote: Compressing objects:  46% (30/65)\u001b[K\rremote: Compressing objects:  47% (31/65)\u001b[K\rremote: Compressing objects:  49% (32/65)\u001b[K\rremote: Compressing objects:  50% (33/65)\u001b[K\rremote: Compressing objects:  52% (34/65)\u001b[K\rremote: Compressing objects:  53% (35/65)\u001b[K\rremote: Compressing objects:  55% (36/65)\u001b[K\rremote: Compressing objects:  56% (37/65)\u001b[K\rremote: Compressing objects:  58% (38/65)\u001b[K\rremote: Compressing objects:  60% (39/65)\u001b[K\rremote: Compressing objects:  61% (40/65)\u001b[K\rremote: Compressing objects:  63% (41/65)\u001b[K\rremote: Compressing objects:  64% (42/65)\u001b[K\rremote: Compressing objects:  66% (43/65)\u001b[K\rremote: Compressing objects:  67% (44/65)\u001b[K\rremote: Compressing objects:  69% (45/65)\u001b[K\rremote: Compressing objects:  70% (46/65)\u001b[K\rremote: Compressing objects:  72% (47/65)\u001b[K\rremote: Compressing objects:  73% (48/65)\u001b[K\rremote: Compressing objects:  75% (49/65)\u001b[K\rremote: Compressing objects:  76% (50/65)\u001b[K\rremote: Compressing objects:  78% (51/65)\u001b[K\rremote: Compressing objects:  80% (52/65)\u001b[K\rremote: Compressing objects:  81% (53/65)\u001b[K\rremote: Compressing objects:  83% (54/65)\u001b[K\rremote: Compressing objects:  84% (55/65)\u001b[K\rremote: Compressing objects:  86% (56/65)\u001b[K\rremote: Compressing objects:  87% (57/65)\u001b[K\rremote: Compressing objects:  89% (58/65)\u001b[K\rremote: Compressing objects:  90% (59/65)\u001b[K\rremote: Compressing objects:  92% (60/65)\u001b[K\rremote: Compressing objects:  93% (61/65)\u001b[K\rremote: Compressing objects:  95% (62/65)\u001b[K\rremote: Compressing objects:  96% (63/65)\u001b[K\rremote: Compressing objects:  98% (64/65)\u001b[K\rremote: Compressing objects: 100% (65/65)\u001b[K\rremote: Compressing objects: 100% (65/65), done.\u001b[K\n","Unpacking objects:   1% (1/84)   \rUnpacking objects:   2% (2/84)   \rUnpacking objects:   3% (3/84)   \rUnpacking objects:   4% (4/84)   \rUnpacking objects:   5% (5/84)   \rUnpacking objects:   7% (6/84)   \rUnpacking objects:   8% (7/84)   \rUnpacking objects:   9% (8/84)   \rUnpacking objects:  10% (9/84)   \rUnpacking objects:  11% (10/84)   \rUnpacking objects:  13% (11/84)   \rUnpacking objects:  14% (12/84)   \rUnpacking objects:  15% (13/84)   \rUnpacking objects:  16% (14/84)   \rUnpacking objects:  17% (15/84)   \rUnpacking objects:  19% (16/84)   \rUnpacking objects:  20% (17/84)   \rUnpacking objects:  21% (18/84)   \rUnpacking objects:  22% (19/84)   \rUnpacking objects:  23% (20/84)   \rUnpacking objects:  25% (21/84)   \rUnpacking objects:  26% (22/84)   \rUnpacking objects:  27% (23/84)   \rUnpacking objects:  28% (24/84)   \rUnpacking objects:  29% (25/84)   \rUnpacking objects:  30% (26/84)   \rUnpacking objects:  32% (27/84)   \rUnpacking objects:  33% (28/84)   \rUnpacking objects:  34% (29/84)   \rUnpacking objects:  35% (30/84)   \rUnpacking objects:  36% (31/84)   \rUnpacking objects:  38% (32/84)   \rUnpacking objects:  39% (33/84)   \rUnpacking objects:  40% (34/84)   \rUnpacking objects:  41% (35/84)   \rUnpacking objects:  42% (36/84)   \rUnpacking objects:  44% (37/84)   \rUnpacking objects:  45% (38/84)   \rUnpacking objects:  46% (39/84)   \rUnpacking objects:  47% (40/84)   \rUnpacking objects:  48% (41/84)   \rUnpacking objects:  50% (42/84)   \rUnpacking objects:  51% (43/84)   \rUnpacking objects:  52% (44/84)   \rUnpacking objects:  53% (45/84)   \rUnpacking objects:  54% (46/84)   \rUnpacking objects:  55% (47/84)   \rUnpacking objects:  57% (48/84)   \rUnpacking objects:  58% (49/84)   \rUnpacking objects:  59% (50/84)   \rUnpacking objects:  60% (51/84)   \rUnpacking objects:  61% (52/84)   \rUnpacking objects:  63% (53/84)   \rUnpacking objects:  64% (54/84)   \rUnpacking objects:  65% (55/84)   \rUnpacking objects:  66% (56/84)   \rUnpacking objects:  67% (57/84)   \rUnpacking objects:  69% (58/84)   \rUnpacking objects:  70% (59/84)   \rUnpacking objects:  71% (60/84)   \rUnpacking objects:  72% (61/84)   \rUnpacking objects:  73% (62/84)   \rUnpacking objects:  75% (63/84)   \rUnpacking objects:  76% (64/84)   \rUnpacking objects:  77% (65/84)   \rUnpacking objects:  78% (66/84)   \rUnpacking objects:  79% (67/84)   \rremote: Total 84 (delta 13), reused 75 (delta 6), pack-reused 0\u001b[K\n","Unpacking objects:  80% (68/84)   \rUnpacking objects:  82% (69/84)   \rUnpacking objects:  83% (70/84)   \rUnpacking objects:  84% (71/84)   \rUnpacking objects:  85% (72/84)   \rUnpacking objects:  86% (73/84)   \rUnpacking objects:  88% (74/84)   \rUnpacking objects:  89% (75/84)   \rUnpacking objects:  90% (76/84)   \rUnpacking objects:  91% (77/84)   \rUnpacking objects:  92% (78/84)   \rUnpacking objects:  94% (79/84)   \rUnpacking objects:  95% (80/84)   \rUnpacking objects:  96% (81/84)   \rUnpacking objects:  97% (82/84)   \rUnpacking objects:  98% (83/84)   \rUnpacking objects: 100% (84/84)   \rUnpacking objects: 100% (84/84), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t0Mrkwl5zH33","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":697},"executionInfo":{"status":"ok","timestamp":1593989251485,"user_tz":-120,"elapsed":9148,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"52c44a2e-8553-4e5b-e063-b0aefbfeef4a"},"source":["%cd /content/CPC_audio\n","!python setup.py develop"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/CPC_audio\n","Compiling cpc/eval/ABX/dtw.pyx because it changed.\n","[1/1] Cythonizing cpc/eval/ABX/dtw.pyx\n","running develop\n","running egg_info\n","creating CPC_audio.egg-info\n","writing CPC_audio.egg-info/PKG-INFO\n","writing dependency_links to CPC_audio.egg-info/dependency_links.txt\n","writing top-level names to CPC_audio.egg-info/top_level.txt\n","writing manifest file 'CPC_audio.egg-info/SOURCES.txt'\n","writing manifest file 'CPC_audio.egg-info/SOURCES.txt'\n","running build_ext\n","building 'cpc.eval.ABX.dtw' extension\n","creating build\n","creating build/temp.linux-x86_64-3.6\n","creating build/temp.linux-x86_64-3.6/cpc\n","creating build/temp.linux-x86_64-3.6/cpc/eval\n","creating build/temp.linux-x86_64-3.6/cpc/eval/ABX\n","x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c cpc/eval/ABX/dtw.c -o build/temp.linux-x86_64-3.6/cpc/eval/ABX/dtw.o\n","In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[Kcpc/eval/ABX/dtw.c:625\u001b[m\u001b[K:\n","\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n"," #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n","  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n","creating build/lib.linux-x86_64-3.6\n","creating build/lib.linux-x86_64-3.6/cpc\n","creating build/lib.linux-x86_64-3.6/cpc/eval\n","creating build/lib.linux-x86_64-3.6/cpc/eval/ABX\n","x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cpc/eval/ABX/dtw.o -o build/lib.linux-x86_64-3.6/cpc/eval/ABX/dtw.cpython-36m-x86_64-linux-gnu.so\n","copying build/lib.linux-x86_64-3.6/cpc/eval/ABX/dtw.cpython-36m-x86_64-linux-gnu.so -> cpc/eval/ABX\n","Creating /usr/local/lib/python3.6/dist-packages/CPC-audio.egg-link (link to .)\n","Adding CPC-audio 1.0 to easy-install.pth file\n","\n","Installed /content/CPC_audio\n","Processing dependencies for CPC-audio==1.0\n","Finished processing dependencies for CPC-audio==1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ppRP3ZqP8vZP","colab":{},"executionInfo":{"status":"ok","timestamp":1593989259420,"user_tz":-120,"elapsed":4950,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["import torch\n","import torchaudio"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YZ1Wywx3BNGS"},"source":["# Part 1 : contrastive predictive coding\n","\n","Contrastive Predictive Coding (CPC) is a method of unsupervised training for speech models. The idea behind it is pretty simple:\n","\n","\n","1.   The raw audio wave is passed through a convolutional network: the ```encoder```\n","2.   Then, the encoder's output is given to a recurrent network the ```context```\n","3. A third party network, the ```prediction_network``` will try to predict the  future embeddings of the encoder using the output of the context network.\n","\n","In order to avoid a collapse to trivial solutions, the prediction_network doesn't try to reconstruct the future features. Instead, using the context output $c_t$ at time $t$ it is trained to discriminate the real  encoder representatioin $g_{t+k}$ at time $t+k$ from several other features $(g_n)_n$ taken elsewhere in the batch. Thus the loss becomes:\n","\n","\\\\[ \\mathcal{L}_c = - \\frac{1}{K} \\sum_{k=1}^K \\text{Cross_entropy}(\\phi_k(c_t), g_{t+k}) \\\\]\n","\n","Or:\n","\n","\\\\[ \\mathcal{L}_c = - \\frac{1}{K} \\sum_{k=1}^K \\log \\frac{ \\exp\\left(\\phi_k(c_t)^\\top g_{t+k}\\right) }{  \\sum_{\\mathbf{n}\\in\\mathcal{N}_t} \\exp\\left(\\phi_k(c_t)^\\top g_n\\right)} \\\\]\n","\n","Where:\n","\n","\n","*   $\\phi_k$ is the prediction network for the kth timestep\n","*   $\\mathcal{N}_t$ is the set of all negative examples sampled for timestep $t$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"frPFYXuPfNPs"},"source":["## Exercice 1 : Building the model\n","\n","In this exercise, we will build and train a small CPC model using the repository CPC_audio.\n","\n","The code below loads a context and an encoder newtorks."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8g-xPSdLRdui","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593990889272,"user_tz":-120,"elapsed":2006,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"3fd73ab1-b668-4544-e1cb-2abf9474f5da"},"source":["%cd /content/CPC_audio\n","from cpc.model import CPCEncoder, CPCAR\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","DIM_ENCODER=256\n","DIM_CONTEXT=256\n","KEEP_HIDDEN_VECTOR=False\n","N_LEVELS_CONTEXT=1\n","CONTEXT_RNN=\"LSTM\"\n","N_PREDICTIONS=12\n","LEARNING_RATE=2e-4\n","N_NEGATIVE_SAMPLE =128"],"execution_count":25,"outputs":[{"output_type":"stream","text":["/content/CPC_audio\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7Wx8WkrQk9bQ","colab":{},"executionInfo":{"status":"ok","timestamp":1593990889274,"user_tz":-120,"elapsed":1673,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["encoder = CPCEncoder(DIM_ENCODER).to(device)\n","context = CPCAR(DIM_ENCODER, DIM_CONTEXT, KEEP_HIDDEN_VECTOR, 1, mode=CONTEXT_RNN).to(device)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"f9BrweAIla4J","colab":{},"executionInfo":{"status":"ok","timestamp":1593990889275,"user_tz":-120,"elapsed":1392,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["# Several functions that will be necessary to load the data later\n","from cpc.dataset import findAllSeqs, AudioBatchData, parseSeqLabels\n","SIZE_WINDOW = 20480\n","BATCH_SIZE=8\n","def load_dataset(path_dataset, file_extension='.wav', phone_label_dict=None):\n","  data_list, speakers = findAllSeqs(path_dataset, extension=file_extension)\n","  dataset = AudioBatchData(path_dataset, SIZE_WINDOW, data_list, phone_label_dict, len(speakers))\n","  return dataset"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"58pQ7ysXk9ZO"},"source":["Now build a new class, ```CPCModel``` which will"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rR5IYRTpRF8T","colab":{},"executionInfo":{"status":"ok","timestamp":1593990890003,"user_tz":-120,"elapsed":1352,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["class CPCModel(torch.nn.Module):\n","\n","    def __init__(self,\n","                 encoder,\n","                 AR):\n","\n","        super(CPCModel, self).__init__()\n","        self.gEncoder = encoder\n","        self.gAR = AR\n","\n","    def forward(self, batch_data):\n","        \n","\n","        encoder_output = self.gEncoder(batch_data)\n","        #print(encoder_output.shape)\n","        # The output of the encoder data does not have the good format \n","        # indeed it is Batch_size x Hidden_size x temp size\n","        # while the context requires Batch_size  x temp size x Hidden_size\n","        # thus you need to permute\n","        context_input = encoder_output.permute(0, 2, 1)\n","\n","        context_output = self.gAR(context_input)\n","        #print(context_output.shape)\n","        return context_output, encoder_output"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"7uy5kBIFzZAU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1593990890480,"user_tz":-120,"elapsed":1309,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"38ebb614-7a5d-48f5-b774-2473d6828def"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XUJgm6Rl4vS4"},"source":["Let's test your code !\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D1x1n4mv4y03","colab":{},"executionInfo":{"status":"ok","timestamp":1593990890844,"user_tz":-120,"elapsed":1040,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["audio = torchaudio.load(\"/content/drive/My Drive/speech/data/train/200702-000842_wol_4c3_elicit/200702-000842_wol_4c3_elicit_0.wav\")[0]\n","audio = audio.view(1, 1, -1)\n","cpc_model = CPCModel(encoder, context).to(device)\n","context_output, encoder_output = cpc_model(audio.to(device))"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"X27ce8Hy3C2p"},"source":["## Exercise 2 : CPC loss\n","\n","We will define a class ```CPCCriterion``` which will hold the prediction networks $\\phi_k$ defined above and perform the classification loss $\\mathcal{L}_c$.\n","\n","a) In this exercise, the $\\phi_k$ will be a linear transform, ie:\n","\n","\\\\[ \\phi_k(c_t) = \\mathbf{A}_k c_t\\\\]\n","\n","Using the class [torch.nn.Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear), define the transformations $\\phi_k$ in the code below and complete the function ```get_prediction_k``` which computes $\\phi_k(c_t)$ for a given batch of vectors $c_t$.\n","\n","b) Using both ```get_prediction_k```  and ```sample_negatives``` defined below, write the forward function which will take as input two batches of features $c_t$ and $g_t$ and outputs the classification loss $\\mathcal{L}_c$ and the average acuracy for all predictions. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UlAs-z1fBc3W","colab":{},"executionInfo":{"status":"ok","timestamp":1593990892744,"user_tz":-120,"elapsed":2029,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["# Exercice 2: write the CPC loss\n","# a) Write the negative sampling (with some help)\n","# ERRATUM: it's really hard, the sampling will be provided\n","\n","class CPCCriterion(torch.nn.Module):\n","\n","  def __init__(self,\n","               K,\n","               dim_context,\n","               dim_encoder,\n","               n_negative):\n","    super(CPCCriterion, self).__init__()\n","    self.K_ = K\n","    self.dim_context = dim_context\n","    self.dim_encoder = dim_encoder\n","    self.n_negative = n_negative\n","\n","    self.predictors = torch.nn.ModuleList() \n","    for k in range(self.K_):\n","      # TO COMPLETE !\n","      \n","      # A affine transformation in pytorch is equivalent to a nn.Linear layer\n","      # To get a linear transformation you must set bias=False\n","      # input dimension of the layer = dimension of the encoder\n","      # output dimension of the layer = dimension of the context\n","      self.predictors.append(torch.nn.Linear(dim_context, dim_encoder, bias=False))\n","\n","  def get_prediction_k(self, context_data):\n","\n","    #TO COMPLETE !\n","    output = [] \n","    # For each time step k\n","    for k in range(self.K_):\n","\n","      # We need to compute phi_k = A_k * c_t\n","      phi_k = self.predictors[k](context_data)\n","      output.append(phi_k)\n","\n","    return output\n","\n","\n","  def sample_negatives(self, encoded_data):\n","    r\"\"\"\n","    Sample some negative examples in the given encoded data.\n","    Input:\n","    - encoded_data size: B x T x H\n","    Returns\n","    - outputs of size B x (n_negative + 1) x (T - K_) x H\n","      outputs[:, 0, :, :] contains the positive example\n","      outputs[:, 1:, :, :] contains negative example sampled in the batch\n","    - labels, long tensor of size B x (T - K_)\n","      Since the positive example is always at coordinates 0 for all sequences \n","      in the batch and all timestep in the sequence, labels is just a tensor\n","      full of zeros !\n","    \"\"\"\n","    batch_size, time_size, dim_encoded = encoded_data.size()\n","    window_size = time_size - self.K_\n","    outputs = []\n","\n","    neg_ext = encoded_data.contiguous().view(-1, dim_encoded)\n","    n_elem_sampled = self.n_negative * window_size * batch_size\n","    # Draw nNegativeExt * batchSize negative samples anywhere in the batch\n","    batch_idx = torch.randint(low=0, high=batch_size,\n","                              size=(n_elem_sampled, ),\n","                              device=encoded_data.device)\n","\n","    seq_idx = torch.randint(low=1, high=time_size,\n","                            size=(n_elem_sampled, ),\n","                            device=encoded_data.device)\n","\n","    base_idx = torch.arange(0, window_size, device=encoded_data.device)\n","    base_idx = base_idx.view(1, 1, window_size)\n","    base_idx = base_idx.expand(1, self.n_negative, window_size)\n","    base_idx = base_idx.expand(batch_size, self.n_negative, window_size)\n","    seq_idx += base_idx.contiguous().view(-1)\n","    seq_idx = torch.remainder(seq_idx, time_size)\n","\n","    ext_idx = seq_idx + batch_idx * time_size\n","    neg_ext = neg_ext[ext_idx].view(batch_size, self.n_negative,\n","                                    window_size, dim_encoded)\n","    label_loss = torch.zeros((batch_size, window_size),\n","                              dtype=torch.long,\n","                              device=encoded_data.device)\n","\n","    for k in range(1, self.K_ + 1):\n","\n","      # Positive samples\n","      if k < self.K_:\n","          pos_seq = encoded_data[:, k:-(self.K_-k)]\n","      else:\n","          pos_seq = encoded_data[:, k:]\n","\n","      pos_seq = pos_seq.view(batch_size, 1, pos_seq.size(1), dim_encoded)\n","      full_seq = torch.cat((pos_seq, neg_ext), dim=1)\n","      outputs.append(full_seq)\n","\n","    return outputs, label_loss\n","\n","  def forward(self, encoded_data, context_data):\n","\n","    # TO COMPLETE:\n","    # Perform the full cpc criterion\n","    # Returns 2 values:\n","    # - the average classification loss avg_loss\n","    # - the average classification acuracy avg_acc\n","\n","    # Reminder : The permuation !\n","    encoded_data = encoded_data.permute(0, 2, 1)\n","\n","    # First we need to sample the negative examples\n","    negative_samples, labels = self.sample_negatives(encoded_data)\n","\n","    # Then we must compute phi_k\n","    phi_k = self.get_prediction_k(context_data)\n","\n","    # Finally we must get the dot product between phi_k and negative_samples \n","    # for each k\n","\n","    #The total loss is the average of all losses\n","    avg_loss = 0\n","\n","    # Average acuracy\n","    avg_acc = 0\n","\n","    for k in range(self.K_):\n","      B, N_sampled, S_small, H = negative_samples[k].size() \n","      B, S, H = phi_k[k].size()\n","\n","      # As told before S = S_small + K. For segments too far in the sequence\n","      # there are no positive exmples anyway, so we must shorten phi_k\n","      phi = phi_k[k][:, :S_small]\n","\n","      # Now the dot product\n","      # You have several ways to do that, let's do the simple but non optimal \n","      # one\n","      # pytorch has a matrix product function https://pytorch.org/docs/stable/torch.html#torch.bmm\n","      # But it takes only 3D tensors of the same batch size !\n","      # To begin negative_samples is a 4D tensor ! \n","      # We want to compute the dot product for each features, of each sequence\n","      # of the batch. Thus we are trying to compute a dot product for all\n","      # B* N_sampled * S_small 1D vector of negative_samples[k]\n","      # Or, a 1D tensor of size H is also a matrix of size 1 x H\n","      # Then, we must view it as a 3D tensor of size (B* N_sampled * S_small, 1, H)\n","      negative_sample_k  =  negative_samples[k].view(B* N_sampled* S_small, 1, H)\n","\n","      # But now phi and negative_sample_k no longer have the same batch size !\n","      # No worries, we can expand phi so that each sequence of the batch\n","      # is repeated N_sampled times\n","      phi = phi.view(B, 1,S_small, H).expand(B, N_sampled, S_small, H)\n","\n","      # And now we can view it as a 3D tensor \n","      phi  = phi.contiguous().view(B * N_sampled * S_small, H, 1)\n","\n","      # We can finally get the dot product !\n","      scores = torch.bmm(negative_sample_k, phi)\n","\n","      # Dot_product has a size (B * N_sampled * S_small , 1, 1)\n","      # Let's reorder it a bit\n","      scores = scores.reshape(B, N_sampled, S_small)\n","\n","      # For each elements of the sequence, and each elements sampled, it gives \n","      # a floating score stating the likelihood of this element being the \n","      # true one.\n","      # Now the classification loss, we need to use the Cross Entropy loss\n","      # https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html\n","\n","      # For each time-step of each sequence of the batch \n","      # we have N_sampled possible predictions. \n","      # Looking at the documentation of torch.nn.CrossEntropyLoss\n","      # we can see that this loss expect a tensor of size M x C where \n","      # - M is the number of elements with a classification score\n","      # - C is the number of possible classes\n","      # There are N_sampled candidates for each predictions so\n","      # C = N_sampled \n","      # Each timestep of each sequence of the batch has a prediction so\n","      # M = B * S_small\n","      # Thus we need an input vector of size B * S_small, N_sampled\n","      # To begin, we need to permute the axis\n","      scores = scores.permute(0, 2, 1) # Now it has size B , S_small, N_sampled\n","\n","      # Then we can cast it into a 2D tensor\n","      scores = scores.reshape(B * S_small, N_sampled)\n","\n","      # Same thing for the labels \n","      labels = labels.reshape(B * S_small)\n","\n","      # Finally we can get the classification loss\n","      loss_criterion = torch.nn.CrossEntropyLoss()\n","      loss_k = loss_criterion(scores, labels)\n","      avg_loss+= loss_k\n","\n","      # And for the acuracy\n","      # The prediction for each elements is the sample with the highest score\n","      # Thus the tensors of all predictions is the tensors of the index of the \n","      # maximal score for each time-step of each sequence of the batch\n","      predictions = torch.argmax(scores, 1)\n","      acc_k  = (labels == predictions).sum() / (B * S_small)\n","      avg_acc += acc_k\n","\n","    # Normalization\n","    avg_loss = avg_loss / self.K_\n","    avg_acc = avg_acc / self.K_\n","      \n","    return avg_loss , avg_acc"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0cqGXhLf-_O1"},"source":["Don't forget to test !"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sYJSMh5I_QCf","colab":{},"executionInfo":{"status":"ok","timestamp":1593990893156,"user_tz":-120,"elapsed":779,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["audio = torchaudio.load(\"/content/drive/My Drive/speech/data/train/200702-000842_wol_4c3_elicit/200702-000842_wol_4c3_elicit_0.wav\")[0]\n","audio = audio.view(1, 1, -1)\n","cpc_criterion = CPCCriterion(N_PREDICTIONS, DIM_CONTEXT, \n","                             DIM_ENCODER, N_NEGATIVE_SAMPLE).to(device)\n","context_output, encoder_output = cpc_model(audio.to(device))\n","loss, avg = cpc_criterion(encoder_output,context_output)"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zLv7GoE0_4C3"},"source":["## Exercise 3: Full training loop !\n","\n","You have the model, you have the criterion. All you need now are a data loader and an optimizer to run your training loop.\n","\n","We will use an Adam optimizer:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zcg29tqPAIR1","colab":{},"executionInfo":{"status":"ok","timestamp":1593990898319,"user_tz":-120,"elapsed":1179,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["parameters = list(cpc_criterion.parameters()) + list(cpc_model.parameters())\n","optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hETDwSfTAuF4"},"source":["And as far as the data loader is concerned, we will rely on the data loader provided by the CPC_audio library."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"O6KES4RXA0tU","colab":{"base_uri":"https://localhost:8080/","height":330},"executionInfo":{"status":"ok","timestamp":1593990905479,"user_tz":-120,"elapsed":7261,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"64e3f148-0fe4-4b71-aa78-88d423a476a2"},"source":["dataset_train = load_dataset('/content/drive/My Drive/speech/data/train')\n","dataset_val = load_dataset('/content/drive/My Drive/speech/data/validation')\n","data_loader_train = dataset_train.getDataLoader(BATCH_SIZE, \"speaker\", True)\n","data_loader_val = dataset_train.getDataLoader(BATCH_SIZE, \"sequence\", False)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["6it [00:00, 470.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Saved cache file at /content/drive/My Drive/speech/data/train/_seqs_cache.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Checking length...\n"],"name":"stdout"},{"output_type":"stream","text":["\r242it [00:00, 118952.49it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Done, elapsed: 0.213 seconds\n","Scanned 242 sequences in 0.21 seconds\n","1 chunks computed\n","Joining pool\n"],"name":"stdout"},{"output_type":"stream","text":["4it [00:00, 68.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Joined process, elapsed=0.928 secs\n","Saved cache file at /content/drive/My Drive/speech/data/validation/_seqs_cache.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n","147it [00:00, 40005.37it/s]"],"name":"stderr"},{"output_type":"stream","text":["Checking length...\n","Done, elapsed: 0.146 seconds\n","Scanned 147 sequences in 0.15 seconds\n","1 chunks computed\n","Joining pool\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Joined process, elapsed=1.358 secs\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uCoMCPL0A8VI"},"source":["Now that everything is ready, complete and test the ```train_step``` function below which trains the model for one epoch."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"U-hIH3p8BsZr","colab":{},"executionInfo":{"status":"ok","timestamp":1593990910150,"user_tz":-120,"elapsed":908,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["def train_step(data_loader,\n","               cpc_model,\n","               cpc_criterion,\n","               optimizer):\n","  \n","  avg_loss = 0\n","  avg_acc = 0\n","  n_items = 0\n","\n","  for step, data in enumerate(data_loader):\n","    x,y = data\n","    bs = len(x)\n","    optimizer.zero_grad()\n","    context_output, encoder_output = cpc_model(x.to(device))\n","    loss , acc = cpc_criterion(encoder_output, context_output)\n","    loss.backward()\n","    n_items+=bs\n","    avg_loss+=loss.item()*bs\n","    avg_acc +=acc.item()*bs\n","  \n","  avg_loss/=n_items\n","  avg_acc/=n_items\n","  return avg_loss, avg_acc"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rIj06giJE_bZ"},"source":["## Exercise 4 : Validation loop\n","\n","Now complete the validation loop."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q7Qwi6jDByyt","colab":{},"executionInfo":{"status":"ok","timestamp":1593990911458,"user_tz":-120,"elapsed":1203,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["def validation_step(data_loader,\n","                    cpc_model,\n","                    cpc_criterion):\n","  \n","  avg_loss = 0\n","  avg_acc = 0\n","  n_items = 0\n","\n","  for step, data in enumerate(data_loader):\n","    x,y = data\n","    bs = len(x)\n","    context_output, encoder_output = cpc_model(x.to(device))\n","    loss , acc = cpc_criterion(encoder_output, context_output)\n","    n_items+=bs\n","    avg_loss+=loss.item()*bs\n","    avg_acc+=acc.item()*bs\n","  \n","  avg_loss/=n_items\n","  avg_acc/=n_items\n","  return avg_loss, avg_acc"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OBVUPKKs2_0U"},"source":["## Exercise 5: Run everything"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZbXsZIRiB1tm","colab":{},"executionInfo":{"status":"ok","timestamp":1593990912822,"user_tz":-120,"elapsed":583,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["def run(train_loader,\n","        val_loader,\n","        cpc_model,\n","        cpc_criterion,\n","        optimizer,\n","        n_epochs):\n","  \n","  for epoch in range(n_epochs):\n","\n","    \n","    print(f\"Running epoch {epoch+1} / {n_epochs}\")\n","    avg_loss_train, avg_acc_train = train_step(train_loader, cpc_model, cpc_criterion, optimizer)\n","    print(\"----------------------\")\n","    print(f\"Training dataset\")\n","    print(f\"- average loss : {avg_loss_train}\")\n","    print(f\"- average acuracy : {avg_acc_train}\")\n","    print(\"----------------------\")\n","    with torch.no_grad():\n","      cpc_model.eval()\n","      cpc_criterion.eval()\n","      avg_loss_val, avg_acc_val = validation_step(val_loader, cpc_model, cpc_criterion)\n","      print(f\"Validation dataset\")\n","      print(f\"- average loss : {avg_loss_val}\")\n","      print(f\"- average acuracy : {avg_acc_val}\")\n","      print(\"----------------------\")\n","      print()\n","      cpc_model.train()\n","      cpc_criterion.train()"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5xx8vN2wpECC","colab":{"base_uri":"https://localhost:8080/","height":208},"executionInfo":{"status":"ok","timestamp":1593991001765,"user_tz":-120,"elapsed":88221,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"b71433dd-8081-4d11-de20-78e8a4fa3acd"},"source":["run(data_loader_train, data_loader_val, cpc_model,cpc_criterion,optimizer,1)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Running epoch 1 / 1\n","----------------------\n","Training dataset\n","- average loss : 4.860483307262947\n","- average acuracy : 0.0\n","----------------------\n","Validation dataset\n","- average loss : 4.86048201856942\n","- average acuracy : 0.0\n","----------------------\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5hoT3_3W6HYY"},"source":["Once everything is donw, clear the memory."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fU5mDOY46KSG","colab":{},"executionInfo":{"status":"ok","timestamp":1593991007812,"user_tz":-120,"elapsed":991,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["del dataset_train\n","del dataset_val\n","del cpc_model\n","del context\n","del encoder"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"srPM5r_LB9v-"},"source":["# Part 2 : Fine tuning"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0Nb_-0IQiJk9"},"source":["## Exercice 1 : Phone separability with aligned phonemes.\n","\n","One option to evaluate the quality of the features trained with CPC can be to check if they can be used to recognize phonemes. \n","To do so, we can fine-tune a pre-trained model using a limited amount of labelled speech data.\n","We are going to start with a simple evaluation setting where we have the phone labels for each timestep corresponding to a CPC feature.\n","\n","We will work with a model already pre-trained on English data. As far as the fine-tuning dataset is concerned, we will use a 1h subset of [librispeech-100](http://www.openslr.org/12/). "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"N-scDMAasXxc","colab":{"base_uri":"https://localhost:8080/","height":662},"executionInfo":{"status":"ok","timestamp":1593991025307,"user_tz":-120,"elapsed":17353,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"87b4aca4-45f0-4bbb-bc48-ff7c39559e46"},"source":["!mkdir checkpoint_data\n","!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_30.pt -P checkpoint_data\n","!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_logs.json -P checkpoint_data\n","!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_args.json -P checkpoint_data\n","!ls checkpoint_data"],"execution_count":40,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘checkpoint_data’: File exists\n","--2020-07-05 23:16:53--  https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_30.pt\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 113599715 (108M) [application/octet-stream]\n","Saving to: ‘checkpoint_data/checkpoint_30.pt.1’\n","\n","checkpoint_30.pt.1  100%[===================>] 108.34M  93.1MB/s    in 1.2s    \n","\n","2020-07-05 23:16:54 (93.1 MB/s) - ‘checkpoint_data/checkpoint_30.pt.1’ saved [113599715/113599715]\n","\n","--2020-07-05 23:16:56--  https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_logs.json\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 20786 (20K) [text/plain]\n","Saving to: ‘checkpoint_data/checkpoint_logs.json.1’\n","\n","checkpoint_logs.jso 100%[===================>]  20.30K  --.-KB/s    in 0.01s   \n","\n","2020-07-05 23:16:56 (1.85 MB/s) - ‘checkpoint_data/checkpoint_logs.json.1’ saved [20786/20786]\n","\n","--2020-07-05 23:16:59--  https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_args.json\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2063 (2.0K) [text/plain]\n","Saving to: ‘checkpoint_data/checkpoint_args.json.1’\n","\n","checkpoint_args.jso 100%[===================>]   2.01K  --.-KB/s    in 0s      \n","\n","2020-07-05 23:16:59 (37.0 MB/s) - ‘checkpoint_data/checkpoint_args.json.1’ saved [2063/2063]\n","\n","checkpoint_30.pt    checkpoint_args.json    checkpoint_logs.json\n","checkpoint_30.pt.1  checkpoint_args.json.1  checkpoint_logs.json.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SSSaiYo82_oY","colab":{"base_uri":"https://localhost:8080/","height":610},"executionInfo":{"status":"error","timestamp":1593991068722,"user_tz":-120,"elapsed":4952,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"a70f1ab1-2e18-4354-9201-9d93499b7aee"},"source":["%cd /content/CPC_audio\n","from cpc.dataset import parseSeqLabels\n","from cpc.feature_loader import loadModel\n","\n","checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n","cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n","cpc_model = cpc_model.cuda()\n","label_dict, N_PHONES = parseSeqLabels('/content/drive/My Drive/speech/all_sessions.txt')\n","dataset_train = load_dataset('/content/drive/My Drive/speech/data/train', file_extension='.wav', phone_label_dict=label_dict)\n","dataset_val = load_dataset('/content/drive/My Drive/speech/data/validation', file_extension='.wav', phone_label_dict=label_dict)\n","data_loader_train = dataset_train.getDataLoader(BATCH_SIZE, \"speaker\", True)\n","data_loader_val = dataset_val.getDataLoader(BATCH_SIZE, \"sequence\", False)"],"execution_count":44,"outputs":[{"output_type":"stream","text":["/content/CPC_audio\n","Loading checkpoint checkpoint_data/checkpoint_30.pt\n","Loading the state dict at checkpoint_data/checkpoint_30.pt\n"],"name":"stdout"},{"output_type":"stream","text":["6it [00:00, 551.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Saved cache file at /content/drive/My Drive/speech/data/train/_seqs_cache.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Checking length...\n"],"name":"stdout"},{"output_type":"stream","text":["242it [00:00, 104469.08it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Done, elapsed: 0.261 seconds\n","Scanned 242 sequences in 0.26 seconds\n","1 chunks computed\n","Joining pool\n","Joined process, elapsed=1.067 secs\n"],"name":"stdout"},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-dde93103724c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcpc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlabel_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_PHONES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparseSeqLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/speech/all_sessions.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/speech/data/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_extension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphone_label_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdataset_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/speech/data/validation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_extension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphone_label_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdata_loader_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"speaker\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-e313e61b0ebf>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path_dataset, file_extension, phone_label_dict)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_extension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphone_label_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeakers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindAllSeqs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_extension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioBatchData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIZE_WINDOW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphone_label_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeakers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/CPC_audio/cpc/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, sizeWindow, seqNames, phoneLabelsDict, nSpeakers, nProcessLoader, MAX_SIZE_LOADED)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphoneLabelsDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoneLabelsDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadNextPack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadNextPack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoubleLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/CPC_audio/cpc/dataset.py\u001b[0m in \u001b[0;36mloadNextPack\u001b[0;34m(self, first)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Joined process, elapsed={time.time()-start_time:.3f} secs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseNextDataBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnextPack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentPack\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackageIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/CPC_audio/cpc/dataset.py\u001b[0m in \u001b[0;36mparseNextDataBlock\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphoneLabelsDict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphoneLabels\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphoneLabelsDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseqName\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0mnewSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphoneLabelsDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseqName\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphoneSize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnewSize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: '200702-000842_wol_4c3_elicit_49'"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3ulgYV3nHcoa","colab":{},"executionInfo":{"status":"aborted","timestamp":1593991029670,"user_tz":-120,"elapsed":17332,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["??cpc_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xkKi-qfosng2"},"source":["Then we will use a simple linear classifier to recognize the phonemes from the features produced by ```cpc_model```. \n","\n","### a) Build the phone classifier \n","\n","Design a class of linear classifiers, ```PhoneClassifier``` that will take as input a batch of sequences of CPC features and output a score vector for each phoneme"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4RpAbz-0CXJJ","colab":{},"executionInfo":{"status":"aborted","timestamp":1593991029673,"user_tz":-120,"elapsed":16087,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["class PhoneClassifier(torch.nn.Module):\n","\n","  def __init__(self,\n","               input_dim : int,\n","               n_phones : int):\n","    super(PhoneClassifier, self).__init__()\n","    self.linear = torch.nn.Linear(input_dim, n_phones)\n","    \n","\n","  def forward(self, x):\n","    return self.linear(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Zt5oa_nqtH-d"},"source":["Our phone classifier will then be:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NRBf_83IuLv5","colab":{"base_uri":"https://localhost:8080/","height":167},"executionInfo":{"status":"error","timestamp":1593991052241,"user_tz":-120,"elapsed":770,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"ebd9ac2d-a4fc-47f2-d6c6-c53417813de3"},"source":["phone_classifier = PhoneClassifier(HIDDEN_CONTEXT_MODEL, N_PHONES).to(device)"],"execution_count":43,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-1fdb817d68c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mphone_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhoneClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHIDDEN_CONTEXT_MODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_PHONES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'PhoneClassifier' is not defined"]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z_Vf5AbUhqm4"},"source":["### b - What would be the correct loss criterion for this task ?\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uhyPM-cgjrtw","colab":{},"executionInfo":{"status":"aborted","timestamp":1593991029674,"user_tz":-120,"elapsed":13640,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["loss_criterion = torch.nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Nv4cSxbaplrz"},"source":["To perform the fine-tuning, we will also need an optimization function.\n","\n","We will use an [Adam optimizer ](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"W5CgYyAlqKxu","colab":{},"executionInfo":{"status":"aborted","timestamp":1593991029675,"user_tz":-120,"elapsed":10928,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["parameters = list(phone_classifier.parameters()) + list(cpc_model.parameters())\n","LEARNING_RATE = 2e-4\n","optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qQB9HS9PvAXc"},"source":["You might also want to perform this training while freezing the weights of the ```cpc_model```. Indeed, if the pre-training was good enough, then ```cpc_model``` phonemes representation should be linearly separable. In this case the optimizer should be defined like this:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nRy0gn6awGUQ","colab":{},"executionInfo":{"status":"aborted","timestamp":1593991029676,"user_tz":-120,"elapsed":9058,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["optimizer_frozen = torch.optim.Adam(list(phone_classifier.parameters()), lr=LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cO93ngIfj4JW"},"source":["### c- Now let's build a training loop. \n","Complete the function ```train_one_epoch``` below.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fabqj3wvLwgU","colab":{},"executionInfo":{"status":"aborted","timestamp":1593991029677,"user_tz":-120,"elapsed":7806,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["def train_one_epoch(cpc_model, \n","                    phone_classifier, \n","                    loss_criterion, \n","                    data_loader, \n","                    optimizer):\n","\n","  cpc_model.train()\n","  loss_criterion.train()\n","\n","  avg_loss = 0\n","  avg_accuracy = 0\n","  n_items = 0\n","  for step, full_data in enumerate(data_loader):\n","    # Each batch is represented by a Tuple of vectors:\n","    # sequence of size : N x 1 x T\n","    # label of size : N x T\n","    # \n","    # With :\n","    # - N number of sequence in the batch\n","    # - T size of each sequence\n","    sequence, label = full_data\n","    \n","    \n","\n","    bs = len(sequence)\n","    seq_len = label.size(1)\n","    optimizer.zero_grad()\n","    context_out, enc_out, _ = cpc_model(sequence.to(device),label.to(device))\n","\n","    scores = phone_classifier(context_out)\n","\n","    scores = scores.permute(0,2,1)\n","    loss = loss_criterion(scores,label.to(device))\n","    loss.backward()\n","    optimizer.step()\n","    avg_loss+=loss.item()*bs\n","    n_items+=bs\n","    correct_labels = scores.argmax(1)\n","    avg_accuracy += ((label==correct_labels.cpu()).float()).mean(1).sum().item()\n","  avg_loss/=n_items\n","  avg_accuracy/=n_items\n","  return avg_loss, avg_accuracy\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"quYtjx_TxIPK"},"source":["Don't forget to test it !"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"50MwxbKhxMKp","colab":{},"executionInfo":{"status":"aborted","timestamp":1593991029677,"user_tz":-120,"elapsed":6859,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["avg_loss, avg_accuracy = train_one_epoch(cpc_model, phone_classifier, loss_criterion, data_loader_train, optimizer_frozen)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_o6yk8XKWnYe","colab":{},"executionInfo":{"status":"aborted","timestamp":1593991029678,"user_tz":-120,"elapsed":5996,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["avg_loss, avg_accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EmUkuJ2bwu4Z"},"source":["### d- Build the validation loop"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kZJMxj6cwzd3","colab":{},"executionInfo":{"status":"aborted","timestamp":1593991029680,"user_tz":-120,"elapsed":4471,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["def validation_step(cpc_model, \n","                    phone_classifier, \n","                    loss_criterion, \n","                    data_loader):\n","  \n","  cpc_model.eval()\n","  phone_classifier.eval()\n","\n","  avg_loss = 0\n","  avg_accuracy = 0\n","  n_items = 0\n","  with torch.no_grad():\n","    for step, full_data in enumerate(data_loader):\n","      # Each batch is represented by a Tuple of vectors:\n","      # sequence of size : N x 1 x T\n","      # label of size : N x T\n","      # \n","      # With :\n","      # - N number of sequence in the batch\n","      # - T size of each sequence\n","      sequence, label = full_data\n","      bs = len(sequence)\n","      seq_len = label.size(1)\n","      context_out, enc_out, _ = cpc_model(sequence.to(device),label.to(device))\n","      scores = phone_classifier(context_out)\n","      scores = scores.permute(0,2,1)\n","      loss = loss_criterion(scores,label.to(device))\n","      avg_loss+=loss.item()*bs\n","      n_items+=bs\n","      correct_labels = scores.argmax(1)\n","      avg_accuracy += ((label==correct_labels.cpu()).float()).mean(1).sum().item()\n","  avg_loss/=n_items\n","  avg_accuracy/=n_items\n","  return avg_loss, avg_accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vownVCt7xbVh"},"source":["### e- Run everything\n","\n","Test this functiion with both ```optimizer``` and ```optimizer_frozen```."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xvO_4nKUxfQx","colab":{},"executionInfo":{"status":"aborted","timestamp":1593991029680,"user_tz":-120,"elapsed":2119,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["def run(cpc_model, \n","        phone_classifier, \n","        loss_criterion, \n","        data_loader_train, \n","        data_loader_val, \n","        optimizer,\n","        n_epoch):\n","\n","  for epoch in range(n_epoch):\n","\n","    print(f\"Running epoch {epoch + 1} / {n_epoch}\")\n","    loss_train, acc_train = train_one_epoch(cpc_model, phone_classifier, loss_criterion, data_loader_train, optimizer)\n","    print(\"-------------------\")\n","    print(f\"Training dataset :\")\n","    print(f\"Average loss : {loss_train}. Average accuracy {acc_train}\")\n","\n","    print(\"-------------------\")\n","    print(\"Validation dataset\")\n","    loss_val, acc_val = validation_step(cpc_model, phone_classifier, loss_criterion, data_loader_val)\n","    print(f\"Average loss : {loss_val}. Average accuracy {acc_val}\")\n","    print(\"-------------------\")\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ceCEO2h2bxAn","colab":{"base_uri":"https://localhost:8080/","height":167},"executionInfo":{"status":"error","timestamp":1593991031812,"user_tz":-120,"elapsed":1364,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"68aa5d89-5feb-49e7-e05c-4a0464035a02"},"source":["run(cpc_model,phone_classifier,loss_criterion,data_loader_train,data_loader_val,optimizer_frozen,n_epoch=10)"],"execution_count":42,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-e8d0288bf866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpc_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphone_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_criterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_loader_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_frozen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'phone_classifier' is not defined"]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TdfWDiFnylMT"},"source":["## Exercise 2 : Phone separability without alignment (PER)\n","\n","Aligned data are very practical, but un real life they are rarely available. That's why in this excercise we will consider a fine-tuning with non-aligned phonemes.\n","\n","The model, the optimizer and the phone classifier will stay the same. However, we will replace our phone criterion with a [CTC loss](https://pytorch.org/docs/master/generated/torch.nn.CTCLoss.html). "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_9BpM_Lpzgx8","colab":{},"executionInfo":{"status":"ok","timestamp":1593900047442,"user_tz":-120,"elapsed":905,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["loss_ctc = torch.nn.CTCLoss()"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AQpYgTyfzsrq"},"source":["Besides, we will use a siglthy different dataset class."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9HRxoatlz3ZZ","colab":{"base_uri":"https://localhost:8080/","height":278},"executionInfo":{"status":"ok","timestamp":1593900323045,"user_tz":-120,"elapsed":5136,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"2e764ca0-21a7-4393-c2cb-857dc66d3193"},"source":["%cd /content/CPC_audio\n","from cpc.eval.common_voices_eval import SingleSequenceDataset, parseSeqLabels, findAllSeqs\n","\n","path_train_data_per = '/content/drive/My Drive/speech/data/train'\n","path_val_data_per = '/content/drive/My Drive/speech/data/validation'\n","path_test_data_per = '/content/drive/My Drive/speech/data/test'\n","path_phone_data_per = '/content/drive/My Drive/speech/all_sessions.txt'\n","BATCH_SIZE=8\n","\n","phone_labels, N_PHONES = parseSeqLabels(path_phone_data_per)\n","data_train_per, _ = findAllSeqs(path_train_data_per, extension='.wav')\n","dataset_train_non_aligned = SingleSequenceDataset(path_train_data_per, data_train_per, phone_labels)\n","data_loader_train = torch.utils.data.DataLoader(dataset_train_non_aligned, batch_size=BATCH_SIZE,\n","                                                shuffle=True)\n","\n","data_val_per, _ = findAllSeqs(path_val_data_per, extension='.wav')\n","dataset_val_non_aligned = SingleSequenceDataset(path_val_data_per, data_val_per, phone_labels)\n","data_loader_val = torch.utils.data.DataLoader(dataset_val_non_aligned, batch_size=BATCH_SIZE,\n","                                              shuffle=True)"],"execution_count":50,"outputs":[{"output_type":"stream","text":["4it [00:00, 583.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/CPC_audio\n","Saved cache file at /content/drive/My Drive/speech/data/train/_seqs_cache.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n","4it [00:00, 368.16it/s]"],"name":"stderr"},{"output_type":"stream","text":["Loaded 143 sequences in 1.74 seconds\n","maxSizeSeq : 323433\n","maxSizePhone : 128\n","minSizePhone : 12\n","Total size dataset 0.39825538194444443 hours\n","Saved cache file at /content/drive/My Drive/speech/data/validation/_seqs_cache.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded 143 sequences in 1.74 seconds\n","maxSizeSeq : 323433\n","maxSizePhone : 128\n","minSizePhone : 12\n","Total size dataset 0.39825538194444443 hours\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GwAckY62z7s9"},"source":["### a- Training\n","\n","Since the phonemes are not aligned, there is no simple direct way to get the classification acuracy of a model. Write and test the three functions ```train_one_epoch_ctc```, ```validation_step_ctc``` and ```run_ctc``` as before but without considering the average acuracy of the model. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oYg5YzW8EHl4","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593900344260,"user_tz":-120,"elapsed":1253,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"799ecc19-9326-43eb-d370-6620b718b9a3"},"source":["from cpc.feature_loader import loadModel\n","\n","checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n","cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n","cpc_model = cpc_model.cuda()\n","phone_classifier = PhoneClassifier(HIDDEN_CONTEXT_MODEL, N_PHONES).to(device)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["Loading checkpoint checkpoint_data/checkpoint_30.pt\n","Loading the state dict at checkpoint_data/checkpoint_30.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CFQ2g3PjErdZ","colab":{},"executionInfo":{"status":"ok","timestamp":1593900346853,"user_tz":-120,"elapsed":1442,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["parameters = list(phone_classifier.parameters()) + list(cpc_model.parameters())\n","LEARNING_RATE = 2e-4\n","optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n","\n","optimizer_frozen = torch.optim.Adam(list(phone_classifier.parameters()), lr=LEARNING_RATE)"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Zsgjv3cD0oqD","colab":{},"executionInfo":{"status":"ok","timestamp":1593900346856,"user_tz":-120,"elapsed":933,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["import torch.nn.functional as F\n","\n","def train_one_epoch_ctc(cpc_model, \n","                        phone_classifier, \n","                        loss_criterion, \n","                        data_loader, \n","                        optimizer):\n","  \n","  cpc_model.train()\n","  loss_criterion.train()\n","\n","  avg_loss = 0\n","  avg_accuracy = 0\n","  n_items = 0\n","  for step, full_data in enumerate(data_loader):\n","\n","    x, x_len, y, y_len = full_data\n","\n","    x_batch_len = x.shape[-1]\n","    x, y = x.to(device), y.to(device)\n","\n","    bs=x.size(0)\n","    optimizer.zero_grad()\n","    context_out, enc_out, _ = cpc_model(x.to(device),y.to(device))\n","  \n","    scores = phone_classifier(context_out)\n","    scores = scores.permute(1,0,2)\n","    scores = F.log_softmax(scores,2)\n","    yhat_len = torch.tensor([int(scores.shape[0]*x_len[i]/x_batch_len) for i in range(scores.shape[1])]) # this is an approximation, should be good enough\n","\n","    loss = loss_criterion(scores,y.to(device),yhat_len,y_len)\n","    loss.backward()\n","    optimizer.step()\n","    avg_loss+=loss.item()*bs\n","    n_items+=bs\n","  avg_loss/=n_items\n","  return avg_loss\n","\n","def validation_step(cpc_model, \n","                    phone_classifier, \n","                    loss_criterion, \n","                    data_loader):\n","\n","  cpc_model.eval()\n","  phone_classifier.eval()\n","  avg_loss = 0\n","  avg_accuracy = 0\n","  n_items = 0\n","  with torch.no_grad():\n","    for step, full_data in enumerate(data_loader):\n","\n","      x, x_len, y, y_len = full_data\n","\n","      x_batch_len = x.shape[-1]\n","      x, y = x.to(device), y.to(device)\n","\n","      bs=x.size(0)\n","      context_out, enc_out, _ = cpc_model(x.to(device),y.to(device))\n","    \n","      scores = phone_classifier(context_out)\n","      scores = scores.permute(1,0,2)\n","      scores = F.log_softmax(scores,2)\n","      yhat_len = torch.tensor([int(scores.shape[0]*x_len[i]/x_batch_len) for i in range(scores.shape[1])]) # this is an approximation, should be good enough\n","\n","      loss = loss_criterion(scores,y.to(device),yhat_len,y_len)\n","      avg_loss+=loss.item()*bs\n","      n_items+=bs\n","  avg_loss/=n_items\n","\n","  return avg_loss\n","\n","def run_ctc(cpc_model, \n","            phone_classifier, \n","            loss_criterion, \n","            data_loader_train, \n","            data_loader_val, \n","            optimizer,\n","            n_epoch):\n","  for epoch in range(n_epoch):\n","\n","    print(f\"Running epoch {epoch + 1} / {n_epoch}\")\n","    loss_train = train_one_epoch_ctc(cpc_model, phone_classifier, loss_criterion, data_loader_train, optimizer)\n","    print(\"-------------------\")\n","    print(f\"Training dataset :\")\n","    print(f\"Average loss : {loss_train}.\")\n","\n","    print(\"-------------------\")\n","    print(\"Validation dataset\")\n","    loss_val = validation_step(cpc_model, phone_classifier, loss_criterion, data_loader_val)\n","    print(f\"Average loss : {loss_val}\")\n","    print(\"-------------------\")\n","    print()"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GSr7tcUdD72c","colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"status":"error","timestamp":1593900369643,"user_tz":-120,"elapsed":23563,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"7570c107-d231-4bee-db5f-eed4d156c6f4"},"source":["run_ctc(cpc_model,phone_classifier,loss_ctc,data_loader_train,data_loader_val,optimizer_frozen,n_epoch=10)"],"execution_count":54,"outputs":[{"output_type":"stream","text":["Running epoch 1 / 10\n","-------------------\n","Training dataset :\n","Average loss : 72.6621567632111.\n","-------------------\n","Validation dataset\n","Average loss : 71.02616280569157\n","-------------------\n","\n","Running epoch 2 / 10\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-54-9c805bbf3104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_ctc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpc_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphone_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_ctc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_loader_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_frozen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-53-5b010bdd3f37>\u001b[0m in \u001b[0;36mrun_ctc\u001b[0;34m(cpc_model, phone_classifier, loss_criterion, data_loader_train, data_loader_val, optimizer, n_epoch)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Running epoch {epoch + 1} / {n_epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch_ctc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpc_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphone_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training dataset :\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-53-5b010bdd3f37>\u001b[0m in \u001b[0;36mtrain_one_epoch_ctc\u001b[0;34m(cpc_model, phone_classifier, loss_criterion, data_loader, optimizer)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mcontext_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpc_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphone_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/CPC_audio/cpc/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batchData, label)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mencodedData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mcFeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgAR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencodedData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/CPC_audio/cpc/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeepHidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 570\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TKrYW4gK1BBF"},"source":["### b- Evaluation: the Phone Error Rate (PER)\n","\n","In order to compute the similarity between two sequences, we can use the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance). This distance estimates the minimum number of insertion, deletion and addition to move from one sequence to another. If we normalize this distance by the number of characters in the reference sequence we get the Phone Error Rate (PER).\n","\n","This value can be interpreted as :\n","\\\\[  PER = \\frac{S + D + I}{N} \\\\]\n","\n","Where:\n","\n","\n","*   N is the number of characters in the reference\n","*   S is the number of substitutiion\n","*   I in the number of insertion\n","*   D in the number of deletion\n","\n","For the best possible alignment of the two sequences.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RoBhsx7GNqI_","colab":{},"executionInfo":{"status":"ok","timestamp":1593900376153,"user_tz":-120,"elapsed":1343,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["import numpy as np\n","\n","def get_PER_sequence(ref_seq, target_seq):\n","\n","  # re = g.split()\n","  # h = h.split()\n","  n = len(ref_seq)\n","  m = len(target_seq)\n","\n","  D = np.zeros((n+1,m+1))\n","  for i in range(1,n+1):\n","    D[i,0] = D[i-1,0]+1\n","  for j in range(1,m+1):\n","    D[0,j] = D[0,j-1]+1\n","  \n","  ### TODO compute the alignment\n","\n","  for i in range(1,n+1):\n","    for j in range(1,m+1):\n","      D[i,j] = min(\n","          D[i-1,j]+1,\n","          D[i-1,j-1]+1,\n","          D[i,j-1]+1,\n","          D[i-1,j-1]+ 0 if ref_seq[i-1]==target_seq[j-1] else float(\"inf\")\n","      )\n","  return D[n,m]/len(ref_seq)\n","  \n","\n","  #return PER"],"execution_count":55,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r-hr0KK0mgcR"},"source":["You can test your function below:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AfTb3yOQmvey","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593900383710,"user_tz":-120,"elapsed":2249,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"6bb72841-330e-4b40-8316-75cc2af70634"},"source":["ref_seq = [0, 1, 1, 2, 0, 2, 2]\n","pred_seq = [1, 1, 2, 2, 0, 0]\n","\n","expected_PER = 4. / 7.\n","print(get_PER_sequence(ref_seq, pred_seq) == expected_PER)"],"execution_count":56,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nHiyChl-m_k7"},"source":["## c- Evaluating the PER of your model on the test dataset\n","\n","Evaluate the PER on the validation dataset. Please notice that you should usually use a separate dataset, called the dev dataset, to perform this operation. However for the sake of simplicity we will work with validation data in this exercise."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DMkX0PoFnclg","colab":{},"executionInfo":{"status":"ok","timestamp":1593900385658,"user_tz":-120,"elapsed":648,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["import progressbar\n","from multiprocessing import Pool\n","\n","def cut_data(seq, sizeSeq):\n","    maxSeq = sizeSeq.max()\n","    return seq[:, :maxSeq]\n","\n","\n","def prepare_data(data):\n","    seq, sizeSeq, phone, sizePhone = data\n","    seq = seq.cuda()\n","    phone = phone.cuda()\n","    sizeSeq = sizeSeq.cuda().view(-1)\n","    sizePhone = sizePhone.cuda().view(-1)\n","\n","    seq = cut_data(seq.permute(0, 2, 1), sizeSeq).permute(0, 2, 1)\n","    return seq, sizeSeq, phone, sizePhone\n","\n","\n","def get_per(test_dataloader,\n","            cpc_model,\n","            phone_classifier):\n","\n","  downsampling_factor = 160\n","  cpc_model.eval()\n","  phone_classifier.eval()\n","\n","  avgPER = 0\n","  nItems = 0 \n","\n","  print(\"Starting the PER computation through beam search\")\n","  bar = progressbar.ProgressBar(maxval=len(test_dataloader))\n","  bar.start()\n","\n","  for index, data in enumerate(test_dataloader):\n","\n","    bar.update(index)\n","\n","    with torch.no_grad():\n","      \n","        seq, sizeSeq, phone, sizePhone = prepare_data(data)\n","        c_feature, _, _ = cpc_model(seq.to(device),phone.to(device))\n","        sizeSeq = sizeSeq / downsampling_factor\n","        predictions = torch.nn.functional.softmax(\n","        phone_classifier(c_feature), dim=2).cpu()\n","        phone = phone.cpu()\n","        sizeSeq = sizeSeq.cpu()\n","        sizePhone = sizePhone.cpu()\n","\n","        bs = c_feature.size(0)\n","        data_per = [(predictions[b].argmax(1),  phone[b]) for b in range(bs)]\n","        # data_per = [(predictions[b], sizeSeq[b], phone[b], sizePhone[b],\n","        #               \"criterion.module.BLANK_LABEL\") for b in range(bs)]\n","\n","        with Pool(bs) as p:\n","            poolData = p.starmap(get_PER_sequence, data_per)\n","        avgPER += sum([x for x in poolData])\n","        nItems += len(poolData)\n","\n","  bar.finish()\n","\n","  avgPER /= nItems\n","\n","  print(f\"Average PER {avgPER}\")\n","  return avgPER\n"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2hvnudh4Osb4","colab":{"base_uri":"https://localhost:8080/","height":538},"executionInfo":{"status":"error","timestamp":1593900425781,"user_tz":-120,"elapsed":38922,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"08a11080-8b31-4683-e64d-5cc5d86dfc24"},"source":["get_per(data_loader_val,cpc_model,phone_classifier)"],"execution_count":58,"outputs":[{"output_type":"stream","text":["\r                                                                               \r\rN/A% (0 of 18) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"],"name":"stderr"},{"output_type":"stream","text":["Starting the PER computation through beam search\n"],"name":"stdout"},{"output_type":"stream","text":[" 11% (2 of 18) |##                       | Elapsed Time: 0:00:29 ETA:   0:03:06Process ForkPoolWorker-478:\n","Process ForkPoolWorker-482:\n","Process ForkPoolWorker-484:\n","Process ForkPoolWorker-483:\n","Process ForkPoolWorker-480:\n","Traceback (most recent call last):\n","Process ForkPoolWorker-481:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","Traceback (most recent call last):\n","Process ForkPoolWorker-477:\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-57a7b60c97d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_per\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcpc_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphone_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-57-0484ee535d08>\u001b[0m in \u001b[0;36mget_per\u001b[0;34m(test_dataloader, cpc_model, phone_classifier)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mpoolData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_PER_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_per\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mavgPER\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpoolData\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mnItems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoolData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         '''\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"p8e9D7g8159k"},"source":["## Exercice 3 : Character error rate (CER) \n","\n","The Character Error Rate (CER) is an evaluation metric similar to the PER but with characters insterad of phonemes. Using the following data, run the functions you defined previously to estimate the CER of your model after fine-tuning."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cXONmKQOuFSn","colab":{"base_uri":"https://localhost:8080/","height":278},"executionInfo":{"status":"ok","timestamp":1593900611308,"user_tz":-120,"elapsed":5083,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"f0b7df23-2a52-4ce7-8623-19c20a866c86"},"source":["# Load a dataset labelled with the letters of each sequence.\n","%cd /content/CPC_audio\n","from cpc.eval.common_voices_eval import SingleSequenceDataset, parseSeqLabels, findAllSeqs\n"," \n","path_train_data_cer = '/content/drive/My Drive/speech/data/train'\n","path_val_data_cer = '/content/drive/My Drive/speech/data/validation'\n","path_test_data_cer = '/content/drive/My Drive/speech/data/test'\n","path_letter_data_cer = '/content/drive/My Drive/speech/all_sessions.txt'\n","\n","BATCH_SIZE=8\n","\n","letters_labels, N_LETTERS = parseSeqLabels(path_letter_data_cer)\n","data_train_cer, _ = findAllSeqs(path_train_data_cer, extension='.wav')\n","dataset_train_non_aligned = SingleSequenceDataset(path_train_data_cer, data_train_cer, letters_labels)\n","\n","\n","data_val_cer, _ = findAllSeqs(path_val_data_cer, extension='.wav')\n","dataset_val_non_aligned = SingleSequenceDataset(path_val_data_cer, data_val_cer, letters_labels)\n","\n","\n","# The data loader will generate a tuple of tensors data, labels for each batch\n","# data : size N x T1 x 1 : the audio sequence\n","# label : size N x T2 the sequence of letters corresponding to the audio data\n","# IMPORTANT NOTE: just like the PER the CER is computed with non-aligned phone data.\n","data_loader_train_letters = torch.utils.data.DataLoader(dataset_train_non_aligned, batch_size=BATCH_SIZE,\n","                                                shuffle=True)\n","data_loader_val_letters = torch.utils.data.DataLoader(dataset_val_non_aligned, batch_size=BATCH_SIZE,\n","                                              shuffle=True)"],"execution_count":62,"outputs":[{"output_type":"stream","text":["4it [00:00, 570.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/CPC_audio\n","Saved cache file at /content/drive/My Drive/speech/data/train/_seqs_cache.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n","4it [00:00, 422.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["Loaded 143 sequences in 1.85 seconds\n","maxSizeSeq : 323433\n","maxSizePhone : 128\n","minSizePhone : 12\n","Total size dataset 0.39825538194444443 hours\n","Saved cache file at /content/drive/My Drive/speech/data/validation/_seqs_cache.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Loaded 143 sequences in 1.84 seconds\n","maxSizeSeq : 323433\n","maxSizePhone : 128\n","minSizePhone : 12\n","Total size dataset 0.39825538194444443 hours\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9h07zI2LjzAU","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593900618484,"user_tz":-120,"elapsed":1566,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"ef728df3-d8ed-4a24-c9ac-b8826fa91638"},"source":["from cpc.feature_loader import loadModel\n","\n","checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n","cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n","cpc_model = cpc_model.cuda()\n","character_classifier = PhoneClassifier(HIDDEN_CONTEXT_MODEL, N_LETTERS).to(device)"],"execution_count":63,"outputs":[{"output_type":"stream","text":["Loading checkpoint checkpoint_data/checkpoint_30.pt\n","Loading the state dict at checkpoint_data/checkpoint_30.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rHCNg1E7lW1L","colab":{},"executionInfo":{"status":"ok","timestamp":1593900619967,"user_tz":-120,"elapsed":713,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["parameters = list(character_classifier.parameters()) + list(cpc_model.parameters())\n","LEARNING_RATE = 2e-4\n","optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n","\n","optimizer_frozen = torch.optim.Adam(list(character_classifier.parameters()), lr=LEARNING_RATE)"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"engpkljbk9hj","colab":{},"executionInfo":{"status":"ok","timestamp":1593900621382,"user_tz":-120,"elapsed":938,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}}},"source":["loss_ctc = torch.nn.CTCLoss()"],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9NBHd2s2kxld","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593900730153,"user_tz":-120,"elapsed":108924,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"1293ee28-bea2-45ee-aff8-a097adb0bb47"},"source":["run_ctc(cpc_model,character_classifier,loss_ctc,data_loader_train_letters,data_loader_val_letters,optimizer_frozen,n_epoch=10)"],"execution_count":66,"outputs":[{"output_type":"stream","text":["Running epoch 1 / 10\n","-------------------\n","Training dataset :\n","Average loss : 74.07048206597986.\n","-------------------\n","Validation dataset\n","Average loss : 72.3986094300176\n","-------------------\n","\n","Running epoch 2 / 10\n","-------------------\n","Training dataset :\n","Average loss : 70.69946375027509.\n","-------------------\n","Validation dataset\n","Average loss : 68.68825015215806\n","-------------------\n","\n","Running epoch 3 / 10\n","-------------------\n","Training dataset :\n","Average loss : 66.85569730946716.\n","-------------------\n","Validation dataset\n","Average loss : 64.64670987196372\n","-------------------\n","\n","Running epoch 4 / 10\n","-------------------\n","Training dataset :\n","Average loss : 62.66919170970648.\n","-------------------\n","Validation dataset\n","Average loss : 60.55429555328799\n","-------------------\n","\n","Running epoch 5 / 10\n","-------------------\n","Training dataset :\n","Average loss : 58.553509779379404.\n","-------------------\n","Validation dataset\n","Average loss : 56.37082059618453\n","-------------------\n","\n","Running epoch 6 / 10\n","-------------------\n","Training dataset :\n","Average loss : 54.435946612290934.\n","-------------------\n","Validation dataset\n","Average loss : 52.31845963169152\n","-------------------\n","\n","Running epoch 7 / 10\n","-------------------\n","Training dataset :\n","Average loss : 50.486305827825845.\n","-------------------\n","Validation dataset\n","Average loss : 48.38736998866981\n","-------------------\n","\n","Running epoch 8 / 10\n","-------------------\n","Training dataset :\n","Average loss : 46.64219928795183.\n","-------------------\n","Validation dataset\n","Average loss : 44.7215024921256\n","-------------------\n","\n","Running epoch 9 / 10\n","-------------------\n","Training dataset :\n","Average loss : 43.06528075312225.\n","-------------------\n","Validation dataset\n","Average loss : 41.24152879312005\n","-------------------\n","\n","Running epoch 10 / 10\n","-------------------\n","Training dataset :\n","Average loss : 39.706682446976785.\n","-------------------\n","Validation dataset\n","Average loss : 38.009181546493316\n","-------------------\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A8oxFr1jm17P","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1593900999527,"user_tz":-120,"elapsed":269334,"user":{"displayName":"mamadou gueye","photoUrl":"","userId":"08729563113881402576"}},"outputId":"611d307e-5e52-4abb-8ab8-ab718737e4c8"},"source":["get_per(data_loader_val_letters,cpc_model,character_classifier)"],"execution_count":67,"outputs":[{"output_type":"stream","text":["\r                                                                               \r\rN/A% (0 of 18) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"],"name":"stderr"},{"output_type":"stream","text":["Starting the PER computation through beam search\n"],"name":"stdout"},{"output_type":"stream","text":["100% (18 of 18) |########################| Elapsed Time: 0:04:26 Time:  0:04:26\n"],"name":"stderr"},{"output_type":"stream","text":["Average PER 0.9346970913183367\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.9346970913183367"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"LZ49V2Af4dXl","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}