{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "m8lzlgz_F4v6",
    "outputId": "6d834f29-9bb5-4a52-e0bd-1285662cebe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: nvidia-smi: not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "qG8mfvEOX4pY",
    "outputId": "f536bb9e-22bf-4732-a8f2-c872d63875fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/pip\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "TypeError: 'module' object is not callable\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/pip\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "TypeError: 'module' object is not callable\n"
     ]
    }
   ],
   "source": [
    "!pip install torchaudio\n",
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HKLxBZsQlWr"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUhyznkDPprW"
   },
   "outputs": [],
   "source": [
    "!unzip \"drive/My Drive/My ASR Dataset.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "5SNSR2EcPtQU",
    "outputId": "d4825a37-841e-4ee8-ff41-5c0f9709b1b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CPC_audio'...\n",
      "remote: Enumerating objects: 84, done.\u001b[K\n",
      "remote: Counting objects:   1% (1/84)\u001b[K\r",
      "remote: Counting objects:   2% (2/84)\u001b[K\r",
      "remote: Counting objects:   3% (3/84)\u001b[K\r",
      "remote: Counting objects:   4% (4/84)\u001b[K\r",
      "remote: Counting objects:   5% (5/84)\u001b[K\r",
      "remote: Counting objects:   7% (6/84)\u001b[K\r",
      "remote: Counting objects:   8% (7/84)\u001b[K\r",
      "remote: Counting objects:   9% (8/84)\u001b[K\r",
      "remote: Counting objects:  10% (9/84)\u001b[K\r",
      "remote: Counting objects:  11% (10/84)\u001b[K\r",
      "remote: Counting objects:  13% (11/84)\u001b[K\r",
      "remote: Counting objects:  14% (12/84)\u001b[K\r",
      "remote: Counting objects:  15% (13/84)\u001b[K\r",
      "remote: Counting objects:  16% (14/84)\u001b[K\r",
      "remote: Counting objects:  17% (15/84)\u001b[K\r",
      "remote: Counting objects:  19% (16/84)\u001b[K\r",
      "remote: Counting objects:  20% (17/84)\u001b[K\r",
      "remote: Counting objects:  21% (18/84)\u001b[K\r",
      "remote: Counting objects:  22% (19/84)\u001b[K\r",
      "remote: Counting objects:  23% (20/84)\u001b[K\r",
      "remote: Counting objects:  25% (21/84)\u001b[K\r",
      "remote: Counting objects:  26% (22/84)\u001b[K\r",
      "remote: Counting objects:  27% (23/84)\u001b[K\r",
      "remote: Counting objects:  28% (24/84)\u001b[K\r",
      "remote: Counting objects:  29% (25/84)\u001b[K\r",
      "remote: Counting objects:  30% (26/84)\u001b[K\r",
      "remote: Counting objects:  32% (27/84)\u001b[K\r",
      "remote: Counting objects:  33% (28/84)\u001b[K\r",
      "remote: Counting objects:  34% (29/84)\u001b[K\r",
      "remote: Counting objects:  35% (30/84)\u001b[K\r",
      "remote: Counting objects:  36% (31/84)\u001b[K\r",
      "remote: Counting objects:  38% (32/84)\u001b[K\r",
      "remote: Counting objects:  39% (33/84)\u001b[K\r",
      "remote: Counting objects:  40% (34/84)\u001b[K\r",
      "remote: Counting objects:  41% (35/84)\u001b[K\r",
      "remote: Counting objects:  42% (36/84)\u001b[K\r",
      "remote: Counting objects:  44% (37/84)\u001b[K\r",
      "remote: Counting objects:  45% (38/84)\u001b[K\r",
      "remote: Counting objects:  46% (39/84)\u001b[K\r",
      "remote: Counting objects:  47% (40/84)\u001b[K\r",
      "remote: Counting objects:  48% (41/84)\u001b[K\r",
      "remote: Counting objects:  50% (42/84)\u001b[K\r",
      "remote: Counting objects:  51% (43/84)\u001b[K\r",
      "remote: Counting objects:  52% (44/84)\u001b[K\r",
      "remote: Counting objects:  53% (45/84)\u001b[K\r",
      "remote: Counting objects:  54% (46/84)\u001b[K\r",
      "remote: Counting objects:  55% (47/84)\u001b[K\r",
      "remote: Counting objects:  57% (48/84)\u001b[K\r",
      "remote: Counting objects:  58% (49/84)\u001b[K\r",
      "remote: Counting objects:  59% (50/84)\u001b[K\r",
      "remote: Counting objects:  60% (51/84)\u001b[K\r",
      "remote: Counting objects:  61% (52/84)\u001b[K\r",
      "remote: Counting objects:  63% (53/84)\u001b[K\r",
      "remote: Counting objects:  64% (54/84)\u001b[K\r",
      "remote: Counting objects:  65% (55/84)\u001b[K\r",
      "remote: Counting objects:  66% (56/84)\u001b[K\r",
      "remote: Counting objects:  67% (57/84)\u001b[K\r",
      "remote: Counting objects:  69% (58/84)\u001b[K\r",
      "remote: Counting objects:  70% (59/84)\u001b[K\r",
      "remote: Counting objects:  71% (60/84)\u001b[K\r",
      "remote: Counting objects:  72% (61/84)\u001b[K\r",
      "remote: Counting objects:  73% (62/84)\u001b[K\r",
      "remote: Counting objects:  75% (63/84)\u001b[K\r",
      "remote: Counting objects:  76% (64/84)\u001b[K\r",
      "remote: Counting objects:  77% (65/84)\u001b[K\r",
      "remote: Counting objects:  78% (66/84)\u001b[K\r",
      "remote: Counting objects:  79% (67/84)\u001b[K\r",
      "remote: Counting objects:  80% (68/84)\u001b[K\r",
      "remote: Counting objects:  82% (69/84)\u001b[K\r",
      "remote: Counting objects:  83% (70/84)\u001b[K\r",
      "remote: Counting objects:  84% (71/84)\u001b[K\r",
      "remote: Counting objects:  85% (72/84)\u001b[K\r",
      "remote: Counting objects:  86% (73/84)\u001b[K\r",
      "remote: Counting objects:  88% (74/84)\u001b[K\r",
      "remote: Counting objects:  89% (75/84)\u001b[K\r",
      "remote: Counting objects:  90% (76/84)\u001b[K\r",
      "remote: Counting objects:  91% (77/84)\u001b[K\r",
      "remote: Counting objects:  92% (78/84)\u001b[K\r",
      "remote: Counting objects:  94% (79/84)\u001b[K\r",
      "remote: Counting objects:  95% (80/84)\u001b[K\r",
      "remote: Counting objects:  96% (81/84)\u001b[K\r",
      "remote: Counting objects:  97% (82/84)\u001b[K\r",
      "remote: Counting objects:  98% (83/84)\u001b[K\r",
      "remote: Counting objects: 100% (84/84)\u001b[K\r",
      "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
      "remote: Compressing objects:   1% (1/65)\u001b[K\r",
      "remote: Compressing objects:   3% (2/65)\u001b[K\r",
      "remote: Compressing objects:   4% (3/65)\u001b[K\r",
      "remote: Compressing objects:   6% (4/65)\u001b[K\r",
      "remote: Compressing objects:   7% (5/65)\u001b[K\r",
      "remote: Compressing objects:   9% (6/65)\u001b[K\r",
      "remote: Compressing objects:  10% (7/65)\u001b[K\r",
      "remote: Compressing objects:  12% (8/65)\u001b[K\r",
      "remote: Compressing objects:  13% (9/65)\u001b[K\r",
      "remote: Compressing objects:  15% (10/65)\u001b[K\r",
      "remote: Compressing objects:  16% (11/65)\u001b[K\r",
      "remote: Compressing objects:  18% (12/65)\u001b[K\r",
      "remote: Compressing objects:  20% (13/65)\u001b[K\r",
      "remote: Compressing objects:  21% (14/65)\u001b[K\r",
      "remote: Compressing objects:  23% (15/65)\u001b[K\r",
      "remote: Compressing objects:  24% (16/65)\u001b[K\r",
      "remote: Compressing objects:  26% (17/65)\u001b[K\r",
      "remote: Compressing objects:  27% (18/65)\u001b[K\r",
      "remote: Compressing objects:  29% (19/65)\u001b[K\r",
      "remote: Compressing objects:  30% (20/65)\u001b[K\r",
      "remote: Compressing objects:  32% (21/65)\u001b[K\r",
      "remote: Compressing objects:  33% (22/65)\u001b[K\r",
      "remote: Compressing objects:  35% (23/65)\u001b[K\r",
      "remote: Compressing objects:  36% (24/65)\u001b[K\r",
      "remote: Compressing objects:  38% (25/65)\u001b[K\r",
      "remote: Compressing objects:  40% (26/65)\u001b[K\r",
      "remote: Compressing objects:  41% (27/65)\u001b[K\r",
      "remote: Compressing objects:  43% (28/65)\u001b[K\r",
      "remote: Compressing objects:  44% (29/65)\u001b[K\r",
      "remote: Compressing objects:  46% (30/65)\u001b[K\r",
      "remote: Compressing objects:  47% (31/65)\u001b[K\r",
      "remote: Compressing objects:  49% (32/65)\u001b[K\r",
      "remote: Compressing objects:  50% (33/65)\u001b[K\r",
      "remote: Compressing objects:  52% (34/65)\u001b[K\r",
      "remote: Compressing objects:  53% (35/65)\u001b[K\r",
      "remote: Compressing objects:  55% (36/65)\u001b[K\r",
      "remote: Compressing objects:  56% (37/65)\u001b[K\r",
      "remote: Compressing objects:  58% (38/65)\u001b[K\r",
      "remote: Compressing objects:  60% (39/65)\u001b[K\r",
      "remote: Compressing objects:  61% (40/65)\u001b[K\r",
      "remote: Compressing objects:  63% (41/65)\u001b[K\r",
      "remote: Compressing objects:  64% (42/65)\u001b[K\r",
      "remote: Compressing objects:  66% (43/65)\u001b[K\r",
      "remote: Compressing objects:  67% (44/65)\u001b[K\r",
      "remote: Compressing objects:  69% (45/65)\u001b[K\r",
      "remote: Compressing objects:  70% (46/65)\u001b[K\r",
      "remote: Compressing objects:  72% (47/65)\u001b[K\r",
      "remote: Compressing objects:  73% (48/65)\u001b[K\r",
      "remote: Compressing objects:  75% (49/65)\u001b[K\r",
      "remote: Compressing objects:  76% (50/65)\u001b[K\r",
      "remote: Compressing objects:  78% (51/65)\u001b[K\r",
      "remote: Compressing objects:  80% (52/65)\u001b[K\r",
      "remote: Compressing objects:  81% (53/65)\u001b[K\r",
      "remote: Compressing objects:  83% (54/65)\u001b[K\r",
      "remote: Compressing objects:  84% (55/65)\u001b[K\r",
      "remote: Compressing objects:  86% (56/65)\u001b[K\r",
      "remote: Compressing objects:  87% (57/65)\u001b[K\r",
      "remote: Compressing objects:  89% (58/65)\u001b[K\r",
      "remote: Compressing objects:  90% (59/65)\u001b[K\r",
      "remote: Compressing objects:  92% (60/65)\u001b[K\r",
      "remote: Compressing objects:  93% (61/65)\u001b[K\r",
      "remote: Compressing objects:  95% (62/65)\u001b[K\r",
      "remote: Compressing objects:  96% (63/65)\u001b[K\r",
      "remote: Compressing objects:  98% (64/65)\u001b[K\r",
      "remote: Compressing objects: 100% (65/65)\u001b[K\r",
      "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
      "Unpacking objects:   1% (1/84)   \r",
      "Unpacking objects:   2% (2/84)   \r",
      "Unpacking objects:   3% (3/84)   \r",
      "Unpacking objects:   4% (4/84)   \r",
      "Unpacking objects:   5% (5/84)   \r",
      "Unpacking objects:   7% (6/84)   \r",
      "Unpacking objects:   8% (7/84)   \r",
      "Unpacking objects:   9% (8/84)   \r",
      "Unpacking objects:  10% (9/84)   \r",
      "Unpacking objects:  11% (10/84)   \r",
      "Unpacking objects:  13% (11/84)   \r",
      "Unpacking objects:  14% (12/84)   \r",
      "Unpacking objects:  15% (13/84)   \r",
      "Unpacking objects:  16% (14/84)   \r",
      "Unpacking objects:  17% (15/84)   \r",
      "Unpacking objects:  19% (16/84)   \r",
      "Unpacking objects:  20% (17/84)   \r",
      "Unpacking objects:  21% (18/84)   \r",
      "Unpacking objects:  22% (19/84)   \r",
      "Unpacking objects:  23% (20/84)   \r",
      "Unpacking objects:  25% (21/84)   \r",
      "Unpacking objects:  26% (22/84)   \r",
      "Unpacking objects:  27% (23/84)   \r",
      "Unpacking objects:  28% (24/84)   \r",
      "Unpacking objects:  29% (25/84)   \r",
      "Unpacking objects:  30% (26/84)   \r",
      "Unpacking objects:  32% (27/84)   \r",
      "Unpacking objects:  33% (28/84)   \r",
      "Unpacking objects:  34% (29/84)   \r",
      "Unpacking objects:  35% (30/84)   \r",
      "Unpacking objects:  36% (31/84)   \r",
      "Unpacking objects:  38% (32/84)   \r",
      "Unpacking objects:  39% (33/84)   \r",
      "Unpacking objects:  40% (34/84)   \r",
      "Unpacking objects:  41% (35/84)   \r",
      "Unpacking objects:  42% (36/84)   \r",
      "Unpacking objects:  44% (37/84)   \r",
      "Unpacking objects:  45% (38/84)   \r",
      "Unpacking objects:  46% (39/84)   \r",
      "Unpacking objects:  47% (40/84)   \r",
      "Unpacking objects:  48% (41/84)   \r",
      "Unpacking objects:  50% (42/84)   \r",
      "Unpacking objects:  51% (43/84)   \r",
      "Unpacking objects:  52% (44/84)   \r",
      "Unpacking objects:  53% (45/84)   \r",
      "Unpacking objects:  54% (46/84)   \r",
      "Unpacking objects:  55% (47/84)   \r",
      "Unpacking objects:  57% (48/84)   \r",
      "Unpacking objects:  58% (49/84)   \r",
      "Unpacking objects:  59% (50/84)   \r",
      "Unpacking objects:  60% (51/84)   \r",
      "Unpacking objects:  61% (52/84)   \r",
      "Unpacking objects:  63% (53/84)   \r",
      "Unpacking objects:  64% (54/84)   \r",
      "Unpacking objects:  65% (55/84)   \r",
      "Unpacking objects:  66% (56/84)   \r",
      "Unpacking objects:  67% (57/84)   \r",
      "Unpacking objects:  69% (58/84)   \r",
      "Unpacking objects:  70% (59/84)   \r",
      "Unpacking objects:  71% (60/84)   \r",
      "Unpacking objects:  72% (61/84)   \r",
      "Unpacking objects:  73% (62/84)   \r",
      "Unpacking objects:  75% (63/84)   \r",
      "Unpacking objects:  76% (64/84)   \r",
      "Unpacking objects:  77% (65/84)   \r",
      "Unpacking objects:  78% (66/84)   \r",
      "Unpacking objects:  79% (67/84)   \r",
      "remote: Total 84 (delta 13), reused 75 (delta 6), pack-reused 0\u001b[K\n",
      "Unpacking objects:  80% (68/84)   \r",
      "Unpacking objects:  82% (69/84)   \r",
      "Unpacking objects:  83% (70/84)   \r",
      "Unpacking objects:  84% (71/84)   \r",
      "Unpacking objects:  85% (72/84)   \r",
      "Unpacking objects:  86% (73/84)   \r",
      "Unpacking objects:  88% (74/84)   \r",
      "Unpacking objects:  89% (75/84)   \r",
      "Unpacking objects:  90% (76/84)   \r",
      "Unpacking objects:  91% (77/84)   \r",
      "Unpacking objects:  92% (78/84)   \r",
      "Unpacking objects:  94% (79/84)   \r",
      "Unpacking objects:  95% (80/84)   \r",
      "Unpacking objects:  96% (81/84)   \r",
      "Unpacking objects:  97% (82/84)   \r",
      "Unpacking objects:  98% (83/84)   \r",
      "Unpacking objects: 100% (84/84)   \r",
      "Unpacking objects: 100% (84/84), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/facebookresearch/CPC_audio.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "O1j8w_OpP2o-",
    "outputId": "f9ee5714-fe5f-4eea-db59-fc6fc71ce153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/CPC_audio\n",
      "Compiling cpc/eval/ABX/dtw.pyx because it changed.\n",
      "[1/1] Cythonizing cpc/eval/ABX/dtw.pyx\n",
      "running develop\n",
      "running egg_info\n",
      "creating CPC_audio.egg-info\n",
      "writing CPC_audio.egg-info/PKG-INFO\n",
      "writing dependency_links to CPC_audio.egg-info/dependency_links.txt\n",
      "writing top-level names to CPC_audio.egg-info/top_level.txt\n",
      "writing manifest file 'CPC_audio.egg-info/SOURCES.txt'\n",
      "writing manifest file 'CPC_audio.egg-info/SOURCES.txt'\n",
      "running build_ext\n",
      "building 'cpc.eval.ABX.dtw' extension\n",
      "creating build\n",
      "creating build/temp.linux-x86_64-3.6\n",
      "creating build/temp.linux-x86_64-3.6/cpc\n",
      "creating build/temp.linux-x86_64-3.6/cpc/eval\n",
      "creating build/temp.linux-x86_64-3.6/cpc/eval/ABX\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c cpc/eval/ABX/dtw.c -o build/temp.linux-x86_64-3.6/cpc/eval/ABX/dtw.o\n",
      "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Kcpc/eval/ABX/dtw.c:625\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
      " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
      "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
      "creating build/lib.linux-x86_64-3.6\n",
      "creating build/lib.linux-x86_64-3.6/cpc\n",
      "creating build/lib.linux-x86_64-3.6/cpc/eval\n",
      "creating build/lib.linux-x86_64-3.6/cpc/eval/ABX\n",
      "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cpc/eval/ABX/dtw.o -o build/lib.linux-x86_64-3.6/cpc/eval/ABX/dtw.cpython-36m-x86_64-linux-gnu.so\n",
      "copying build/lib.linux-x86_64-3.6/cpc/eval/ABX/dtw.cpython-36m-x86_64-linux-gnu.so -> cpc/eval/ABX\n",
      "Creating /usr/local/lib/python3.6/dist-packages/CPC-audio.egg-link (link to .)\n",
      "Adding CPC-audio 1.0 to easy-install.pth file\n",
      "\n",
      "Installed /content/CPC_audio\n",
      "Processing dependencies for CPC-audio==1.0\n",
      "Finished processing dependencies for CPC-audio==1.0\n"
     ]
    }
   ],
   "source": [
    "%cd CPC_audio\n",
    "!python setup.py develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kte50H2QPmY_"
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eaibi7fsPmZC"
   },
   "outputs": [],
   "source": [
    "meta_files = glob.glob(\"/content/My ASR Dataset/*/*.json\")\n",
    "corpus_files = glob.glob(\"/content/My ASR Dataset/corpus*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y6SGabpBPmZE"
   },
   "outputs": [],
   "source": [
    "yoruba_character_set = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'á', 'è', 'é', 'ì', 'í', 'ò', 'ó',\n",
    "'ù', 'ú', 'ń', 'ǹ', 'ḿ', 'ṣ', 'ẹ', 'ọ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PMOHJiC6PmZI",
    "outputId": "7135f7e3-900e-4909-a884-a4d9be764f1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Recording Duration in minutes is: 84\n",
      "Total Number of Recordings is 751\n"
     ]
    }
   ],
   "source": [
    "total_time = 0\n",
    "count = 0\n",
    "for file in meta_files:\n",
    "    with open(file, \"r\") as meta:\n",
    "        sample = json.load(meta)\n",
    "        total_time += sample[\"durationMsec\"]\n",
    "        count += 1\n",
    "print(f\"Total Recording Duration in minutes is: {int((total_time*1e-3)/60)}\")\n",
    "print(f\"Total Number of Recordings is {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xuIZrBgAPmZN",
    "outputId": "b523afb7-135a-486d-9746-4fc648bdc4fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Linker files is: 15\n"
     ]
    }
   ],
   "source": [
    "all_linkers = []\n",
    "linker_files = glob.glob(\"/content/My ASR Dataset/*/*.txt\")\n",
    "for file in linker_files:\n",
    "    with open(file, \"r\") as linker:\n",
    "        all_linkers.extend(linker.readlines())\n",
    "print(f\"Total Number of Linker files is: {len(linker_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xjDF62psPmZR"
   },
   "outputs": [],
   "source": [
    "line_nos = []\n",
    "transcript_file = []\n",
    "audio_file = []\n",
    "\n",
    "for linker_info in all_linkers:\n",
    "    transcript_file.append(re.findall(r\"\\w+.txt\", linker_info)[0])\n",
    "    line_nos.append(re.findall(r\"line.\\d+\", linker_info)[0])\n",
    "    audio_file.append(re.findall(r\"\\d.[^/]+wav\", linker_info)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "J8Lp3Xp4PmZU",
    "outputId": "a87bc4ae-3642-4951-b658-908dfa9592ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_nos</th>\n",
       "      <th>transcript_file</th>\n",
       "      <th>audio_file</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>line 1 corpus6.txt</th>\n",
       "      <td>line 1</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_0.wav</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line 2 corpus6.txt</th>\n",
       "      <td>line 2</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_1.wav</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line 3 corpus6.txt</th>\n",
       "      <td>line 3</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_2.wav</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line 4 corpus6.txt</th>\n",
       "      <td>line 4</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_3.wav</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line 5 corpus6.txt</th>\n",
       "      <td>line 5</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_4.wav</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   line_nos  ... transcript\n",
       "index                        ...           \n",
       "line 1 corpus6.txt   line 1  ...           \n",
       "line 2 corpus6.txt   line 2  ...           \n",
       "line 3 corpus6.txt   line 3  ...           \n",
       "line 4 corpus6.txt   line 4  ...           \n",
       "line 5 corpus6.txt   line 5  ...           \n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({\"line_nos\":line_nos, \"transcript_file\":transcript_file, \"audio_file\":audio_file})\n",
    "data[\"index\"] = data[\"line_nos\"]+\" \"+data[\"transcript_file\"]\n",
    "data[\"transcript\"] = \" \"\n",
    "data = data.set_index(\"index\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "jz5URi8oTBlK",
    "outputId": "a3fbe40a-40dc-47d1-8b81-4429ff6b2d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "line_nos           0\n",
       "transcript_file    0\n",
       "audio_file         0\n",
       "transcript         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2PE2Q7P8PmZZ"
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for file in corpus_files:\n",
    "    # file2 = os.path.join(\"/content/My ASR Dataset\", file)\n",
    "    with open(file, \"r\") as content:\n",
    "        filename = os.path.split(file)[-1]\n",
    "        file_content = content.readlines()\n",
    "        corpus.extend(file_content)\n",
    "        for idx, line in enumerate(file_content):\n",
    "            line = line.strip()\n",
    "            if line != \"\":\n",
    "                index = f\"line {idx+1} {filename}\"\n",
    "                data.loc[index, \"transcript\"] = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "j2OJaicmPmZd",
    "outputId": "328d079a-a792-48fd-b567-9d510528c03d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_nos</th>\n",
       "      <th>transcript_file</th>\n",
       "      <th>audio_file</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>line 1 corpus6.txt</th>\n",
       "      <td>line 1</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_0.wav</td>\n",
       "      <td>Bí ìlàrí bá fẹ́ tẹ́, a ní kí lọba ó ṣe? ## Whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line 2 corpus6.txt</th>\n",
       "      <td>line 2</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_1.wav</td>\n",
       "      <td>Bí iná bá dun ọbẹ̀, a dá ọ̀rọ̀ sọ. ## If the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line 3 corpus6.txt</th>\n",
       "      <td>line 3</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_2.wav</td>\n",
       "      <td>Bí kò sí àkópọ̀, kí lewúrẹ́ wá dé ìsọ̀ adìẹ? #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line 4 corpus6.txt</th>\n",
       "      <td>line 4</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_3.wav</td>\n",
       "      <td>Bí kò sí tọ̀bùn èèyàn, ta ni ìbá jí lówùúrọ̀ t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line 5 corpus6.txt</th>\n",
       "      <td>line 5</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_4.wav</td>\n",
       "      <td>Bí mo bá torí oko kú ng ó rò fáhéré; bí mo bá ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   line_nos  ...                                         transcript\n",
       "index                        ...                                                   \n",
       "line 1 corpus6.txt   line 1  ...  Bí ìlàrí bá fẹ́ tẹ́, a ní kí lọba ó ṣe? ## Whe...\n",
       "line 2 corpus6.txt   line 2  ...  Bí iná bá dun ọbẹ̀, a dá ọ̀rọ̀ sọ. ## If the f...\n",
       "line 3 corpus6.txt   line 3  ...  Bí kò sí àkópọ̀, kí lewúrẹ́ wá dé ìsọ̀ adìẹ? #...\n",
       "line 4 corpus6.txt   line 4  ...  Bí kò sí tọ̀bùn èèyàn, ta ni ìbá jí lówùúrọ̀ t...\n",
       "line 5 corpus6.txt   line 5  ...  Bí mo bá torí oko kú ng ó rò fáhéré; bí mo bá ...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn4e6qF5PmZg"
   },
   "outputs": [],
   "source": [
    "# character = []\n",
    "# for line in corpus:\n",
    "#     character.extend(list(line.strip().lower()))\n",
    "# character = set(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuZfl8bzPmZj"
   },
   "outputs": [],
   "source": [
    "character_to_token = {}\n",
    "for idx, character in enumerate(yoruba_character_set):\n",
    "    character_to_token[character] = idx\n",
    "\n",
    "token_to_character = {v:k for k,v in character_to_token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tr_b58f0PmZm"
   },
   "outputs": [],
   "source": [
    "def tokenize(transcript):\n",
    "    transcript = list(transcript.lower())\n",
    "    transcript = [character for character in transcript if (character in yoruba_character_set)]\n",
    "    tokens = [character_to_token[character] for character in transcript]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HaDiJfxdPmZp"
   },
   "outputs": [],
   "source": [
    "data[\"tokens\"] = data[\"transcript\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "1fpTCek9PmZr",
    "outputId": "f287c4cf-ba32-48a6-bd2a-cd6e476499ea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_nos</th>\n",
       "      <th>transcript_file</th>\n",
       "      <th>audio_file</th>\n",
       "      <th>transcript</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>line 1 corpus6.txt</th>\n",
       "      <td>line 1</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_0.wav</td>\n",
       "      <td>Bí ìlàrí bá fẹ́ tẹ́, a ní kí lọba ó ṣe? ## Whe...</td>\n",
       "      <td>[1, 31, 30, 11, 26, 17, 31, 1, 27, 5, 40, 19, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line 2 corpus6.txt</th>\n",
       "      <td>line 2</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_1.wav</td>\n",
       "      <td>Bí iná bá dun ọbẹ̀, a dá ọ̀rọ̀ sọ. ## If the f...</td>\n",
       "      <td>[1, 31, 8, 13, 27, 1, 27, 3, 20, 13, 41, 1, 40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line 3 corpus6.txt</th>\n",
       "      <td>line 3</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_2.wav</td>\n",
       "      <td>Bí kò sí àkópọ̀, kí lewúrẹ́ wá dé ìsọ̀ adìẹ? #...</td>\n",
       "      <td>[1, 31, 10, 32, 18, 31, 26, 10, 33, 15, 41, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line 4 corpus6.txt</th>\n",
       "      <td>line 4</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_3.wav</td>\n",
       "      <td>Bí kò sí tọ̀bùn èèyàn, ta ni ìbá jí lówùúrọ̀ t...</td>\n",
       "      <td>[1, 31, 10, 32, 18, 31, 19, 41, 1, 34, 13, 28,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line 5 corpus6.txt</th>\n",
       "      <td>line 5</td>\n",
       "      <td>corpus6.txt</td>\n",
       "      <td>200701-120223_yor_7f4_elicit_4.wav</td>\n",
       "      <td>Bí mo bá torí oko kú ng ó rò fáhéré; bí mo bá ...</td>\n",
       "      <td>[1, 31, 12, 14, 1, 27, 19, 14, 17, 31, 14, 10,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   line_nos  ...                                             tokens\n",
       "index                        ...                                                   \n",
       "line 1 corpus6.txt   line 1  ...  [1, 31, 30, 11, 26, 17, 31, 1, 27, 5, 40, 19, ...\n",
       "line 2 corpus6.txt   line 2  ...  [1, 31, 8, 13, 27, 1, 27, 3, 20, 13, 41, 1, 40...\n",
       "line 3 corpus6.txt   line 3  ...  [1, 31, 10, 32, 18, 31, 26, 10, 33, 15, 41, 10...\n",
       "line 4 corpus6.txt   line 4  ...  [1, 31, 10, 32, 18, 31, 19, 41, 1, 34, 13, 28,...\n",
       "line 5 corpus6.txt   line 5  ...  [1, 31, 12, 14, 1, 27, 19, 14, 17, 31, 14, 10,...\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTjpjU1_VkQO"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# data = pd.read_csv(\"/content/My ASR Dataset/processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XzahWUW_PmZu"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkuxzAPo8cbj"
   },
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "3uqbxu-1Xi0v",
    "outputId": "f4f3ea3b-8284-4fbc-9010-33c2fc2e56c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-04 02:52:30--  https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_30.pt\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 113599715 (108M) [application/octet-stream]\n",
      "Saving to: ‘checkpoint_data/checkpoint_30.pt’\n",
      "\n",
      "checkpoint_30.pt    100%[===================>] 108.34M  22.4MB/s    in 5.4s    \n",
      "\n",
      "2020-07-04 02:52:36 (20.0 MB/s) - ‘checkpoint_data/checkpoint_30.pt’ saved [113599715/113599715]\n",
      "\n",
      "--2020-07-04 02:52:37--  https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_logs.json\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20786 (20K) [text/plain]\n",
      "Saving to: ‘checkpoint_data/checkpoint_logs.json’\n",
      "\n",
      "checkpoint_logs.jso 100%[===================>]  20.30K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2020-07-04 02:52:38 (235 KB/s) - ‘checkpoint_data/checkpoint_logs.json’ saved [20786/20786]\n",
      "\n",
      "--2020-07-04 02:52:40--  https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_args.json\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2063 (2.0K) [text/plain]\n",
      "Saving to: ‘checkpoint_data/checkpoint_args.json’\n",
      "\n",
      "checkpoint_args.jso 100%[===================>]   2.01K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-07-04 02:52:40 (33.0 MB/s) - ‘checkpoint_data/checkpoint_args.json’ saved [2063/2063]\n",
      "\n",
      "checkpoint_30.pt  checkpoint_args.json\tcheckpoint_logs.json\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoint_data\n",
    "!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_30.pt -P checkpoint_data\n",
    "!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_logs.json -P checkpoint_data\n",
    "!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_args.json -P checkpoint_data\n",
    "!ls checkpoint_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "laABczQ5PmZu"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "label_dict = {}\n",
    "label_dict[\"step\"] = 160\n",
    "\n",
    "for idx in range(len(data)):\n",
    "    row = data.iloc[idx]\n",
    "    label_dict[os.path.splitext(row[\"audio_file\"])[0]] = row[\"tokens\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "aupCAcIAPmZ2",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torch.optim as optim\n",
    "from torch.multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTOc4pz0PmZ4"
   },
   "outputs": [],
   "source": [
    "class CharClassifier(torch.nn.Module):\n",
    "\n",
    "  def __init__(self,\n",
    "               input_dim : int,\n",
    "               n_phones : int):\n",
    "    super(CharClassifier, self).__init__()\n",
    "    self.linear = nn.Sequential(\n",
    "                  nn.Linear(input_dim, input_dim*2),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(input_dim*2, n_phones)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "hu-GNtoxPmZ7",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "91655d68-bcae-4eb2-ec6b-68f306fb1577"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:00, 2363.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'CPC_audio'\n",
      "/content/CPC_audio\n",
      "Saved cache file at /content/My ASR Dataset/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "17it [00:00, 2146.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 752 sequences in 2.43 seconds\n",
      "maxSizeSeq : 700227\n",
      "maxSizePhone : 459\n",
      "minSizePhone : 7\n",
      "Total size dataset 1.41503828125 hours\n",
      "Saved cache file at /content/My ASR Dataset/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 752 sequences in 2.42 seconds\n",
      "maxSizeSeq : 700227\n",
      "maxSizePhone : 459\n",
      "minSizePhone : 7\n",
      "Total size dataset 1.41503828125 hours\n"
     ]
    }
   ],
   "source": [
    "%cd CPC_audio\n",
    "from cpc.eval.common_voices_eval import SingleSequenceDataset, parseSeqLabels, findAllSeqs\n",
    "path_train_data_per = '/content/My ASR Dataset'\n",
    "path_val_data_per = '/content/My ASR Dataset'\n",
    "BATCH_SIZE=8\n",
    "N_CHARS = len(token_to_character)\n",
    "\n",
    "data_train_per, _ = findAllSeqs(path_train_data_per, extension='.wav')\n",
    "dataset_train_non_aligned = SingleSequenceDataset(path_train_data_per, data_train_per, label_dict)\n",
    "data_loader_train = torch.utils.data.DataLoader(dataset_train_non_aligned, batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True)\n",
    "\n",
    "data_val_per, _ = findAllSeqs(path_val_data_per, extension='.wav')\n",
    "dataset_val_non_aligned = SingleSequenceDataset(path_val_data_per, data_val_per, label_dict)\n",
    "data_loader_val = torch.utils.data.DataLoader(dataset_val_non_aligned, batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_PyrNsvPmZ9"
   },
   "outputs": [],
   "source": [
    "loss_ctc = torch.nn.CTCLoss()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZD_HWfQ2PmZ_",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch_ctc(cpc_model, \n",
    "                        phone_classifier, \n",
    "                        loss_criterion, \n",
    "                        data_loader, \n",
    "                        optimizer):\n",
    "  cpc_model.train()\n",
    "  phone_classifier.train()\n",
    "\n",
    "  avg_loss = 0\n",
    "  n_items = 0\n",
    "\n",
    "  for step, full_data in enumerate(data_loader):\n",
    "    input_seq, size_seq, label, label_size  = full_data\n",
    "  \n",
    "    # TO COMPLETE\n",
    "    x, y = input_seq.to(device), label.to(device)\n",
    "    context_output, encoder_output, label = cpc_model(x, y)\n",
    "    # loss, acc = cpc_criterion(context_output, encoder_output)\n",
    "    logits = phone_classifier(context_output)\n",
    "    logits = torch.log_softmax(logits, dim=-1).permute(1,0,2)\n",
    "    input_length = torch.full(size=(x.size(0),), fill_value=encoder_output.size(1), dtype=torch.long)\n",
    "    # target_length = torch.full(size=(x.size(0),), fill_value=label.size(1), dtype=torch.long)\n",
    "    loss = loss_criterion(logits, label, input_length, label_size)\n",
    "    avg_loss += loss.item()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()    \n",
    "    n_items+=1\n",
    "    \n",
    "  avg_loss = avg_loss/n_items\n",
    "  return avg_loss\n",
    "\n",
    "def validation_step_ctc(cpc_model, \n",
    "                    phone_classifier, \n",
    "                    loss_criterion, \n",
    "                    data_loader):\n",
    "  # TO COMPLETE\n",
    "  cpc_model.eval()\n",
    "  phone_classifier.eval()\n",
    "\n",
    "  avg_loss = 0\n",
    "  n_items = 0\n",
    "\n",
    "  for step, full_data in enumerate(data_loader):\n",
    "    input_seq, size_seq, label, label_size  = full_data\n",
    "  \n",
    "    # TO COMPLETE\n",
    "    x, y = input_seq.to(device), label.to(device)\n",
    "    context_output, encoder_output, label = cpc_model(x, y)\n",
    "    logits = phone_classifier(context_output)\n",
    "    logits = torch.log_softmax(logits, dim=-1).permute(1,0,2)\n",
    "    input_length = torch.full(size=(x.size(0),), fill_value=encoder_output.size(1), dtype=torch.long)\n",
    "    # target_length = torch.full(size=(x.size(0),), fill_value=label.size(1), dtype=torch.long)\n",
    "    loss = loss_criterion(logits, label, input_length, label_size)\n",
    "    avg_loss += loss.item()  \n",
    "    n_items+=1\n",
    "    \n",
    "  avg_loss = avg_loss/n_items\n",
    "  return avg_loss\n",
    "\n",
    "def run_ctc(cpc_model, \n",
    "            phone_classifier, \n",
    "            loss_criterion, \n",
    "            data_loader_train, \n",
    "            data_loader_val, \n",
    "            optimizer,\n",
    "            n_epoch):\n",
    "  # TO COMPLETE\n",
    "  for epoch in range(n_epoch):\n",
    "\n",
    "    print(f\"Running epoch {epoch + 1} / {n_epoch}\")\n",
    "    loss_train = train_one_epoch_ctc(cpc_model, phone_classifier, loss_criterion, data_loader_train, optimizer)\n",
    "    print(\"-------------------\")\n",
    "    print(f\"Training dataset :\")\n",
    "    print(f\"Average loss : {loss_train}.\")\n",
    "\n",
    "    print(\"-------------------\")\n",
    "    print(\"Validation dataset\")\n",
    "    loss_val = validation_step_ctc(cpc_model, phone_classifier, loss_criterion, data_loader_val)\n",
    "    print(f\"Average loss : {loss_val}.\")\n",
    "    print(\"-------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "q-mRKtGDehJK",
    "outputId": "01923e01-7a42-4d14-e77a-09b12403e1f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'CPC_audio'\n",
      "/content/CPC_audio\n"
     ]
    }
   ],
   "source": [
    "%cd CPC_audio\n",
    "from cpc.feature_loader import loadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "xxwdErouPmaD",
    "outputId": "551325b2-4986-4139-aa61-b99906f934b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint_data/checkpoint_30.pt\n",
      "Loading the state dict at checkpoint_data/checkpoint_30.pt\n"
     ]
    }
   ],
   "source": [
    "#define optimizer and model\n",
    "checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n",
    "cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n",
    "cpc_model = cpc_model.to(device)\n",
    "\n",
    "#define character classifier\n",
    "char_classifier = CharClassifier(HIDDEN_CONTEXT_MODEL, N_CHARS)\n",
    "char_classifier = char_classifier.to(device)\n",
    "\n",
    "parameters = list(char_classifier.parameters()) + list(cpc_model.parameters())\n",
    "LEARNING_RATE = 2e-4\n",
    "optimizer = optim.Adam(parameters, lr=LEARNING_RATE)\n",
    "optimizer_frozen = optim.Adam(list(char_classifier.parameters()), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hPMZfmaxPmaF",
    "outputId": "571b09bf-645d-4289-c0d3-72ce6a13c892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1 / 1\n"
     ]
    }
   ],
   "source": [
    "run_ctc(cpc_model, char_classifier, loss_ctc, data_loader_train, data_loader_val, optimizer, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndLpPh7zBOfU"
   },
   "source": [
    "### Compute CER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGsoiM9uBOKS"
   },
   "outputs": [],
   "source": [
    "import progressbar\n",
    "\n",
    "def cut_data(seq, sizeSeq):\n",
    "    maxSeq = sizeSeq.max()\n",
    "    return seq[:, :maxSeq]\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    seq, sizeSeq, phone, sizePhone = data\n",
    "    seq = seq.cuda()\n",
    "    phone = phone.cuda()\n",
    "    sizeSeq = sizeSeq.cuda().view(-1)\n",
    "    sizePhone = sizePhone.cuda().view(-1)\n",
    "\n",
    "    seq = cut_data(seq.permute(0, 2, 1), sizeSeq).permute(0, 2, 1)\n",
    "    return seq, sizeSeq, phone, sizePhone\n",
    "\n",
    "\n",
    "def get_per(test_dataloader,\n",
    "            cpc_model,\n",
    "            phone_classifier):\n",
    "\n",
    "  downsampling_factor = 160\n",
    "  cpc_model.eval()\n",
    "  phone_classifier.eval()\n",
    "\n",
    "  avgPER = 0\n",
    "\n",
    "  print(\"Starting the PER computation through beam search\")\n",
    "  bar = progressbar.ProgressBar(maxval=len(test_dataloader))\n",
    "  bar.start()\n",
    "\n",
    "  for index, data in enumerate(test_dataloader):\n",
    "\n",
    "    bar.update(index)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      \n",
    "        seq, sizeSeq, phone, sizePhone = prepare_data(data)\n",
    "        c_feature, _, _ = cpc_model(seq, None)\n",
    "        sizeSeq = sizeSeq / downsampling_factor\n",
    "        predictions = torch.nn.functional.softmax(\n",
    "        phone_classifier(c_feature), dim=2).cpu()\n",
    "        phone = phone.cpu()\n",
    "        sizeSeq = sizeSeq.cpu()\n",
    "        sizePhone = sizePhone.cpu()\n",
    "\n",
    "        bs = c_feature.size(0)\n",
    "        data_per = [(predictions[b], sizeSeq[b], phone[b], sizePhone[b],\n",
    "                      0) for b in range(bs)]\n",
    "\n",
    "        with Pool(bs) as p:\n",
    "            poolData = p.map(get_per, data_per)\n",
    "        avgPER += sum([x for x in poolData])\n",
    "        nItems += len(poolData)\n",
    "\n",
    "  bar.finish()\n",
    "\n",
    "  avgPER /= nItems\n",
    "\n",
    "  print(f\"Average PER {avgPER}\")\n",
    "  return avgPER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHsbTS_BBN63"
   },
   "outputs": [],
   "source": [
    "get_per(data_loader_val, cpc_model, char_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-xRwRWP8iEH"
   },
   "source": [
    "## Training from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cU26XI8s_R6j"
   },
   "outputs": [],
   "source": [
    "%cd /content/CPC_audio\n",
    "from cpc.model import CPCEncoder, CPCAR\n",
    "\n",
    "DIM_ENCODER=256\n",
    "DIM_CONTEXT=256\n",
    "KEEP_HIDDEN_VECTOR=False\n",
    "N_LEVELS_CONTEXT=1\n",
    "CONTEXT_RNN=\"LSTM\"\n",
    "N_PREDICTIONS=12\n",
    "LEARNING_RATE=2e-4\n",
    "N_NEGATIVE_SAMPLE =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwInLO2cPmaK"
   },
   "outputs": [],
   "source": [
    "# Several functions that will be necessary to load the data later\n",
    "from cpc.dataset import findAllSeqs, AudioBatchData\n",
    "SIZE_WINDOW = 20480\n",
    "BATCH_SIZE=8\n",
    "def load_dataset(path_dataset, file_extension='.flac', phone_label_dict=None):\n",
    "  data_list, speakers = findAllSeqs(path_dataset, extension=file_extension)\n",
    "  dataset = AudioBatchData(path_dataset, SIZE_WINDOW, data_list, phone_label_dict, len(speakers))\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7idM7CZ_CV-"
   },
   "outputs": [],
   "source": [
    "class CPCModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 AR):\n",
    "\n",
    "        super(CPCModel, self).__init__()\n",
    "        self.gEncoder = encoder\n",
    "        self.gAR = AR\n",
    "\n",
    "    def forward(self, batch_data, label=None):\n",
    "        # TO COMPLETE\n",
    "        encoder_output = self.gEncoder(batch_data)\n",
    "        context_input = encoder_output.permute(0, 2, 1)\n",
    "        context_output = self.gAR(context_input)\n",
    "        return context_output, encoder_output, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HkUlsMcv_OiO"
   },
   "outputs": [],
   "source": [
    "encoder = CPCEncoder(DIM_ENCODER)\n",
    "context = CPCAR(dimEncoded=DIM_ENCODER, dimOutput=DIM_CONTEXT, keepHidden=KEEP_HIDDEN_VECTOR, nLevelsGRU=N_LEVELS_CONTEXT, mode=CONTEXT_RNN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9CEbo9T_Zzf"
   },
   "outputs": [],
   "source": [
    "audio = torchaudio.load(\"/content/My ASR Dataset/200701-072308_yor_7f4_elicit/200701-072308_yor_7f4_elicit_0.wav\")[0]\n",
    "audio = audio.view(1, 1, -1)\n",
    "fresh_cpc_model = CPCModel(encoder, context)\n",
    "context_output, encoder_output, _ = fresh_cpc_model(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cu72Vje8_l5m"
   },
   "outputs": [],
   "source": [
    "# Exercice 2: write the CPC loss\n",
    "# a) Write the negative sampling (with some help)\n",
    "# ERRATUM: it's really hard, the sampling will be provided\n",
    "\n",
    "class CPCCriterion(torch.nn.Module):\n",
    "\n",
    "  def __init__(self,\n",
    "               K,\n",
    "               dim_context,\n",
    "               dim_encoder,\n",
    "               n_negative):\n",
    "    super(CPCCriterion, self).__init__()\n",
    "    self.K_ = K\n",
    "    self.dim_context = dim_context\n",
    "    self.dim_encoder = dim_encoder\n",
    "    self.n_negative = n_negative\n",
    "    self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    self.predictors = torch.nn.ModuleList() \n",
    "    for k in range(self.K_):\n",
    "      # TO COMPLETE !\n",
    "      self.predictors.append(torch.nn.Linear(DIM_CONTEXT, DIM_ENCODER))\n",
    "\n",
    "  def get_prediction_k(self, context_data):\n",
    "\n",
    "    #TO COMPLETE !\n",
    "    # Take a batch of encoded tensors context vectors context_data\n",
    "    # and returns a list of k tensors /phi_k(context_data)\n",
    "    preds = []\n",
    "    for predictors in self.predictors:\n",
    "      preds.append(predictors(context_data))\n",
    "    return preds\n",
    "\n",
    "  def sample_negatives(self, encoded_data):\n",
    "    r\"\"\"\n",
    "    Sample some negative examples in the given encoded data.\n",
    "    Input:\n",
    "    - encoded_data size: B x T x H\n",
    "    Returns\n",
    "    - outputs of size B x (n_negative + 1) x (T - K_) x H\n",
    "      outputs[:, 0, :, :] contains the positive example\n",
    "      outputs[:, 1:, :, :] contains negative example sampled in the batch\n",
    "    - labels, long tensor of size B x (n_negative + 1) x (T - K_)\n",
    "      Since the positive example is always at coordinates 0 for all sequences \n",
    "      in the batch and all timestep in the sequence, labels is just a tensor\n",
    "      full of zeros !\n",
    "    \"\"\"\n",
    "    batch_size, time_size, dim_encoded = encoded_data.size()\n",
    "    window_size = time_size - self.K_\n",
    "    outputs = []\n",
    "\n",
    "    neg_ext = encoded_data.contiguous().view(-1, dim_encoded)\n",
    "    n_elem_sampled = self.n_negative * window_size * batch_size\n",
    "    # Draw nNegativeExt * batchSize negative samples anywhere in the batch\n",
    "    batch_idx = torch.randint(low=0, high=batch_size,\n",
    "                              size=(n_elem_sampled, ),\n",
    "                              device=encoded_data.device)\n",
    "\n",
    "    seq_idx = torch.randint(low=1, high=time_size,\n",
    "                            size=(n_elem_sampled, ),\n",
    "                            device=encoded_data.device)\n",
    "\n",
    "    base_idx = torch.arange(0, window_size, device=encoded_data.device)\n",
    "    base_idx = base_idx.view(1, 1, window_size)\n",
    "    base_idx = base_idx.expand(1, self.n_negative, window_size)\n",
    "    base_idx = base_idx.expand(batch_size, self.n_negative, window_size)\n",
    "    seq_idx += base_idx.contiguous().view(-1)\n",
    "    seq_idx = torch.remainder(seq_idx, time_size)\n",
    "\n",
    "    ext_idx = seq_idx + batch_idx * time_size\n",
    "    neg_ext = neg_ext[ext_idx].view(batch_size, self.n_negative,\n",
    "                                    window_size, dim_encoded)\n",
    "\n",
    "    label_loss = torch.zeros((batch_size * window_size),\n",
    "                              dtype=torch.long,\n",
    "                              device=encoded_data.device)\n",
    "\n",
    "    for k in range(1, self.K_ + 1):\n",
    "\n",
    "      # Positive samples\n",
    "      if k < self.K_:\n",
    "          pos_seq = encoded_data[:, k:-(self.K_-k)]\n",
    "      else:\n",
    "          pos_seq = encoded_data[:, k:]\n",
    "\n",
    "      pos_seq = pos_seq.view(batch_size, 1, pos_seq.size(1), dim_encoded)\n",
    "      full_seq = torch.cat((pos_seq, neg_ext), dim=1)\n",
    "      outputs.append(full_seq)\n",
    "\n",
    "    return outputs, label_loss\n",
    "\n",
    "  def forward(self, encoded_data, context_data):\n",
    "\n",
    "    # TO COMPLETE:\n",
    "    # Perform the full cpc criterion\n",
    "    # Returns 2 values:\n",
    "    # - the average classification loss avg_loss\n",
    "    # - the average classification acuracy avg_acc\n",
    "    encoded_data = encoded_data.permute(0, 2, 1)\n",
    "    avg_acc = []\n",
    "    avg_loss = 0\n",
    "\n",
    "    pos_neg_samples, labels = self.sample_negatives(encoded_data)\n",
    "    num_timesteps, num_samples = pos_neg_samples[0].size(2), pos_neg_samples[0].size(1)\n",
    "    predictions = self.get_prediction_k(context_data[:, :num_timesteps, :].reshape(-1, self.dim_encoder))\n",
    "\n",
    "    for head in range(self.K_):\n",
    "      samples = pos_neg_samples[head].permute(0,2,1,3).reshape(-1, num_samples, self.dim_encoder)\n",
    "      # print(\"samples shape is:\", samples.shape)\n",
    "      # print(\"prediction for head k is:\", predictions[head].shape)\n",
    "      pred = (samples@predictions[head].unsqueeze(1).transpose(1,2)).squeeze(-1)\n",
    "      # print(\"dot product is:\", pred.shape)\n",
    "      avg_loss += self.criterion(pred, labels) #.cpu()\n",
    "      avg_acc.append((pred.argmax(dim=1) == labels[:len(pred)]).sum().cpu())\n",
    "\n",
    "    avg_loss = avg_loss/self.K_\n",
    "    avg_acc = torch.mean(torch.tensor(avg_acc, dtype=torch.float32))\n",
    "    del pos_neg_samples\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kv0I9JYk_8LQ"
   },
   "outputs": [],
   "source": [
    "audio = torchaudio.load(\"/content/My ASR Dataset/200701-072308_yor_7f4_elicit/200701-072308_yor_7f4_elicit_0.wav\")[0]\n",
    "audio = audio.view(1, 1, -1)\n",
    "cpc_criterion = CPCCriterion(N_PREDICTIONS, DIM_ENCODER, \n",
    "                             DIM_CONTEXT, N_NEGATIVE_SAMPLE)\n",
    "context_output, encoder_output, _ = fresh_cpc_model(audio)\n",
    "loss, avg = cpc_criterion(encoder_output, context_output)\n",
    "print(loss, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KrwgXUVKALax"
   },
   "outputs": [],
   "source": [
    "parameters = list(cpc_criterion.parameters()) + list(cpc_model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VlMEXL-0AU73"
   },
   "outputs": [],
   "source": [
    "fresh_dataset_train = load_dataset('/content/My ASR Dataset/')\n",
    "fresh_dataset_val = load_dataset('/content/My ASR Dataset/')\n",
    "fresh_data_loader_train = fresh_dataset_train.getDataLoader(BATCH_SIZE, \"speaker\", True)\n",
    "fresh_data_loader_val = fresh_dataset_train.getDataLoader(BATCH_SIZE, \"sequence\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gbavxcDaAoZD"
   },
   "outputs": [],
   "source": [
    "def train_step(data_loader,\n",
    "               cpc_model,\n",
    "               cpc_criterion,\n",
    "               optimizer):\n",
    "  \n",
    "  avg_loss = 0\n",
    "  avg_acc = 0\n",
    "  n_items = 0\n",
    "\n",
    "  for step, data in enumerate(data_loader):\n",
    "\n",
    "    # COMPLETE\n",
    "    cpc_model.train()\n",
    "    x, y = data\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    context_output, encoder_output, _ = cpc_model(x)\n",
    "    loss, acc = cpc_criterion(encoder_output, context_output)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    avg_loss += loss\n",
    "    avg_acc += acc\n",
    "    n_items+=1\n",
    "    del x\n",
    "    del y\n",
    "  avg_loss = avg_loss/n_items\n",
    "  avg_acc = avg_acc/n_items\n",
    "  return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-4x1cWGBAhB"
   },
   "outputs": [],
   "source": [
    "def validation_step(data_loader,\n",
    "                    cpc_model,\n",
    "                    cpc_criterion):\n",
    "  \n",
    "  avg_loss = 0\n",
    "  avg_acc = 0\n",
    "  n_items = 0\n",
    "\n",
    "  for step, data in enumerate(data_loader):\n",
    "\n",
    "    # TO COMPLETE\n",
    "    cpc_model.eval()\n",
    "    x, y = data\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    context_output, encoder_output, _ = cpc_model(x)\n",
    "    loss, acc = cpc_criterion(encoder_output, context_output)\n",
    "    avg_loss += loss\n",
    "    avg_acc += acc\n",
    "    n_items+=1\n",
    "    del x\n",
    "    del y\n",
    "  avg_loss = avg_loss/n_items\n",
    "  avg_acc = avg_acc/n_items\n",
    "\n",
    "  return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CdM_HK67BEE-"
   },
   "outputs": [],
   "source": [
    "def run(train_loader,\n",
    "        val_loader,\n",
    "        cpc_model,\n",
    "        cpc_criterion,\n",
    "        optimizer,\n",
    "        n_epochs):\n",
    "  \n",
    "  cpc_model = cpc_model.to(device)\n",
    "  cpc_criterion = cpc_criterion.to(device)\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "\n",
    "    \n",
    "    print(f\"Running epoch {epoch+1} / {n_epochs}\")\n",
    "    avg_loss_train, avg_acc_train = train_step(train_loader, cpc_model, cpc_criterion, optimizer)\n",
    "    print(\"----------------------\")\n",
    "    print(f\"Training dataset\")\n",
    "    print(f\"- average loss : {avg_loss_train}\")\n",
    "    print(f\"- average acuracy : {avg_acc_train}\")\n",
    "    print(\"----------------------\")\n",
    "\n",
    "    avg_loss_val, avg_acc_val = validation_step(val_loader, cpc_model, cpc_criterion)\n",
    "    print(f\"Validation dataset\")\n",
    "    print(f\"- average loss : {avg_loss_train}\")\n",
    "    print(f\"- average acuracy : {avg_acc_train}\")\n",
    "    print(\"----------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLIrDkzmBHFS"
   },
   "outputs": [],
   "source": [
    "run(fresh_data_loader_train, fresh_data_loader_val, fresh_cpc_model, cpc_criterion, optimizer, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hvwjgPQ5CVQb"
   },
   "source": [
    "### Checkpoint Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91M3XuwGCEuB"
   },
   "outputs": [],
   "source": [
    "torch.save(\"fresh_model_checkpoint.pth\", fresh_cpc_model.statedict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3WtBKWYoC24C"
   },
   "source": [
    "### Train a Character Classifier Using Fresh CPC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yl2RVbJVC0pt"
   },
   "outputs": [],
   "source": [
    "## Reload the Initial Dataset\n",
    "\n",
    "%cd CPC_audio\n",
    "from cpc.eval.common_voices_eval import SingleSequenceDataset, parseSeqLabels, findAllSeqs\n",
    "path_train_data_per = '/content/My ASR Dataset'\n",
    "path_val_data_per = '/content/My ASR Dataset'\n",
    "BATCH_SIZE=8\n",
    "N_CHARS = len(token_to_character)\n",
    "\n",
    "data_train_per, _ = findAllSeqs(path_train_data_per, extension='.wav')\n",
    "dataset_train_non_aligned = SingleSequenceDataset(path_train_data_per, data_train_per, label_dict)\n",
    "data_loader_train = torch.utils.data.DataLoader(dataset_train_non_aligned, batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True)\n",
    "\n",
    "data_val_per, _ = findAllSeqs(path_val_data_per, extension='.wav')\n",
    "dataset_val_non_aligned = SingleSequenceDataset(path_val_data_per, data_val_per, label_dict)\n",
    "data_loader_val = torch.utils.data.DataLoader(dataset_val_non_aligned, batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VS_de4YDDxvO"
   },
   "outputs": [],
   "source": [
    "#define optimizer and model\n",
    "checkpoint_path = 'fresh_model_checkpoint.pth'\n",
    "fresh_cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n",
    "fresh_cpc_model = fresh_cpc_model.to(device)\n",
    "\n",
    "#define character classifier\n",
    "char_classifier = CharClassifier(HIDDEN_CONTEXT_MODEL, N_CHARS)\n",
    "char_classifier = char_classifier.to(device)\n",
    "\n",
    "parameters = list(char_classifier.parameters()) + list(cpc_model.parameters())\n",
    "LEARNING_RATE = 2e-4\n",
    "optimizer = optim.Adam(parameters, lr=LEARNING_RATE)\n",
    "optimizer_frozen = optim.Adam(list(char_classifier.parameters()), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGJH4RwfE4TJ"
   },
   "outputs": [],
   "source": [
    "run_ctc(fresh_cpc_model, char_classifier, loss_ctc, data_loader_train, data_loader_val, optimizer, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8EhsZBK7FPnG"
   },
   "source": [
    "### Compute CER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ad9QNBIqFPWC"
   },
   "outputs": [],
   "source": [
    "get_per(data_loader_val, fresh_cpc_model, char_classifier)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Ibrahim SR Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
