{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of CTC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zim12RtTt8Wx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9570c403-dc1a-4d82-ce8a-7257ab9ebb8f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFJDhJOStteY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "69d89056-8faa-4f6b-e235-368f271d05cc"
      },
      "source": [
        "!pip install soundfile\n",
        "!pip install torchaudio"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Installing collected packages: soundfile\n",
            "Successfully installed soundfile-0.10.3.post1\n",
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/0a/40e53c686c2af65b2a4e818d11d9b76fa79178440caf99f3ceb2a32c3b04/torchaudio-0.5.1-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.6/dist-packages (from torchaudio) (1.5.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->torchaudio) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->torchaudio) (1.18.5)\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUbR4Q5AwGHR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "5ada5062-ae61-4b6a-d46c-2a48184c4313"
      },
      "source": [
        "! git clone https://github.com/facebookresearch/CPC_audio.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CPC_audio'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/84)\u001b[K\rremote: Counting objects:   2% (2/84)\u001b[K\rremote: Counting objects:   3% (3/84)\u001b[K\rremote: Counting objects:   4% (4/84)\u001b[K\rremote: Counting objects:   5% (5/84)\u001b[K\rremote: Counting objects:   7% (6/84)\u001b[K\rremote: Counting objects:   8% (7/84)\u001b[K\rremote: Counting objects:   9% (8/84)\u001b[K\rremote: Counting objects:  10% (9/84)\u001b[K\rremote: Counting objects:  11% (10/84)\u001b[K\rremote: Counting objects:  13% (11/84)\u001b[K\rremote: Counting objects:  14% (12/84)\u001b[K\rremote: Counting objects:  15% (13/84)\u001b[K\rremote: Counting objects:  16% (14/84)\u001b[K\rremote: Counting objects:  17% (15/84)\u001b[K\rremote: Counting objects:  19% (16/84)\u001b[K\rremote: Counting objects:  20% (17/84)\u001b[K\rremote: Counting objects:  21% (18/84)\u001b[K\rremote: Counting objects:  22% (19/84)\u001b[K\rremote: Counting objects:  23% (20/84)\u001b[K\rremote: Counting objects:  25% (21/84)\u001b[K\rremote: Counting objects:  26% (22/84)\u001b[K\rremote: Counting objects:  27% (23/84)\u001b[K\rremote: Counting objects:  28% (24/84)\u001b[K\rremote: Counting objects:  29% (25/84)\u001b[K\rremote: Counting objects:  30% (26/84)\u001b[K\rremote: Counting objects:  32% (27/84)\u001b[K\rremote: Counting objects:  33% (28/84)\u001b[K\rremote: Counting objects:  34% (29/84)\u001b[K\rremote: Counting objects:  35% (30/84)\u001b[K\rremote: Counting objects:  36% (31/84)\u001b[K\rremote: Counting objects:  38% (32/84)\u001b[K\rremote: Counting objects:  39% (33/84)\u001b[K\rremote: Counting objects:  40% (34/84)\u001b[K\rremote: Counting objects:  41% (35/84)\u001b[K\rremote: Counting objects:  42% (36/84)\u001b[K\rremote: Counting objects:  44% (37/84)\u001b[K\rremote: Counting objects:  45% (38/84)\u001b[K\rremote: Counting objects:  46% (39/84)\u001b[K\rremote: Counting objects:  47% (40/84)\u001b[K\rremote: Counting objects:  48% (41/84)\u001b[K\rremote: Counting objects:  50% (42/84)\u001b[K\rremote: Counting objects:  51% (43/84)\u001b[K\rremote: Counting objects:  52% (44/84)\u001b[K\rremote: Counting objects:  53% (45/84)\u001b[K\rremote: Counting objects:  54% (46/84)\u001b[K\rremote: Counting objects:  55% (47/84)\u001b[K\rremote: Counting objects:  57% (48/84)\u001b[K\rremote: Counting objects:  58% (49/84)\u001b[K\rremote: Counting objects:  59% (50/84)\u001b[K\rremote: Counting objects:  60% (51/84)\u001b[K\rremote: Counting objects:  61% (52/84)\u001b[K\rremote: Counting objects:  63% (53/84)\u001b[K\rremote: Counting objects:  64% (54/84)\u001b[K\rremote: Counting objects:  65% (55/84)\u001b[K\rremote: Counting objects:  66% (56/84)\u001b[K\rremote: Counting objects:  67% (57/84)\u001b[K\rremote: Counting objects:  69% (58/84)\u001b[K\rremote: Counting objects:  70% (59/84)\u001b[K\rremote: Counting objects:  71% (60/84)\u001b[K\rremote: Counting objects:  72% (61/84)\u001b[K\rremote: Counting objects:  73% (62/84)\u001b[K\rremote: Counting objects:  75% (63/84)\u001b[K\rremote: Counting objects:  76% (64/84)\u001b[K\rremote: Counting objects:  77% (65/84)\u001b[K\rremote: Counting objects:  78% (66/84)\u001b[K\rremote: Counting objects:  79% (67/84)\u001b[K\rremote: Counting objects:  80% (68/84)\u001b[K\rremote: Counting objects:  82% (69/84)\u001b[K\rremote: Counting objects:  83% (70/84)\u001b[K\rremote: Counting objects:  84% (71/84)\u001b[K\rremote: Counting objects:  85% (72/84)\u001b[K\rremote: Counting objects:  86% (73/84)\u001b[K\rremote: Counting objects:  88% (74/84)\u001b[K\rremote: Counting objects:  89% (75/84)\u001b[K\rremote: Counting objects:  90% (76/84)\u001b[K\rremote: Counting objects:  91% (77/84)\u001b[K\rremote: Counting objects:  92% (78/84)\u001b[K\rremote: Counting objects:  94% (79/84)\u001b[K\rremote: Counting objects:  95% (80/84)\u001b[K\rremote: Counting objects:  96% (81/84)\u001b[K\rremote: Counting objects:  97% (82/84)\u001b[K\rremote: Counting objects:  98% (83/84)\u001b[K\rremote: Counting objects: 100% (84/84)\u001b[K\rremote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects:   1% (1/65)\u001b[K\rremote: Compressing objects:   3% (2/65)\u001b[K\rremote: Compressing objects:   4% (3/65)\u001b[K\rremote: Compressing objects:   6% (4/65)\u001b[K\rremote: Compressing objects:   7% (5/65)\u001b[K\rremote: Compressing objects:   9% (6/65)\u001b[K\rremote: Compressing objects:  10% (7/65)\u001b[K\rremote: Compressing objects:  12% (8/65)\u001b[K\rremote: Compressing objects:  13% (9/65)\u001b[K\rremote: Compressing objects:  15% (10/65)\u001b[K\rremote: Compressing objects:  16% (11/65)\u001b[K\rremote: Compressing objects:  18% (12/65)\u001b[K\rremote: Compressing objects:  20% (13/65)\u001b[K\rremote: Compressing objects:  21% (14/65)\u001b[K\rremote: Compressing objects:  23% (15/65)\u001b[K\rremote: Compressing objects:  24% (16/65)\u001b[K\rremote: Compressing objects:  26% (17/65)\u001b[K\rremote: Compressing objects:  27% (18/65)\u001b[K\rremote: Compressing objects:  29% (19/65)\u001b[K\rremote: Compressing objects:  30% (20/65)\u001b[K\rremote: Compressing objects:  32% (21/65)\u001b[K\rremote: Compressing objects:  33% (22/65)\u001b[K\rremote: Compressing objects:  35% (23/65)\u001b[K\rremote: Compressing objects:  36% (24/65)\u001b[K\rremote: Compressing objects:  38% (25/65)\u001b[K\rremote: Compressing objects:  40% (26/65)\u001b[K\rremote: Compressing objects:  41% (27/65)\u001b[K\rremote: Compressing objects:  43% (28/65)\u001b[K\rremote: Compressing objects:  44% (29/65)\u001b[K\rremote: Compressing objects:  46% (30/65)\u001b[K\rremote: Compressing objects:  47% (31/65)\u001b[K\rremote: Compressing objects:  49% (32/65)\u001b[K\rremote: Compressing objects:  50% (33/65)\u001b[K\rremote: Compressing objects:  52% (34/65)\u001b[K\rremote: Compressing objects:  53% (35/65)\u001b[K\rremote: Compressing objects:  55% (36/65)\u001b[K\rremote: Compressing objects:  56% (37/65)\u001b[K\rremote: Compressing objects:  58% (38/65)\u001b[K\rremote: Compressing objects:  60% (39/65)\u001b[K\rremote: Compressing objects:  61% (40/65)\u001b[K\rremote: Compressing objects:  63% (41/65)\u001b[K\rremote: Compressing objects:  64% (42/65)\u001b[K\rremote: Compressing objects:  66% (43/65)\u001b[K\rremote: Compressing objects:  67% (44/65)\u001b[K\rremote: Compressing objects:  69% (45/65)\u001b[K\rremote: Compressing objects:  70% (46/65)\u001b[K\rremote: Compressing objects:  72% (47/65)\u001b[K\rremote: Compressing objects:  73% (48/65)\u001b[K\rremote: Compressing objects:  75% (49/65)\u001b[K\rremote: Compressing objects:  76% (50/65)\u001b[K\rremote: Compressing objects:  78% (51/65)\u001b[K\rremote: Compressing objects:  80% (52/65)\u001b[K\rremote: Compressing objects:  81% (53/65)\u001b[K\rremote: Compressing objects:  83% (54/65)\u001b[K\rremote: Compressing objects:  84% (55/65)\u001b[K\rremote: Compressing objects:  86% (56/65)\u001b[K\rremote: Compressing objects:  87% (57/65)\u001b[K\rremote: Compressing objects:  89% (58/65)\u001b[K\rremote: Compressing objects:  90% (59/65)\u001b[K\rremote: Compressing objects:  92% (60/65)\u001b[K\rremote: Compressing objects:  93% (61/65)\u001b[K\rremote: Compressing objects:  95% (62/65)\u001b[K\rremote: Compressing objects:  96% (63/65)\u001b[K\rremote: Compressing objects:  98% (64/65)\u001b[K\rremote: Compressing objects: 100% (65/65)\u001b[K\rremote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "Unpacking objects:   1% (1/84)   \rUnpacking objects:   2% (2/84)   \rUnpacking objects:   3% (3/84)   \rUnpacking objects:   4% (4/84)   \rUnpacking objects:   5% (5/84)   \rUnpacking objects:   7% (6/84)   \rUnpacking objects:   8% (7/84)   \rUnpacking objects:   9% (8/84)   \rUnpacking objects:  10% (9/84)   \rUnpacking objects:  11% (10/84)   \rUnpacking objects:  13% (11/84)   \rUnpacking objects:  14% (12/84)   \rUnpacking objects:  15% (13/84)   \rUnpacking objects:  16% (14/84)   \rUnpacking objects:  17% (15/84)   \rUnpacking objects:  19% (16/84)   \rUnpacking objects:  20% (17/84)   \rUnpacking objects:  21% (18/84)   \rUnpacking objects:  22% (19/84)   \rUnpacking objects:  23% (20/84)   \rUnpacking objects:  25% (21/84)   \rUnpacking objects:  26% (22/84)   \rUnpacking objects:  27% (23/84)   \rUnpacking objects:  28% (24/84)   \rUnpacking objects:  29% (25/84)   \rUnpacking objects:  30% (26/84)   \rUnpacking objects:  32% (27/84)   \rUnpacking objects:  33% (28/84)   \rUnpacking objects:  34% (29/84)   \rUnpacking objects:  35% (30/84)   \rUnpacking objects:  36% (31/84)   \rUnpacking objects:  38% (32/84)   \rUnpacking objects:  39% (33/84)   \rUnpacking objects:  40% (34/84)   \rUnpacking objects:  41% (35/84)   \rUnpacking objects:  42% (36/84)   \rUnpacking objects:  44% (37/84)   \rUnpacking objects:  45% (38/84)   \rUnpacking objects:  46% (39/84)   \rUnpacking objects:  47% (40/84)   \rUnpacking objects:  48% (41/84)   \rUnpacking objects:  50% (42/84)   \rUnpacking objects:  51% (43/84)   \rUnpacking objects:  52% (44/84)   \rUnpacking objects:  53% (45/84)   \rUnpacking objects:  54% (46/84)   \rUnpacking objects:  55% (47/84)   \rUnpacking objects:  57% (48/84)   \rUnpacking objects:  58% (49/84)   \rUnpacking objects:  59% (50/84)   \rUnpacking objects:  60% (51/84)   \rUnpacking objects:  61% (52/84)   \rUnpacking objects:  63% (53/84)   \rUnpacking objects:  64% (54/84)   \rUnpacking objects:  65% (55/84)   \rUnpacking objects:  66% (56/84)   \rUnpacking objects:  67% (57/84)   \rUnpacking objects:  69% (58/84)   \rUnpacking objects:  70% (59/84)   \rUnpacking objects:  71% (60/84)   \rUnpacking objects:  72% (61/84)   \rUnpacking objects:  73% (62/84)   \rUnpacking objects:  75% (63/84)   \rUnpacking objects:  76% (64/84)   \rUnpacking objects:  77% (65/84)   \rUnpacking objects:  78% (66/84)   \rUnpacking objects:  79% (67/84)   \rremote: Total 84 (delta 13), reused 75 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects:  80% (68/84)   \rUnpacking objects:  82% (69/84)   \rUnpacking objects:  83% (70/84)   \rUnpacking objects:  84% (71/84)   \rUnpacking objects:  85% (72/84)   \rUnpacking objects:  86% (73/84)   \rUnpacking objects:  88% (74/84)   \rUnpacking objects:  89% (75/84)   \rUnpacking objects:  90% (76/84)   \rUnpacking objects:  91% (77/84)   \rUnpacking objects:  92% (78/84)   \rUnpacking objects:  94% (79/84)   \rUnpacking objects:  95% (80/84)   \rUnpacking objects:  96% (81/84)   \rUnpacking objects:  97% (82/84)   \rUnpacking objects:  98% (83/84)   \rUnpacking objects: 100% (84/84)   \rUnpacking objects: 100% (84/84), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpP5xHufwSYQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "outputId": "aeb32949-93a5-470e-948a-4cc0ccf7f90c"
      },
      "source": [
        "%cd /content/CPC_audio\n",
        "!python setup.py develop"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CPC_audio\n",
            "Compiling cpc/eval/ABX/dtw.pyx because it changed.\n",
            "[1/1] Cythonizing cpc/eval/ABX/dtw.pyx\n",
            "running develop\n",
            "running egg_info\n",
            "creating CPC_audio.egg-info\n",
            "writing CPC_audio.egg-info/PKG-INFO\n",
            "writing dependency_links to CPC_audio.egg-info/dependency_links.txt\n",
            "writing top-level names to CPC_audio.egg-info/top_level.txt\n",
            "writing manifest file 'CPC_audio.egg-info/SOURCES.txt'\n",
            "writing manifest file 'CPC_audio.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "building 'cpc.eval.ABX.dtw' extension\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.6\n",
            "creating build/temp.linux-x86_64-3.6/cpc\n",
            "creating build/temp.linux-x86_64-3.6/cpc/eval\n",
            "creating build/temp.linux-x86_64-3.6/cpc/eval/ABX\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c cpc/eval/ABX/dtw.c -o build/temp.linux-x86_64-3.6/cpc/eval/ABX/dtw.o\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/arrayobject.h:4\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kcpc/eval/ABX/dtw.c:625\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "creating build/lib.linux-x86_64-3.6/cpc\n",
            "creating build/lib.linux-x86_64-3.6/cpc/eval\n",
            "creating build/lib.linux-x86_64-3.6/cpc/eval/ABX\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cpc/eval/ABX/dtw.o -o build/lib.linux-x86_64-3.6/cpc/eval/ABX/dtw.cpython-36m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.6/cpc/eval/ABX/dtw.cpython-36m-x86_64-linux-gnu.so -> cpc/eval/ABX\n",
            "Creating /usr/local/lib/python3.6/dist-packages/CPC-audio.egg-link (link to .)\n",
            "Adding CPC-audio 1.0 to easy-install.pth file\n",
            "\n",
            "Installed /content/CPC_audio\n",
            "Processing dependencies for CPC-audio==1.0\n",
            "Finished processing dependencies for CPC-audio==1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ppRP3ZqP8vZP",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchaudio"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YZ1Wywx3BNGS"
      },
      "source": [
        "# Part 1 : contrastive predictive coding\n",
        "\n",
        "Contrastive Predictive Coding (CPC) is a method of unsupervised training for speech models. The idea behind it is pretty simple:\n",
        "\n",
        "\n",
        "1.   The raw audio wave is passed through a convolutional network: the ```encoder```\n",
        "2.   Then, the encoder's output is given to a recurrent network the ```context```\n",
        "3. A third party network, the ```prediction_network``` will try to predict the  future embeddings of the encoder using the output of the context network.\n",
        "\n",
        "In order to avoid a collapse to trivial solutions, the prediction_network doesn't try to reconstruct the future features. Instead, using the context output $c_t$ at time $t$ it is trained to discriminate the real  encoder representatioin $g_{t+k}$ at time $t+k$ from several other features $(g_n)_n$ taken elsewhere in the batch. Thus the loss becomes:\n",
        "\n",
        "\\\\[ \\mathcal{L}_c = - \\frac{1}{K} \\sum_{k=1}^K \\text{Cross_entropy}(\\phi_k(c_t), g_{t+k}) \\\\]\n",
        "\n",
        "Or:\n",
        "\n",
        "\\\\[ \\mathcal{L}_c = - \\frac{1}{K} \\sum_{k=1}^K \\log \\frac{ \\exp\\left(\\phi_k(c_t)^\\top g_{t+k}\\right) }{  \\sum_{\\mathbf{n}\\in\\mathcal{N}_t} \\exp\\left(\\phi_k(c_t)^\\top g_n\\right)} \\\\]\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "*   $\\phi_k$ is the prediction network for the kth timestep\n",
        "*   $\\mathcal{N}_t$ is the set of all negative examples sampled for timestep $t$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "frPFYXuPfNPs"
      },
      "source": [
        "## Exercice 1 : Building the model\n",
        "\n",
        "In this exercise, we will build and train a small CPC model using the repository CPC_audio.\n",
        "\n",
        "The code below loads a context and an encoder newtorks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8g-xPSdLRdui",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c94cff8c-3bd6-4e1d-a09d-f325e5adcd35"
      },
      "source": [
        "%cd /content/CPC_audio\n",
        "from cpc.model import CPCEncoder, CPCAR\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "DIM_ENCODER=256\n",
        "DIM_CONTEXT=256\n",
        "KEEP_HIDDEN_VECTOR=False\n",
        "N_LEVELS_CONTEXT=1\n",
        "CONTEXT_RNN=\"LSTM\"\n",
        "N_PREDICTIONS=12\n",
        "LEARNING_RATE=2e-4\n",
        "N_NEGATIVE_SAMPLE =128"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CPC_audio\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Wx8WkrQk9bQ",
        "colab": {}
      },
      "source": [
        "encoder = CPCEncoder(DIM_ENCODER).to(device)\n",
        "context = CPCAR(DIM_ENCODER, DIM_CONTEXT, KEEP_HIDDEN_VECTOR, 1, mode=CONTEXT_RNN).to(device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f9BrweAIla4J",
        "colab": {}
      },
      "source": [
        "# Several functions that will be necessary to load the data later\n",
        "from cpc.dataset import findAllSeqs, AudioBatchData, parseSeqLabels\n",
        "SIZE_WINDOW = 20483\n",
        "BATCH_SIZE=8\n",
        "def load_dataset(path_dataset, file_extension='.wav', phone_label_dict=None):\n",
        "  data_list, speakers = findAllSeqs(path_dataset, extension=file_extension)\n",
        "  dataset = AudioBatchData(path_dataset, SIZE_WINDOW, data_list, phone_label_dict, len(speakers))\n",
        "  return dataset"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "58pQ7ysXk9ZO"
      },
      "source": [
        "Now build a new class, ```CPCModel``` which will"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rR5IYRTpRF8T",
        "colab": {}
      },
      "source": [
        "class CPCModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 encoder,\n",
        "                 AR):\n",
        "\n",
        "        super(CPCModel, self).__init__()\n",
        "        self.gEncoder = encoder\n",
        "        self.gAR = AR\n",
        "\n",
        "    def forward(self, batch_data):\n",
        "        \n",
        "\n",
        "        encoder_output = self.gEncoder(batch_data)\n",
        "        #print(encoder_output.shape)\n",
        "        # The output of the encoder data does not have the good format \n",
        "        # indeed it is Batch_size x Hidden_size x temp size\n",
        "        # while the context requires Batch_size  x temp size x Hidden_size\n",
        "        # thus you need to permute\n",
        "        context_input = encoder_output.permute(0, 2, 1)\n",
        "\n",
        "        context_output = self.gAR(context_input)\n",
        "        #print(context_output.shape)\n",
        "        return context_output, encoder_output"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XUJgm6Rl4vS4"
      },
      "source": [
        "Let's test your code !\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D1x1n4mv4y03",
        "colab": {}
      },
      "source": [
        "audio = torchaudio.load(\"/content/drive/My Drive/Speec Recognition/DATA/train/200624-151721_swa_5a1_elicit/200624-151721_swa_5a1_elicit_0.wav\")[0]\n",
        "audio = audio.view(1, 1, -1)\n",
        "cpc_model = CPCModel(encoder, context).to(device)\n",
        "context_output, encoder_output = cpc_model(audio.to(device))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIVibQsn2Jrz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "c54f23d1-fcb6-4aea-e4dc-b83223100b81"
      },
      "source": [
        "print(context_output, encoder_output)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.1750,  0.1437, -0.0775,  ...,  0.0054,  0.0782, -0.0268],\n",
            "         [ 0.2582,  0.1703, -0.1478,  ...,  0.0690,  0.1857, -0.0867],\n",
            "         [ 0.2954,  0.1914, -0.1851,  ...,  0.0868,  0.2349, -0.1277],\n",
            "         ...,\n",
            "         [ 0.3484,  0.2164, -0.2242,  ...,  0.0881,  0.3080, -0.1733],\n",
            "         [ 0.3527,  0.2125, -0.2232,  ...,  0.0855,  0.3140, -0.1733],\n",
            "         [ 0.3132,  0.2142, -0.2721,  ...,  0.0389,  0.2069, -0.1660]]],\n",
            "       device='cuda:0', grad_fn=<CudnnRnnBackward>) tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [0.2708, 0.8965, 0.8965,  ..., 0.8639, 0.8853, 1.7936],\n",
            "         [0.5071, 0.9558, 0.9558,  ..., 0.9638, 0.9599, 1.6318],\n",
            "         ...,\n",
            "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "         [1.3468, 0.7271, 0.7271,  ..., 0.7315, 0.7303, 0.0000],\n",
            "         [0.8451, 0.3271, 0.3271,  ..., 0.3295, 0.3361, 0.3586]]],\n",
            "       device='cuda:0', grad_fn=<ReluBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X27ce8Hy3C2p"
      },
      "source": [
        "## Exercise 2 : CPC loss\n",
        "\n",
        "We will define a class ```CPCCriterion``` which will hold the prediction networks $\\phi_k$ defined above and perform the classification loss $\\mathcal{L}_c$.\n",
        "\n",
        "a) In this exercise, the $\\phi_k$ will be a linear transform, ie:\n",
        "\n",
        "\\\\[ \\phi_k(c_t) = \\mathbf{A}_k c_t\\\\]\n",
        "\n",
        "Using the class [torch.nn.Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear), define the transformations $\\phi_k$ in the code below and complete the function ```get_prediction_k``` which computes $\\phi_k(c_t)$ for a given batch of vectors $c_t$.\n",
        "\n",
        "b) Using both ```get_prediction_k```  and ```sample_negatives``` defined below, write the forward function which will take as input two batches of features $c_t$ and $g_t$ and outputs the classification loss $\\mathcal{L}_c$ and the average acuracy for all predictions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UlAs-z1fBc3W",
        "colab": {}
      },
      "source": [
        "# Exercice 2: write the CPC loss\n",
        "# a) Write the negative sampling (with some help)\n",
        "# ERRATUM: it's really hard, the sampling will be provided\n",
        "\n",
        "class CPCCriterion(torch.nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               K,\n",
        "               dim_context,\n",
        "               dim_encoder,\n",
        "               n_negative):\n",
        "    super(CPCCriterion, self).__init__()\n",
        "    self.K_ = K\n",
        "    self.dim_context = dim_context\n",
        "    self.dim_encoder = dim_encoder\n",
        "    self.n_negative = n_negative\n",
        "\n",
        "    self.predictors = torch.nn.ModuleList() \n",
        "    for k in range(self.K_):\n",
        "      # TO COMPLETE !\n",
        "      \n",
        "      # A affine transformation in pytorch is equivalent to a nn.Linear layer\n",
        "      # To get a linear transformation you must set bias=False\n",
        "      # input dimension of the layer = dimension of the encoder\n",
        "      # output dimension of the layer = dimension of the context\n",
        "      self.predictors.append(torch.nn.Linear(dim_context, dim_encoder, bias=False))\n",
        "\n",
        "  def get_prediction_k(self, context_data):\n",
        "\n",
        "    #TO COMPLETE !\n",
        "    output = [] \n",
        "    # For each time step k\n",
        "    for k in range(self.K_):\n",
        "\n",
        "      # We need to compute phi_k = A_k * c_t\n",
        "      phi_k = self.predictors[k](context_data)\n",
        "      output.append(phi_k)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "  def sample_negatives(self, encoded_data):\n",
        "    r\"\"\"\n",
        "    Sample some negative examples in the given encoded data.\n",
        "    Input:\n",
        "    - encoded_data size: B x T x H\n",
        "    Returns\n",
        "    - outputs of size B x (n_negative + 1) x (T - K_) x H\n",
        "      outputs[:, 0, :, :] contains the positive example\n",
        "      outputs[:, 1:, :, :] contains negative example sampled in the batch\n",
        "    - labels, long tensor of size B x (T - K_)\n",
        "      Since the positive example is always at coordinates 0 for all sequences \n",
        "      in the batch and all timestep in the sequence, labels is just a tensor\n",
        "      full of zeros !\n",
        "    \"\"\"\n",
        "    batch_size, time_size, dim_encoded = encoded_data.size()\n",
        "    window_size = time_size - self.K_\n",
        "    outputs = []\n",
        "\n",
        "    neg_ext = encoded_data.contiguous().view(-1, dim_encoded)\n",
        "    n_elem_sampled = self.n_negative * window_size * batch_size\n",
        "    # Draw nNegativeExt * batchSize negative samples anywhere in the batch\n",
        "    batch_idx = torch.randint(low=0, high=batch_size,\n",
        "                              size=(n_elem_sampled, ),\n",
        "                              device=encoded_data.device)\n",
        "\n",
        "    seq_idx = torch.randint(low=1, high=time_size,\n",
        "                            size=(n_elem_sampled, ),\n",
        "                            device=encoded_data.device)\n",
        "\n",
        "    base_idx = torch.arange(0, window_size, device=encoded_data.device)\n",
        "    base_idx = base_idx.view(1, 1, window_size)\n",
        "    base_idx = base_idx.expand(1, self.n_negative, window_size)\n",
        "    base_idx = base_idx.expand(batch_size, self.n_negative, window_size)\n",
        "    seq_idx += base_idx.contiguous().view(-1)\n",
        "    seq_idx = torch.remainder(seq_idx, time_size)\n",
        "\n",
        "    ext_idx = seq_idx + batch_idx * time_size\n",
        "    neg_ext = neg_ext[ext_idx].view(batch_size, self.n_negative,\n",
        "                                    window_size, dim_encoded)\n",
        "    label_loss = torch.zeros((batch_size, window_size),\n",
        "                              dtype=torch.long,\n",
        "                              device=encoded_data.device)\n",
        "\n",
        "    for k in range(1, self.K_ + 1):\n",
        "\n",
        "      # Positive samples\n",
        "      if k < self.K_:\n",
        "          pos_seq = encoded_data[:, k:-(self.K_-k)]\n",
        "      else:\n",
        "          pos_seq = encoded_data[:, k:]\n",
        "\n",
        "      pos_seq = pos_seq.view(batch_size, 1, pos_seq.size(1), dim_encoded)\n",
        "      full_seq = torch.cat((pos_seq, neg_ext), dim=1)\n",
        "      outputs.append(full_seq)\n",
        "\n",
        "    return outputs, label_loss\n",
        "\n",
        "  def forward(self, encoded_data, context_data):\n",
        "\n",
        "    # TO COMPLETE:\n",
        "    # Perform the full cpc criterion\n",
        "    # Returns 2 values:\n",
        "    # - the average classification loss avg_loss\n",
        "    # - the average classification acuracy avg_acc\n",
        "\n",
        "    # Reminder : The permuation !\n",
        "    encoded_data = encoded_data.permute(0, 2, 1)\n",
        "\n",
        "    # First we need to sample the negative examples\n",
        "    negative_samples, labels = self.sample_negatives(encoded_data)\n",
        "\n",
        "    # Then we must compute phi_k\n",
        "    phi_k = self.get_prediction_k(context_data)\n",
        "\n",
        "    # Finally we must get the dot product between phi_k and negative_samples \n",
        "    # for each k\n",
        "\n",
        "    #The total loss is the average of all losses\n",
        "    avg_loss = 0\n",
        "\n",
        "    # Average acuracy\n",
        "    avg_acc = 0\n",
        "\n",
        "    for k in range(self.K_):\n",
        "      B, N_sampled, S_small, H = negative_samples[k].size() \n",
        "      B, S, H = phi_k[k].size()\n",
        "\n",
        "      # As told before S = S_small + K. For segments too far in the sequence\n",
        "      # there are no positive exmples anyway, so we must shorten phi_k\n",
        "      phi = phi_k[k][:, :S_small]\n",
        "\n",
        "      # Now the dot product\n",
        "      # You have several ways to do that, let's do the simple but non optimal \n",
        "      # one\n",
        "      # pytorch has a matrix product function https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
        "      # But it takes only 3D tensors of the same batch size !\n",
        "      # To begin negative_samples is a 4D tensor ! \n",
        "      # We want to compute the dot product for each features, of each sequence\n",
        "      # of the batch. Thus we are trying to compute a dot product for all\n",
        "      # B* N_sampled * S_small 1D vector of negative_samples[k]\n",
        "      # Or, a 1D tensor of size H is also a matrix of size 1 x H\n",
        "      # Then, we must view it as a 3D tensor of size (B* N_sampled * S_small, 1, H)\n",
        "      negative_sample_k  =  negative_samples[k].view(B* N_sampled* S_small, 1, H)\n",
        "\n",
        "      # But now phi and negative_sample_k no longer have the same batch size !\n",
        "      # No worries, we can expand phi so that each sequence of the batch\n",
        "      # is repeated N_sampled times\n",
        "      phi = phi.view(B, 1,S_small, H).expand(B, N_sampled, S_small, H)\n",
        "\n",
        "      # And now we can view it as a 3D tensor \n",
        "      phi  = phi.contiguous().view(B * N_sampled * S_small, H, 1)\n",
        "\n",
        "      # We can finally get the dot product !\n",
        "      scores = torch.bmm(negative_sample_k, phi)\n",
        "\n",
        "      # Dot_product has a size (B * N_sampled * S_small , 1, 1)\n",
        "      # Let's reorder it a bit\n",
        "      scores = scores.reshape(B, N_sampled, S_small)\n",
        "\n",
        "      # For each elements of the sequence, and each elements sampled, it gives \n",
        "      # a floating score stating the likelihood of this element being the \n",
        "      # true one.\n",
        "      # Now the classification loss, we need to use the Cross Entropy loss\n",
        "      # https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html\n",
        "\n",
        "      # For each time-step of each sequence of the batch \n",
        "      # we have N_sampled possible predictions. \n",
        "      # Looking at the documentation of torch.nn.CrossEntropyLoss\n",
        "      # we can see that this loss expect a tensor of size M x C where \n",
        "      # - M is the number of elements with a classification score\n",
        "      # - C is the number of possible classes\n",
        "      # There are N_sampled candidates for each predictions so\n",
        "      # C = N_sampled \n",
        "      # Each timestep of each sequence of the batch has a prediction so\n",
        "      # M = B * S_small\n",
        "      # Thus we need an input vector of size B * S_small, N_sampled\n",
        "      # To begin, we need to permute the axis\n",
        "      scores = scores.permute(0, 2, 1) # Now it has size B , S_small, N_sampled\n",
        "\n",
        "      # Then we can cast it into a 2D tensor\n",
        "      scores = scores.reshape(B * S_small, N_sampled)\n",
        "\n",
        "      # Same thing for the labels \n",
        "      labels = labels.reshape(B * S_small)\n",
        "\n",
        "      # Finally we can get the classification loss\n",
        "      loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "      loss_k = loss_criterion(scores, labels)\n",
        "      avg_loss+= loss_k\n",
        "\n",
        "      # And for the acuracy\n",
        "      # The prediction for each elements is the sample with the highest score\n",
        "      # Thus the tensors of all predictions is the tensors of the index of the \n",
        "      # maximal score for each time-step of each sequence of the batch\n",
        "      predictions = torch.argmax(scores, 1)\n",
        "      acc_k  = (labels == predictions).sum() / (B * S_small)\n",
        "      avg_acc += acc_k\n",
        "\n",
        "    # Normalization\n",
        "    avg_loss = avg_loss / self.K_\n",
        "    avg_acc = avg_acc / self.K_\n",
        "      \n",
        "    return avg_loss , avg_acc"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0cqGXhLf-_O1"
      },
      "source": [
        "Don't forget to test !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sYJSMh5I_QCf",
        "colab": {}
      },
      "source": [
        "audio = torchaudio.load(\"/content/drive/My Drive/Speec Recognition/DATA/train/200624-151721_swa_5a1_elicit/200624-151721_swa_5a1_elicit_0.wav\")[0]\n",
        "audio = audio.view(1, 1, -1)\n",
        "cpc_criterion = CPCCriterion(N_PREDICTIONS, DIM_CONTEXT, \n",
        "                             DIM_ENCODER, N_NEGATIVE_SAMPLE).to(device)\n",
        "context_output, encoder_output = cpc_model(audio.to(device))\n",
        "loss, avg = cpc_criterion(encoder_output,context_output)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zLv7GoE0_4C3"
      },
      "source": [
        "## Exercise 3: Full training loop !\n",
        "\n",
        "You have the model, you have the criterion. All you need now are a data loader and an optimizer to run your training loop.\n",
        "\n",
        "We will use an Adam optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zcg29tqPAIR1",
        "colab": {}
      },
      "source": [
        "parameters = list(cpc_criterion.parameters()) + list(cpc_model.parameters())\n",
        "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hETDwSfTAuF4"
      },
      "source": [
        "And as far as the data loader is concerned, we will rely on the data loader provided by the CPC_audio library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O6KES4RXA0tU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "348088c7-0de5-41ff-d1c6-2220bbc5df1a"
      },
      "source": [
        "dataset_train = load_dataset('/content/drive/My Drive/Speec Recognition/DATA/train')\n",
        "dataset_val = load_dataset('/content/drive/My Drive/Speec Recognition/DATA/validation')\n",
        "#dataset_test = load_dataset('/content/drive/My Drive/Speec Recognition/DATA/test')\n",
        "data_loader_train = dataset_train.getDataLoader(BATCH_SIZE, \"speaker\", True)\n",
        "data_loader_val = dataset_train.getDataLoader(BATCH_SIZE, \"sequence\", False)\n",
        "#data_loader_test = dataset_train.getDataLoader(BATCH_SIZE, 'sequence', False)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6it [00:00, 373.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved cache file at /content/drive/My Drive/Speec Recognition/DATA/train/_seqs_cache.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Checking length...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "340it [00:00, 347735.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done, elapsed: 0.391 seconds\n",
            "Scanned 340 sequences in 0.39 seconds\n",
            "1 chunks computed\n",
            "Joining pool\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Joined process, elapsed=1.234 secs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4it [00:00, 37.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved cache file at /content/drive/My Drive/Speec Recognition/DATA/validation/_seqs_cache.txt\n",
            "Checking length...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "204it [00:00, 210489.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done, elapsed: 0.278 seconds\n",
            "Scanned 204 sequences in 0.28 seconds\n",
            "1 chunks computed\n",
            "Joining pool\n",
            "Joined process, elapsed=0.768 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uCoMCPL0A8VI"
      },
      "source": [
        "Now that everything is ready, complete and test the ```train_step``` function below which trains the model for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U-hIH3p8BsZr",
        "colab": {}
      },
      "source": [
        "def train_step(data_loader,\n",
        "               cpc_model,\n",
        "               cpc_criterion,\n",
        "               optimizer):\n",
        "  \n",
        "  avg_loss = 0\n",
        "  avg_acc = 0\n",
        "  n_items = 0\n",
        "\n",
        "  for step, data in enumerate(data_loader):\n",
        "    x,y = data\n",
        "    bs = len(x)\n",
        "    optimizer.zero_grad()\n",
        "    context_output, encoder_output = cpc_model(x.to(device))\n",
        "    loss , acc = cpc_criterion(encoder_output, context_output)\n",
        "    loss.backward()\n",
        "    n_items+=bs\n",
        "    avg_loss+=loss.item()*bs\n",
        "    avg_acc +=acc.item()*bs\n",
        "  \n",
        "  avg_loss/=n_items\n",
        "  avg_acc/=n_items\n",
        "  return avg_loss, avg_acc"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rIj06giJE_bZ"
      },
      "source": [
        "## Exercise 4 : Validation loop\n",
        "\n",
        "Now complete the validation loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q7Qwi6jDByyt",
        "colab": {}
      },
      "source": [
        "def validation_step(data_loader,\n",
        "                    cpc_model,\n",
        "                    cpc_criterion):\n",
        "  \n",
        "  avg_loss = 0\n",
        "  avg_acc = 0\n",
        "  n_items = 0\n",
        "\n",
        "  for step, data in enumerate(data_loader):\n",
        "    x,y = data\n",
        "    bs = len(x)\n",
        "    context_output, encoder_output = cpc_model(x.to(device))\n",
        "    loss , acc = cpc_criterion(encoder_output, context_output)\n",
        "    n_items+=bs\n",
        "    avg_loss+=loss.item()*bs\n",
        "    avg_acc+=acc.item()*bs\n",
        "  \n",
        "  avg_loss/=n_items\n",
        "  avg_acc/=n_items\n",
        "  return avg_loss, avg_acc"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OBVUPKKs2_0U"
      },
      "source": [
        "## Exercise 5: Run everything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZbXsZIRiB1tm",
        "colab": {}
      },
      "source": [
        "def run(train_loader,\n",
        "        val_loader,\n",
        "        #test_loader,\n",
        "        cpc_model,\n",
        "        cpc_criterion,\n",
        "        optimizer,\n",
        "        n_epochs):\n",
        "  \n",
        "  for epoch in range(n_epochs):\n",
        "\n",
        "    \n",
        "    print(f\"Running epoch {epoch+1} / {n_epochs}\")\n",
        "    avg_loss_train, avg_acc_train = train_step(train_loader, cpc_model, cpc_criterion, optimizer)\n",
        "    print(\"----------------------\")\n",
        "    print(f\"Training dataset\")\n",
        "    print(f\"- average loss : {avg_loss_train}\")\n",
        "    print(f\"- average acuracy : {avg_acc_train}\")\n",
        "    print(\"----------------------\")\n",
        "    with torch.no_grad():\n",
        "      cpc_model.eval()\n",
        "      cpc_criterion.eval()\n",
        "      avg_loss_val, avg_acc_val = validation_step(val_loader, cpc_model, cpc_criterion)\n",
        "      print(f\"Validation dataset\")\n",
        "      print(f\"- average loss : {avg_loss_val}\")\n",
        "      print(f\"- average acuracy : {avg_acc_val}\")\n",
        "      print(\"----------------------\")\n",
        "      print()\n",
        "      cpc_model.train()\n",
        "      cpc_criterion.train()\n",
        "\n",
        "    "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5xx8vN2wpECC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "99236769-5623-4cf5-cc96-0b7fbef424cc"
      },
      "source": [
        "run(data_loader_train, data_loader_val, cpc_model,cpc_criterion,optimizer,1)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running epoch 1 / 1\n",
            "----------------------\n",
            "Training dataset\n",
            "- average loss : 4.891607015297331\n",
            "- average acuracy : 0.0\n",
            "----------------------\n",
            "Validation dataset\n",
            "- average loss : 4.89142224921689\n",
            "- average acuracy : 0.0\n",
            "----------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5hoT3_3W6HYY"
      },
      "source": [
        "Once everything is donw, clear the memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fU5mDOY46KSG",
        "colab": {}
      },
      "source": [
        "del dataset_train\n",
        "del dataset_val\n",
        "del cpc_model\n",
        "del context\n",
        "del encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "srPM5r_LB9v-"
      },
      "source": [
        "# Part 2 : Fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Nb_-0IQiJk9"
      },
      "source": [
        "## Exercice 1 : Phone separability with aligned phonemes.\n",
        "\n",
        "One option to evaluate the quality of the features trained with CPC can be to check if they can be used to recognize phonemes. \n",
        "To do so, we can fine-tune a pre-trained model using a limited amount of labelled speech data.\n",
        "We are going to start with a simple evaluation setting where we have the phone labels for each timestep corresponding to a CPC feature.\n",
        "\n",
        "We will work with a model already pre-trained on English data. As far as the fine-tuning dataset is concerned, we will use a 1h subset of [librispeech-100](http://www.openslr.org/12/). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N-scDMAasXxc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "8bf8aadd-8368-4018-c260-a832f5e2da85"
      },
      "source": [
        "!mkdir checkpoint_data\n",
        "!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_30.pt -P checkpoint_data\n",
        "!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_logs.json -P checkpoint_data\n",
        "!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_args.json -P checkpoint_data\n",
        "!ls checkpoint_data"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-04 15:22:18--  https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_30.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 113599715 (108M) [application/octet-stream]\n",
            "Saving to: ‘checkpoint_data/checkpoint_30.pt’\n",
            "\n",
            "checkpoint_30.pt    100%[===================>] 108.34M  93.1MB/s    in 1.2s    \n",
            "\n",
            "2020-07-04 15:22:19 (93.1 MB/s) - ‘checkpoint_data/checkpoint_30.pt’ saved [113599715/113599715]\n",
            "\n",
            "--2020-07-04 15:22:21--  https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_logs.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20786 (20K) [text/plain]\n",
            "Saving to: ‘checkpoint_data/checkpoint_logs.json’\n",
            "\n",
            "checkpoint_logs.jso 100%[===================>]  20.30K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2020-07-04 15:22:21 (4.05 MB/s) - ‘checkpoint_data/checkpoint_logs.json’ saved [20786/20786]\n",
            "\n",
            "--2020-07-04 15:22:23--  https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_args.json\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2063 (2.0K) [text/plain]\n",
            "Saving to: ‘checkpoint_data/checkpoint_args.json’\n",
            "\n",
            "checkpoint_args.jso 100%[===================>]   2.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-07-04 15:22:23 (10.4 MB/s) - ‘checkpoint_data/checkpoint_args.json’ saved [2063/2063]\n",
            "\n",
            "checkpoint_30.pt  checkpoint_args.json\tcheckpoint_logs.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SSSaiYo82_oY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "277949f5-bb45-4198-bdf6-25affe8908f0"
      },
      "source": [
        "%cd /content/CPC_audio\n",
        "from cpc.dataset import parseSeqLabels\n",
        "from cpc.feature_loader import loadModel\n",
        "\n",
        "checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n",
        "cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n",
        "cpc_model = cpc_model.cuda()\n",
        "label_dict, N_PHONES = parseSeqLabels('/content/drive/My Drive/Speec Recognition/DATA/all_sessions.txt')\n",
        "dataset_train = load_dataset('/content/drive/My Drive/Speec Recognition/DATA/train', file_extension='.wav', phone_label_dict=label_dict)\n",
        "dataset_val = load_dataset('/content/drive/My Drive/Speec Recognition/DATA/validation', file_extension='.wav', phone_label_dict=label_dict)\n",
        "\n",
        "data_loader_train = dataset_train.getDataLoader(BATCH_SIZE, \"speaker\", True)\n",
        "data_loader_val = dataset_val.getDataLoader(BATCH_SIZE, \"sequence\", False)\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CPC_audio\n",
            "Loading checkpoint checkpoint_data/checkpoint_30.pt\n",
            "Loading the state dict at checkpoint_data/checkpoint_30.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6it [00:00, 407.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved cache file at /content/drive/My Drive/Speec Recognition/DATA/train/_seqs_cache.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Checking length...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "340it [00:00, 71128.90it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done, elapsed: 0.432 seconds\n",
            "Scanned 340 sequences in 0.43 seconds\n",
            "1 chunks computed\n",
            "Joining pool\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4it [00:00, 49.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Joined process, elapsed=1.207 secs\n",
            "Saved cache file at /content/drive/My Drive/Speec Recognition/DATA/validation/_seqs_cache.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Checking length...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "204it [00:00, 45575.69it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done, elapsed: 0.279 seconds\n",
            "Scanned 204 sequences in 0.28 seconds\n",
            "1 chunks computed\n",
            "Joining pool\n",
            "Joined process, elapsed=0.759 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0TvT8UXbNFG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "0430c000-660c-4152-8e86-f6ef2716051f"
      },
      "source": [
        "# Loading the test dataset\n",
        "dataset_test = load_dataset('/content/drive/My Drive/Speec Recognition/DATA/test', file_extension='.wav', phone_label_dict=label_dict)\n",
        "data_loader_test = dataset_test.getDataLoader(BATCH_SIZE, \"sequence\", False)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11it [00:03,  3.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved cache file at /content/drive/My Drive/Speec Recognition/DATA/test/_seqs_cache.txt\n",
            "Checking length...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "619it [00:00, 863410.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done, elapsed: 22.352 seconds\n",
            "Scanned 619 sequences in 22.35 seconds\n",
            "1 chunks computed\n",
            "Joining pool\n",
            "Joined process, elapsed=2.159 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xkKi-qfosng2"
      },
      "source": [
        "Then we will use a simple linear classifier to recognize the phonemes from the features produced by ```cpc_model```. \n",
        "\n",
        "### a) Build the phone classifier \n",
        "\n",
        "Design a class of linear classifiers, ```PhoneClassifier``` that will take as input a batch of sequences of CPC features and output a score vector for each phoneme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4RpAbz-0CXJJ",
        "colab": {}
      },
      "source": [
        "class PhoneClassifier(torch.nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               input_dim : int,\n",
        "               n_phones : int):\n",
        "    super(PhoneClassifier, self).__init__()\n",
        "    self.linear = torch.nn.Linear(input_dim, n_phones)\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zt5oa_nqtH-d"
      },
      "source": [
        "Our phone classifier will then be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NRBf_83IuLv5",
        "colab": {}
      },
      "source": [
        "phone_classifier = PhoneClassifier(HIDDEN_CONTEXT_MODEL, N_PHONES).to(device)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEUSG20auU0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9b77168b-60a0-452b-ce49-3af603443055"
      },
      "source": [
        "# Checking the length of the alphabet in Swahili language\n",
        "N_PHONES"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z_Vf5AbUhqm4"
      },
      "source": [
        "### b - What would be the correct loss criterion for this task ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uhyPM-cgjrtw",
        "colab": {}
      },
      "source": [
        "loss_criterion = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nv4cSxbaplrz"
      },
      "source": [
        "To perform the fine-tuning, we will also need an optimization function.\n",
        "\n",
        "We will use an [Adam optimizer ](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W5CgYyAlqKxu",
        "colab": {}
      },
      "source": [
        "parameters = list(phone_classifier.parameters()) + list(cpc_model.parameters())\n",
        "LEARNING_RATE = 2e-4\n",
        "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qQB9HS9PvAXc"
      },
      "source": [
        "You might also want to perform this training while freezing the weights of the ```cpc_model```. Indeed, if the pre-training was good enough, then ```cpc_model``` phonemes representation should be linearly separable. In this case the optimizer should be defined like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nRy0gn6awGUQ",
        "colab": {}
      },
      "source": [
        "optimizer_frozen = torch.optim.Adam(list(phone_classifier.parameters()), lr=LEARNING_RATE)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cO93ngIfj4JW"
      },
      "source": [
        "### c- Now let's build a training loop. \n",
        "Complete the function ```train_one_epoch``` below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fabqj3wvLwgU",
        "colab": {}
      },
      "source": [
        "def train_one_epoch(cpc_model, \n",
        "                    phone_classifier, \n",
        "                    loss_criterion, \n",
        "                    data_loader, \n",
        "                    optimizer):\n",
        "\n",
        "  cpc_model.train()\n",
        "  loss_criterion.train()\n",
        "\n",
        "  avg_loss = 0\n",
        "  avg_accuracy = 0\n",
        "  n_items = 0\n",
        "  for step, full_data in enumerate(data_loader):\n",
        "    # Each batch is represented by a Tuple of vectors:\n",
        "    # sequence of size : N x 1 x T\n",
        "    # label of size : N x T\n",
        "    # \n",
        "    # With :\n",
        "    # - N number of sequence in the batch\n",
        "    # - T size of each sequence\n",
        "    sequence, label = full_data\n",
        "    \n",
        "    \n",
        "\n",
        "    bs = len(sequence)\n",
        "    seq_len = label.size(1)\n",
        "    optimizer.zero_grad()\n",
        "    context_out, enc_out, _ = cpc_model(sequence.to(device),label.to(device))\n",
        "\n",
        "    scores = phone_classifier(context_out)\n",
        "\n",
        "    scores = scores.permute(0,2,1)\n",
        "    loss = loss_criterion(scores,label.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    avg_loss+=loss.item()*bs\n",
        "    n_items+=bs\n",
        "    correct_labels = scores.argmax(1)\n",
        "    avg_accuracy += ((label==correct_labels.cpu()).float()).mean(1).sum().item()\n",
        "  avg_loss/=n_items\n",
        "  avg_accuracy/=n_items\n",
        "  return avg_loss, avg_accuracy\n",
        "    "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "quYtjx_TxIPK"
      },
      "source": [
        "Don't forget to test it !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "50MwxbKhxMKp",
        "colab": {}
      },
      "source": [
        "avg_loss, avg_accuracy = train_one_epoch(cpc_model, phone_classifier, loss_criterion, data_loader_train, optimizer_frozen)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_o6yk8XKWnYe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c3b66da-df55-45c7-d05b-b0e8f887cce2"
      },
      "source": [
        "avg_loss, avg_accuracy"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3.177700082461039, 0.040568033854166664)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EmUkuJ2bwu4Z"
      },
      "source": [
        "### d- Build the validation loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kZJMxj6cwzd3",
        "colab": {}
      },
      "source": [
        "def validation_step(cpc_model, \n",
        "                    phone_classifier, \n",
        "                    loss_criterion, \n",
        "                    data_loader):\n",
        "  \n",
        "  cpc_model.eval()\n",
        "  phone_classifier.eval()\n",
        "\n",
        "  avg_loss = 0\n",
        "  avg_accuracy = 0\n",
        "  n_items = 0\n",
        "  with torch.no_grad():\n",
        "    for step, full_data in enumerate(data_loader):\n",
        "      # Each batch is represented by a Tuple of vectors:\n",
        "      # sequence of size : N x 1 x T\n",
        "      # label of size : N x T\n",
        "      # \n",
        "      # With :\n",
        "      # - N number of sequence in the batch\n",
        "      # - T size of each sequence\n",
        "      sequence, label = full_data\n",
        "      bs = len(sequence)\n",
        "      seq_len = label.size(1)\n",
        "      context_out, enc_out, _ = cpc_model(sequence.to(device),label.to(device))\n",
        "      scores = phone_classifier(context_out)\n",
        "      scores = scores.permute(0,2,1)\n",
        "      loss = loss_criterion(scores,label.to(device))\n",
        "      avg_loss+=loss.item()*bs\n",
        "      n_items+=bs\n",
        "      correct_labels = scores.argmax(1)\n",
        "      avg_accuracy += ((label==correct_labels.cpu()).float()).mean(1).sum().item()\n",
        "  avg_loss/=n_items\n",
        "  avg_accuracy/=n_items\n",
        "  return avg_loss, avg_accuracy"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vownVCt7xbVh"
      },
      "source": [
        "### e- Run everything\n",
        "\n",
        "Test this functiion with both ```optimizer``` and ```optimizer_frozen```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xvO_4nKUxfQx",
        "colab": {}
      },
      "source": [
        "def run(cpc_model, \n",
        "        phone_classifier, \n",
        "        loss_criterion, \n",
        "        data_loader_train, \n",
        "        data_loader_val,\n",
        "        #data_loader_test, \n",
        "        optimizer,\n",
        "        n_epoch):\n",
        "\n",
        "  for epoch in range(n_epoch):\n",
        "\n",
        "    print(f\"Running epoch {epoch + 1} / {n_epoch}\")\n",
        "    loss_train, acc_train = train_one_epoch(cpc_model, phone_classifier, loss_criterion, data_loader_train, optimizer)\n",
        "    print(\"-------------------\")\n",
        "    print(f\"Training dataset :\")\n",
        "    print(f\"Average loss : {loss_train}. Average accuracy {acc_train}\")\n",
        "\n",
        "    print(\"-------------------\")\n",
        "    print(\"Validation dataset\")\n",
        "    loss_val, acc_val = validation_step(cpc_model, phone_classifier, loss_criterion, data_loader_val)\n",
        "    print(f\"Average loss : {loss_val}. Average accuracy {acc_val}\")\n",
        "    print(\"-------------------\")\n",
        "    print()\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ceCEO2h2bxAn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5494211a-e1cf-43a5-e1a4-f44dca7ff1c7"
      },
      "source": [
        "run(cpc_model,phone_classifier,loss_criterion,data_loader_train,data_loader_val,optimizer_frozen,n_epoch=10)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running epoch 1 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 3.093122124671936. Average accuracy 0.11279296875\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 3.0517117579778037. Average accuracy 0.16267903645833334\n",
            "-------------------\n",
            "\n",
            "Running epoch 2 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 3.027117043733597. Average accuracy 0.16849772135416666\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 2.992135922114054. Average accuracy 0.19588216145833334\n",
            "-------------------\n",
            "\n",
            "Running epoch 3 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 2.9742109179496765. Average accuracy 0.183837890625\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 2.944512188434601. Average accuracy 0.2060546875\n",
            "-------------------\n",
            "\n",
            "Running epoch 4 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 2.9318833152453103. Average accuracy 0.19588216145833334\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 2.907429893811544. Average accuracy 0.21134440104166666\n",
            "-------------------\n",
            "\n",
            "Running epoch 5 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 2.8956220348676047. Average accuracy 0.20308430989583334\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 2.8768456975618997. Average accuracy 0.21614583333333334\n",
            "-------------------\n",
            "\n",
            "Running epoch 6 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 2.867035190264384. Average accuracy 0.2086181640625\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 2.852810541788737. Average accuracy 0.21712239583333334\n",
            "-------------------\n",
            "\n",
            "Running epoch 7 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 2.8436205983161926. Average accuracy 0.21390787760416666\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 2.8341439962387085. Average accuracy 0.21915690104166666\n",
            "-------------------\n",
            "\n",
            "Running epoch 8 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 2.8261441787083945. Average accuracy 0.2183837890625\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 2.8210066159566245. Average accuracy 0.22062174479166666\n",
            "-------------------\n",
            "\n",
            "Running epoch 9 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 2.809636414051056. Average accuracy 0.22342936197916666\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 2.8053833842277527. Average accuracy 0.2236328125\n",
            "-------------------\n",
            "\n",
            "Running epoch 10 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 2.7979572316010795. Average accuracy 0.224365234375\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 2.799255649248759. Average accuracy 0.2255859375\n",
            "-------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TdfWDiFnylMT"
      },
      "source": [
        "## Exercise 2 : Phone separability without alignment (PER)\n",
        "\n",
        "Aligned data are very practical, but un real life they are rarely available. That's why in this excercise we will consider a fine-tuning with non-aligned phonemes.\n",
        "\n",
        "The model, the optimizer and the phone classifier will stay the same. However, we will replace our phone criterion with a [CTC loss](https://pytorch.org/docs/master/generated/torch.nn.CTCLoss.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_9BpM_Lpzgx8",
        "colab": {}
      },
      "source": [
        "loss_ctc = torch.nn.CTCLoss()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AQpYgTyfzsrq"
      },
      "source": [
        "Besides, we will use a siglthy different dataset class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9HRxoatlz3ZZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "b392080b-881e-4d5c-8608-b76888ec6f51"
      },
      "source": [
        "%cd /content/CPC_audio\n",
        "from cpc.eval.common_voices_eval import SingleSequenceDataset, parseSeqLabels, findAllSeqs\n",
        "path_train_data_per = '/content/drive/My Drive/Speec Recognition/DATA/train'\n",
        "path_val_data_per = '/content/drive/My Drive/Speec Recognition/DATA/validation'\n",
        "path_test_data_per = '/content/drive/My Drive/Speec Recognition/DATA/test'\n",
        "path_phone_data_per = '/content/drive/My Drive/Speec Recognition/DATA/all_sessions.txt'\n",
        "BATCH_SIZE=8\n",
        " \n",
        "phone_labels, N_PHONES = parseSeqLabels(path_phone_data_per)\n",
        "data_train_per, _ = findAllSeqs(path_train_data_per, extension='.wav')\n",
        "dataset_train_non_aligned = SingleSequenceDataset(path_train_data_per, data_train_per, phone_labels)\n",
        "data_loader_train = torch.utils.data.DataLoader(dataset_train_non_aligned, batch_size=BATCH_SIZE,\n",
        "                                                shuffle=True)\n",
        "\n",
        "data_val_per, _ = findAllSeqs(path_val_data_per, extension='.wav')\n",
        "dataset_val_non_aligned = SingleSequenceDataset(path_val_data_per, data_val_per, phone_labels)\n",
        "data_loader_val = torch.utils.data.DataLoader(dataset_val_non_aligned, batch_size=BATCH_SIZE,\n",
        "                                              shuffle=True)\n",
        "\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6it [00:00, 338.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/CPC_audio\n",
            "Saved cache file at /content/drive/My Drive/Speec Recognition/DATA/train/_seqs_cache.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "4it [00:00, 217.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 341 sequences in 6.87 seconds\n",
            "maxSizeSeq : 349569\n",
            "maxSizePhone : 196\n",
            "minSizePhone : 17\n",
            "Total size dataset 0.6630421875 hours\n",
            "Saved cache file at /content/drive/My Drive/Speec Recognition/DATA/validation/_seqs_cache.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 205 sequences in 2.50 seconds\n",
            "maxSizeSeq : 155727\n",
            "maxSizePhone : 107\n",
            "minSizePhone : 31\n",
            "Total size dataset 0.34706203125 hours\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GwAckY62z7s9"
      },
      "source": [
        "### a- Training\n",
        "\n",
        "Since the phonemes are not aligned, there is no simple direct way to get the classification acuracy of a model. Write and test the three functions ```train_one_epoch_ctc```, ```validation_step_ctc``` and ```run_ctc``` as before but without considering the average acuracy of the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oYg5YzW8EHl4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "91d724d3-32ee-4d06-9c12-b846132da096"
      },
      "source": [
        "from cpc.feature_loader import loadModel\n",
        "\n",
        "checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n",
        "cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n",
        "cpc_model = cpc_model.cuda()\n",
        "phone_classifier = PhoneClassifier(HIDDEN_CONTEXT_MODEL, N_PHONES).to(device)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint_data/checkpoint_30.pt\n",
            "Loading the state dict at checkpoint_data/checkpoint_30.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CFQ2g3PjErdZ",
        "colab": {}
      },
      "source": [
        "parameters = list(phone_classifier.parameters()) + list(cpc_model.parameters())\n",
        "LEARNING_RATE = 2e-4\n",
        "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n",
        "\n",
        "optimizer_frozen = torch.optim.Adam(list(phone_classifier.parameters()), lr=LEARNING_RATE)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zsgjv3cD0oqD",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_one_epoch_ctc(cpc_model, \n",
        "                        phone_classifier, \n",
        "                        loss_criterion, \n",
        "                        data_loader, \n",
        "                        optimizer):\n",
        "  \n",
        "  cpc_model.train()\n",
        "  loss_criterion.train()\n",
        "\n",
        "  avg_loss = 0\n",
        "  avg_accuracy = 0\n",
        "  n_items = 0\n",
        "  for step, full_data in enumerate(data_loader):\n",
        "\n",
        "    x, x_len, y, y_len = full_data\n",
        "\n",
        "    x_batch_len = x.shape[-1]\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    bs=x.size(0)\n",
        "    optimizer.zero_grad()\n",
        "    context_out, enc_out, _ = cpc_model(x.to(device),y.to(device))\n",
        "  \n",
        "    scores = phone_classifier(context_out)\n",
        "    scores = scores.permute(1,0,2)\n",
        "    scores = F.log_softmax(scores,2)\n",
        "    yhat_len = torch.tensor([int(scores.shape[0]*x_len[i]/x_batch_len) for i in range(scores.shape[1])]) # this is an approximation, should be good enough\n",
        "\n",
        "    loss = loss_criterion(scores,y.to(device),yhat_len,y_len)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    avg_loss+=loss.item()*bs\n",
        "    n_items+=bs\n",
        "  avg_loss/=n_items\n",
        "  return avg_loss\n",
        "\n",
        "def validation_step(cpc_model, \n",
        "                    phone_classifier, \n",
        "                    loss_criterion, \n",
        "                    data_loader):\n",
        "\n",
        "  cpc_model.eval()\n",
        "  phone_classifier.eval()\n",
        "  avg_loss = 0\n",
        "  avg_accuracy = 0\n",
        "  n_items = 0\n",
        "  with torch.no_grad():\n",
        "    for step, full_data in enumerate(data_loader):\n",
        "\n",
        "      x, x_len, y, y_len = full_data\n",
        "\n",
        "      x_batch_len = x.shape[-1]\n",
        "      x, y = x.to(device), y.to(device)\n",
        "\n",
        "      bs=x.size(0)\n",
        "      context_out, enc_out, _ = cpc_model(x.to(device),y.to(device))\n",
        "    \n",
        "      scores = phone_classifier(context_out)\n",
        "      scores = scores.permute(1,0,2)\n",
        "      scores = F.log_softmax(scores,2)\n",
        "      yhat_len = torch.tensor([int(scores.shape[0]*x_len[i]/x_batch_len) for i in range(scores.shape[1])]) # this is an approximation, should be good enough\n",
        "\n",
        "      loss = loss_criterion(scores,y.to(device),yhat_len,y_len)\n",
        "      avg_loss+=loss.item()*bs\n",
        "      n_items+=bs\n",
        "  avg_loss/=n_items\n",
        "\n",
        "  return avg_loss\n",
        "\n",
        "\n",
        "def run_ctc(cpc_model, \n",
        "            phone_classifier, \n",
        "            loss_criterion, \n",
        "            data_loader_train, \n",
        "            data_loader_val,\n",
        "            #data_loader_test, \n",
        "            optimizer,\n",
        "            n_epoch):\n",
        "  for epoch in range(n_epoch):\n",
        "\n",
        "    print(f\"Running epoch {epoch + 1} / {n_epoch}\")\n",
        "    loss_train = train_one_epoch_ctc(cpc_model, phone_classifier, loss_criterion, data_loader_train, optimizer)\n",
        "    print(\"-------------------\")\n",
        "    print(f\"Training dataset :\")\n",
        "    print(f\"Average loss : {loss_train}.\")\n",
        "\n",
        "    print(\"-------------------\")\n",
        "    print(\"Validation dataset\")\n",
        "    loss_val = validation_step(cpc_model, phone_classifier, loss_criterion, data_loader_val)\n",
        "    print(f\"Average loss : {loss_val}\")\n",
        "    print(\"-------------------\")\n",
        "    print()\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GSr7tcUdD72c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "11963c1e-850e-4279-9e74-68bde9668592"
      },
      "source": [
        "run_ctc(cpc_model,phone_classifier,loss_ctc,data_loader_train,data_loader_val,optimizer_frozen,n_epoch=10)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running epoch 1 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 25.51865036908318.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 23.815385706284466\n",
            "-------------------\n",
            "\n",
            "Running epoch 2 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 22.454673811968636.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 20.703503851797066\n",
            "-------------------\n",
            "\n",
            "Running epoch 3 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 19.009753866756665.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 17.458949743532667\n",
            "-------------------\n",
            "\n",
            "Running epoch 4 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 15.643460845947265.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 14.456631735259412\n",
            "-------------------\n",
            "\n",
            "Running epoch 5 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 12.665040072272806.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 11.857097046048034\n",
            "-------------------\n",
            "\n",
            "Running epoch 6 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 10.201874850778019.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 9.695339371176328\n",
            "-------------------\n",
            "\n",
            "Running epoch 7 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 8.230332127739402.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 8.003072832144943\n",
            "-------------------\n",
            "\n",
            "Running epoch 8 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 6.704844912360696.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 6.651257122264189\n",
            "-------------------\n",
            "\n",
            "Running epoch 9 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 5.529997438542983.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 5.601187238506243\n",
            "-------------------\n",
            "\n",
            "Running epoch 10 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 4.630809935401468.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 4.778547043893852\n",
            "-------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TKrYW4gK1BBF"
      },
      "source": [
        "### b- Evaluation: the Phone Error Rate (PER)\n",
        "\n",
        "In order to compute the similarity between two sequences, we can use the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance). This distance estimates the minimum number of insertion, deletion and addition to move from one sequence to another. If we normalize this distance by the number of characters in the reference sequence we get the Phone Error Rate (PER).\n",
        "\n",
        "This value can be interpreted as :\n",
        "\\\\[  PER = \\frac{S + D + I}{N} \\\\]\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "*   N is the number of characters in the reference\n",
        "*   S is the number of substitutiion\n",
        "*   I in the number of insertion\n",
        "*   D in the number of deletion\n",
        "\n",
        "For the best possible alignment of the two sequences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RoBhsx7GNqI_",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_PER_sequence(ref_seq, target_seq):\n",
        "\n",
        "  n = len(ref_seq)\n",
        "  m = len(target_seq)\n",
        "\n",
        "  D = np.zeros((n+1,m+1))\n",
        "  for i in range(1,n+1):\n",
        "    D[i,0] = D[i-1,0]+1\n",
        "  for j in range(1,m+1):\n",
        "    D[0,j] = D[0,j-1]+1\n",
        "  \n",
        "  ### TODO compute the alignment\n",
        "\n",
        "  for i in range(1,n+1):\n",
        "    for j in range(1,m+1):\n",
        "      D[i,j] = min(\n",
        "          D[i-1,j]+1,\n",
        "          D[i-1,j-1]+1,\n",
        "          D[i,j-1]+1,\n",
        "          D[i-1,j-1]+ 0 if ref_seq[i-1]==target_seq[j-1] else float(\"inf\")\n",
        "      )\n",
        "  return D[n,m]/len(ref_seq)\n",
        "  "
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r-hr0KK0mgcR"
      },
      "source": [
        "You can test your function below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AfTb3yOQmvey",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d8694528-7b71-490f-ef07-c371c7571384"
      },
      "source": [
        "ref_seq = [0, 1, 1, 2, 0, 2, 2]\n",
        "pred_seq = [1, 1, 2, 2, 0, 0]\n",
        "\n",
        "expected_PER = 4. / 7.\n",
        "print(get_PER_sequence(ref_seq, pred_seq) == expected_PER)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nHiyChl-m_k7"
      },
      "source": [
        "## c- Evaluating the PER of your model on the test dataset\n",
        "\n",
        "Evaluate the PER on the validation dataset. Please notice that you should usually use a separate dataset, called the dev dataset, to perform this operation. However for the sake of simplicity we will work with validation data in this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DMkX0PoFnclg",
        "colab": {}
      },
      "source": [
        "import progressbar\n",
        "from multiprocessing import Pool\n",
        "\n",
        "def cut_data(seq, sizeSeq):\n",
        "    maxSeq = sizeSeq.max()\n",
        "    return seq[:, :maxSeq]\n",
        "\n",
        "\n",
        "def prepare_data(data):\n",
        "    seq, sizeSeq, phone, sizePhone = data\n",
        "    seq = seq.cuda()\n",
        "    phone = phone.cuda()\n",
        "    sizeSeq = sizeSeq.cuda().view(-1)\n",
        "    sizePhone = sizePhone.cuda().view(-1)\n",
        "\n",
        "    seq = cut_data(seq.permute(0, 2, 1), sizeSeq).permute(0, 2, 1)\n",
        "    return seq, sizeSeq, phone, sizePhone\n",
        "\n",
        "\n",
        "def get_per(test_dataloader,\n",
        "            cpc_model,\n",
        "            phone_classifier):\n",
        "\n",
        "  downsampling_factor = 160\n",
        "  cpc_model.eval()\n",
        "  phone_classifier.eval()\n",
        "\n",
        "  avgPER = 0\n",
        "  nItems = 0 \n",
        "\n",
        "  print(\"Starting the PER computation through beam search\")\n",
        "  bar = progressbar.ProgressBar(maxval=len(test_dataloader))\n",
        "  bar.start()\n",
        "\n",
        "  for index, data in enumerate(test_dataloader):\n",
        "\n",
        "    bar.update(index)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      \n",
        "        seq, sizeSeq, phone, sizePhone = prepare_data(data)\n",
        "        c_feature, _, _ = cpc_model(seq.to(device),phone.to(device))\n",
        "        sizeSeq = sizeSeq / downsampling_factor\n",
        "        predictions = torch.nn.functional.softmax(\n",
        "        phone_classifier(c_feature), dim=2).cpu()\n",
        "        phone = phone.cpu()\n",
        "        sizeSeq = sizeSeq.cpu()\n",
        "        sizePhone = sizePhone.cpu()\n",
        "\n",
        "        bs = c_feature.size(0)\n",
        "        data_per = [(predictions[b].argmax(1),  phone[b]) for b in range(bs)]\n",
        "        # data_per = [(predictions[b], sizeSeq[b], phone[b], sizePhone[b],\n",
        "        #               \"criterion.module.BLANK_LABEL\") for b in range(bs)]\n",
        "\n",
        "        with Pool(bs) as p:\n",
        "            poolData = p.starmap(get_PER_sequence, data_per)\n",
        "        avgPER += sum([x for x in poolData])\n",
        "        nItems += len(poolData)\n",
        "\n",
        "  bar.finish()\n",
        "\n",
        "  avgPER /= nItems\n",
        "\n",
        "  print(f\"Average PER {avgPER}\")\n",
        "  return avgPER\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2hvnudh4Osb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "1b68d84d-26ef-44aa-ac62-869dd79de0a0"
      },
      "source": [
        "get_per(data_loader_val,cpc_model,phone_classifier)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\rN/A% (0 of 26) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting the PER computation through beam search\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (26 of 26) |########################| Elapsed Time: 0:04:07 Time:  0:04:07\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average PER 0.9749471555275155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9749471555275155"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p8e9D7g8159k"
      },
      "source": [
        "## Exercice 3 : Character error rate (CER) \n",
        "\n",
        "The Character Error Rate (CER) is an evaluation metric similar to the PER but with characters insterad of phonemes. Using the following data, run the functions you defined previously to estimate the CER of your model after fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cXONmKQOuFSn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "e174d770-71d9-4e36-fc7e-95024e48023c"
      },
      "source": [
        "# Load a dataset labelled with the letters of each sequence.\n",
        "%cd /content/CPC_audio\n",
        "from cpc.eval.common_voices_eval import SingleSequenceDataset, parseSeqLabels, findAllSeqs\n",
        "path_train_data_cer = '/content/drive/My Drive/Speec Recognition/DATA/train'\n",
        "path_val_data_cer = '/content/drive/My Drive/Speec Recognition/DATA/validation'\n",
        "path_letter_data_cer = '/content/drive/My Drive/Speec Recognition/DATA/all_sessions.txt'\n",
        "BATCH_SIZE=8\n",
        "\n",
        "letters_labels, N_LETTERS = parseSeqLabels(path_letter_data_cer)\n",
        "data_train_cer, _ = findAllSeqs(path_train_data_cer, extension='.wav')\n",
        "dataset_train_non_aligned = SingleSequenceDataset(path_train_data_cer, data_train_cer, letters_labels)\n",
        "\n",
        "\n",
        "data_val_cer, _ = findAllSeqs(path_val_data_cer, extension='.wav')\n",
        "dataset_val_non_aligned = SingleSequenceDataset(path_val_data_cer, data_val_cer, letters_labels)\n",
        "\n",
        "\n",
        "# The data loader will generate a tuple of tensors data, labels for each batch\n",
        "# data : size N x T1 x 1 : the audio sequence\n",
        "# label : size N x T2 the sequence of letters corresponding to the audio data\n",
        "# IMPORTANT NOTE: just like the PER the CER is computed with non-aligned phone data.\n",
        "data_loader_train_letters = torch.utils.data.DataLoader(dataset_train_non_aligned, batch_size=BATCH_SIZE,\n",
        "                                                shuffle=True)\n",
        "data_loader_val_letters = torch.utils.data.DataLoader(dataset_val_non_aligned, batch_size=BATCH_SIZE,\n",
        "                                              shuffle=True)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6it [00:00, 451.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/CPC_audio\n",
            "Saved cache file at /content/drive/My Drive/Speec Recognition/DATA/train/_seqs_cache.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "4it [00:00, 249.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 341 sequences in 3.34 seconds\n",
            "maxSizeSeq : 349569\n",
            "maxSizePhone : 196\n",
            "minSizePhone : 17\n",
            "Total size dataset 0.6630421875 hours\n",
            "Saved cache file at /content/drive/My Drive/Speec Recognition/DATA/validation/_seqs_cache.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded 205 sequences in 2.70 seconds\n",
            "maxSizeSeq : 155727\n",
            "maxSizePhone : 107\n",
            "minSizePhone : 31\n",
            "Total size dataset 0.34706203125 hours\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9h07zI2LjzAU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e34d5093-4b20-49a9-e5de-8944dd9385f6"
      },
      "source": [
        "from cpc.feature_loader import loadModel\n",
        "\n",
        "checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n",
        "cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n",
        "cpc_model = cpc_model.cuda()\n",
        "character_classifier = PhoneClassifier(HIDDEN_CONTEXT_MODEL, N_LETTERS).to(device)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint_data/checkpoint_30.pt\n",
            "Loading the state dict at checkpoint_data/checkpoint_30.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rHCNg1E7lW1L",
        "colab": {}
      },
      "source": [
        "parameters = list(character_classifier.parameters()) + list(cpc_model.parameters())\n",
        "LEARNING_RATE = 2e-4\n",
        "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n",
        "\n",
        "optimizer_frozen = torch.optim.Adam(list(character_classifier.parameters()), lr=LEARNING_RATE)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "engpkljbk9hj",
        "colab": {}
      },
      "source": [
        "loss_ctc = torch.nn.CTCLoss()"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9NBHd2s2kxld",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67c61b77-1267-4523-b4ec-264b1e69ebe3"
      },
      "source": [
        "run_ctc(cpc_model,character_classifier,loss_ctc,data_loader_train_letters,data_loader_val_letters,optimizer_frozen,n_epoch=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running epoch 1 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 27.058655166625975.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 26.584593380198758\n",
            "-------------------\n",
            "\n",
            "Running epoch 2 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 27.058655009550208.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 26.584593828986673\n",
            "-------------------\n",
            "\n",
            "Running epoch 3 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 27.058654874913834.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 26.584593417597752\n",
            "-------------------\n",
            "\n",
            "Running epoch 4 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 27.058654807595644.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 26.584593454996746\n",
            "-------------------\n",
            "\n",
            "Running epoch 5 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 27.05865514418658.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 26.58459352979473\n",
            "-------------------\n",
            "\n",
            "Running epoch 6 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 27.05865525638356.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 26.584593342799767\n",
            "-------------------\n",
            "\n",
            "Running epoch 7 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 27.058654964671415.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 26.58459352979473\n",
            "-------------------\n",
            "\n",
            "Running epoch 8 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 27.05865498711081.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 26.584593380198758\n",
            "-------------------\n",
            "\n",
            "Running epoch 9 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 27.058655211504767.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 26.584593492395737\n",
            "-------------------\n",
            "\n",
            "Running epoch 10 / 10\n",
            "-------------------\n",
            "Training dataset :\n",
            "Average loss : 27.058655009550208.\n",
            "-------------------\n",
            "Validation dataset\n",
            "Average loss : 26.58459379158768\n",
            "-------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A8oxFr1jm17P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "8fc2f0ac-40dd-4668-fced-e79eaf4777bf"
      },
      "source": [
        "get_per(data_loader_val_letters,cpc_model,character_classifier)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\rN/A% (0 of 26) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting the PER computation through beam search\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100% (26 of 26) |########################| Elapsed Time: 0:04:14 Time:  0:04:14\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average PER 0.967439847540553\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.967439847540553"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM-Zl4ma65Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}